{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## üß† LangChain v0.3 for Generative AI ‚Äî Structured Roadmap\n",
    "\n",
    "---\n",
    "\n",
    "### üü¢ **I. Foundation Concepts**\n",
    "\n",
    "| Topics                     | Description                                                            |\n",
    "| -------------------------- | ---------------------------------------------------------------------- |\n",
    "| üß± What is LangChain?      | Core purpose, architecture, and how it simplifies GenAI app building   |\n",
    "| ü™Ñ Why LangChain?          | Use cases: Chatbots, QA, Agents, Retrieval systems, Workflow pipelines |\n",
    "| ‚öôÔ∏è LangChain v0.3 Overview | Key updates, simplified interfaces (LCEL, Runnable), modular design    |\n",
    "\n",
    "---\n",
    "\n",
    "### üü° **II. Core Components in LangChain v0.3**\n",
    "\n",
    "#### üîπ **1. Models**\n",
    "\n",
    "* `ChatModel`, `LLM`, `TextEmbeddingModel`\n",
    "* Using OpenAI, Anthropic, HuggingFace, etc.\n",
    "* Temperature, Top-p, max tokens\n",
    "\n",
    "#### üîπ **2. Prompts**\n",
    "\n",
    "* PromptTemplates (LLM + Chat)\n",
    "* ChatMessagePromptTemplate (system, user, AI roles)\n",
    "* Partial prompts & dynamic variables\n",
    "\n",
    "#### üîπ **3. Output Parsers**\n",
    "\n",
    "* `StrOutputParser`, `JsonOutputParser`, `PydanticOutputParser`\n",
    "* Structured output from LLMs\n",
    "* Combining with prompt chains\n",
    "\n",
    "#### üîπ **4. Chains**\n",
    "\n",
    "* Runnable + LCEL (LangChain Expression Language)\n",
    "* Sequential + Parallel chains\n",
    "* RouterChain, Map-Reduce, Conditionally Routed Chains\n",
    "\n",
    "#### üîπ **5. Tools & Toolkits**\n",
    "\n",
    "* Google Search Tool, Wikipedia Tool\n",
    "* Custom tool creation using `@tool` decorators\n",
    "* Tool integration in Agent workflows\n",
    "\n",
    "#### üîπ **6. Agents**\n",
    "\n",
    "* ReAct Agent, OpenAI Functions Agent, Tool-Using Agent\n",
    "* Multi-tool orchestration\n",
    "* Memory-integrated agents\n",
    "\n",
    "#### üîπ **7. Memory**\n",
    "\n",
    "* `ConversationBufferMemory`, `SummaryMemory`, `EntityMemory`\n",
    "* Adding memory to chains and agents\n",
    "* Custom memory types\n",
    "\n",
    "---\n",
    "\n",
    "### üü† **III. Data-Aware / RAG Systems**\n",
    "\n",
    "#### üîπ **1. Document Loaders**\n",
    "\n",
    "* TextLoader, PDFLoader, WebBaseLoader\n",
    "* Unstructured.io, Playwright\n",
    "\n",
    "#### üîπ **2. Text Splitters**\n",
    "\n",
    "* RecursiveCharacterTextSplitter\n",
    "* Sentence & Token-based splitters\n",
    "* Chunk overlap best practices\n",
    "\n",
    "#### üîπ **3. Embedding Models**\n",
    "\n",
    "* OpenAI, HuggingFace, Cohere embeddings\n",
    "* Creating and normalizing vector representations\n",
    "\n",
    "#### üîπ **4. Vector Stores**\n",
    "\n",
    "* FAISS, Chroma, Qdrant, Weaviate, Pinecone\n",
    "* Adding, searching, filtering documents\n",
    "* Custom metadata support\n",
    "\n",
    "#### üîπ **5. Retrievers**\n",
    "\n",
    "* VectorStoreRetriever\n",
    "* MultiQueryRetriever, ContextualCompressionRetriever\n",
    "\n",
    "#### üîπ **6. Retrieval Chains**\n",
    "\n",
    "* `RetrievalQA`, `ConversationalRetrievalChain`\n",
    "* RAG with memory, search tools, filters\n",
    "\n",
    "---\n",
    "\n",
    "### üîµ **IV. Advanced Topics**\n",
    "\n",
    "#### üîπ **1. LangGraph (stateful flows)**\n",
    "\n",
    "* Multi-agent collaboration\n",
    "* Graph-based workflow orchestration\n",
    "* Building dynamic task routing systems\n",
    "\n",
    "#### üîπ **2. Async & Streaming**\n",
    "\n",
    "* Async LLM calls, streaming outputs\n",
    "* Event-driven architecture\n",
    "\n",
    "#### üîπ **3. LangServe (API deployment)**\n",
    "\n",
    "* Turning chains/agents into REST APIs\n",
    "* FastAPI integration\n",
    "* LangSmith + LangServe monitoring\n",
    "\n",
    "#### üîπ **4. Tracing with LangSmith**\n",
    "\n",
    "* Observability for prompts, tokens, agents\n",
    "* Debugging, optimization\n",
    "* Custom metadata logs\n",
    "\n",
    "---\n",
    "\n",
    "### üü£ **V. Practical Projects & Use Cases**\n",
    "\n",
    "| Level           | Projects                                                      |\n",
    "| --------------- | ------------------------------------------------------------- |\n",
    "| üß© Beginner     | PDF Q\\&A Bot, LLM Summarizer, SQL Agent                       |\n",
    "| üß† Intermediate | Memory Chatbot, RAG QA System, AI Tutor                       |\n",
    "| ü§ñ Advanced     | Multi-Agent Planner, Voice-based Assistant, AutoGPT-style app |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## üß† **Foundation Concepts of LangChain v0.3**\n",
    "\n",
    "---\n",
    "\n",
    "### üß± **1. What is LangChain?**\n",
    "\n",
    "**üßë‚Äçüî¨ Definition:**\n",
    "LangChain is an open-source **framework to build applications powered by Large Language Models (LLMs)** like GPT-4, Claude, and others.\n",
    "\n",
    "**üì¶ Core Purpose:**\n",
    "It abstracts away the complexity of LLM workflows by combining:\n",
    "\n",
    "* üß† Language models\n",
    "* üìÑ External data (documents, APIs, tools)\n",
    "* üîÅ Multi-step logic (chains, agents, routing)\n",
    "* üß∑ Memory & state (chatbots, long-term history)\n",
    "* üîß Deployment & observability\n",
    "\n",
    "> üß∞ LangChain = *LLM + Tools + Data + Logic + Deployment*\n",
    "\n",
    "---\n",
    "\n",
    "### üìê **LangChain Architecture Overview**\n",
    "\n",
    "| Layer                 | Purpose                                      |\n",
    "| --------------------- | -------------------------------------------- |\n",
    "| **LLMs & Tools**      | GPT-4, Claude, calculators, APIs             |\n",
    "| **Prompts & Parsers** | Templates, formatting, structured output     |\n",
    "| **Chains**            | Sequences of steps (like function pipelines) |\n",
    "| **Agents**            | Dynamically decide next actions              |\n",
    "| **Memory**            | Store chat or state context                  |\n",
    "| **RAG / Data-aware**  | Connect to vector stores for retrieval       |\n",
    "| **Deployment**        | Via LangServe, traced with LangSmith         |\n",
    "\n",
    "---\n",
    "\n",
    "### ü™Ñ **2. Why LangChain?**\n",
    "\n",
    "LangChain is used when you want to go beyond a single prompt. It's ideal for building:\n",
    "\n",
    "| Use Case                       | Description                                           |\n",
    "| ------------------------------ | ----------------------------------------------------- |\n",
    "| ü§ñ **Chatbots**                | Multi-turn, memory-aware LLM chat interfaces          |\n",
    "| ‚ùì **Question Answering (QA)**  | Answering from PDFs, websites, private docs           |\n",
    "| üß† **Agents**                  | Dynamic LLMs that reason, decide, act with tools      |\n",
    "| üîé **Retrieval Systems (RAG)** | Search + generate pipelines using vector DBs          |\n",
    "| üîÑ **Workflow Pipelines**      | Step-by-step document processing, summarization, etc. |\n",
    "\n",
    "**‚ú® Benefits:**\n",
    "\n",
    "* Abstracts boilerplate LLM logic\n",
    "* Supports modular composition (`.pipe()`, `invoke()`)\n",
    "* Easily scalable, testable, and debuggable\n",
    "* Fast integration with vector stores, databases, APIs, and tools\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è **3. What‚Äôs New in LangChain v0.3?**\n",
    "\n",
    "LangChain v0.3 is a **complete redesign** for clarity, speed, and production use.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîÑ **Major Improvements:**\n",
    "\n",
    "| Feature                                    | Description                                                  |\n",
    "| ------------------------------------------ | ------------------------------------------------------------ |\n",
    "| ‚úÖ **LCEL** (LangChain Expression Language) | Chain logic as simple function calls: `.pipe()`, `.invoke()` |\n",
    "| ‚úÖ **Runnable Interfaces**                  | Standard interface for LLMs, retrievers, chains, etc.        |\n",
    "| ‚úÖ **Streaming + Async**                    | Built-in real-time output + concurrency                      |\n",
    "| ‚úÖ **Modular & Typed**                      | You can plug anything into anything (chains, tools, LLMs)    |\n",
    "| ‚úÖ **LangServe Integration**                | Deploy chains as FastAPI REST APIs                           |\n",
    "| ‚úÖ **LangSmith**                            | Trace, debug, and evaluate everything easily                 |\n",
    "| ‚úÖ **LangGraph**                            | Orchestrate complex multi-agent workflows                    |\n",
    "\n",
    "---\n",
    "\n",
    "#### üîß Example LCEL Code:\n",
    "\n",
    "```python\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = (\n",
    "  PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "  .pipe(ChatOpenAI())\n",
    "  .pipe(StrOutputParser())\n",
    ")\n",
    "\n",
    "response = chain.invoke({\"topic\": \"AI\"})\n",
    "print(response)\n",
    "```\n",
    "\n",
    "‚úÖ That‚Äôs it! Clean, composable, async/stream-ready chain logic.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Summary\n",
    "\n",
    "| Concept               | Description                                            |\n",
    "| --------------------- | ------------------------------------------------------ |\n",
    "| üß± What is LangChain? | Framework to build intelligent LLM apps                |\n",
    "| ü™Ñ Why use it?        | Ideal for chatbots, agents, retrieval, pipelines       |\n",
    "| ‚öôÔ∏è v0.3 Update        | LCEL, modularity, deployability, LangGraph & LangSmith |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# üß± **1. Models**\n",
    "\n",
    "---\n",
    "\n",
    "### 1. ‚úÖ Definition\n",
    "\n",
    "**LangChain models** are wrappers for different types of large language models (LLMs), chat models, and embedding models. These interfaces abstract away vendor-specific APIs and unify them under a common interface so you can switch easily between OpenAI, Anthropic, HuggingFace, etc.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. ‚úÖ Types & Built-in Functions\n",
    "\n",
    "#### üî∏ A. Chat Models\n",
    "\n",
    "| Class            | Import                                              | Description            | When to Use                        |\n",
    "| ---------------- | --------------------------------------------------- | ---------------------- | ---------------------------------- |\n",
    "| `ChatOpenAI`     | `from langchain_openai import ChatOpenAI`           | OpenAI GPT-3.5 / GPT-4 | Best for conversations             |\n",
    "| `ChatAnthropic`  | `from langchain_anthropic import ChatAnthropic`     | Claude models          | For long context & safer reasoning |\n",
    "| `ChatGooglePalm` | `from langchain_google_genai import ChatGooglePalm` | Google PaLM 2          | Google‚Äôs chat models               |\n",
    "| `ChatCohere`     | `from langchain_cohere import ChatCohere`           | Cohere models          | Simpler chat setups                |\n",
    "\n",
    "```python\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0.7)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üî∏ B. LLMs (text completion)\n",
    "\n",
    "| Class       | Import                                      | Description                    | When to Use                       |\n",
    "| ----------- | ------------------------------------------- | ------------------------------ | --------------------------------- |\n",
    "| `OpenAI`    | `from langchain_openai import OpenAI`       | Traditional LLM (text-davinci) | For legacy/line-based completions |\n",
    "| `Anthropic` | `from langchain_anthropic import Anthropic` | Claude in non-chat format      | If chat abstraction isn‚Äôt needed  |\n",
    "\n",
    "```python\n",
    "llm = OpenAI(model=\"text-davinci-003\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üî∏ C. Embedding Models\n",
    "\n",
    "| Class                   | Import                                                             | Description                     | When to Use            |\n",
    "| ----------------------- | ------------------------------------------------------------------ | ------------------------------- | ---------------------- |\n",
    "| `OpenAIEmbeddings`      | `from langchain_openai import OpenAIEmbeddings`                    | Embeds text as vectors          | For search, RAG        |\n",
    "| `HuggingFaceEmbeddings` | `from langchain_community.embeddings import HuggingFaceEmbeddings` | Local or open-source embeddings | Privacy, offline setup |\n",
    "| `CohereEmbeddings`      | `from langchain_cohere import CohereEmbeddings`                    | Uses Cohere API                 | Alternatives to OpenAI |\n",
    "\n",
    "```python\n",
    "embedding = OpenAIEmbeddings()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. ‚úÖ Use Cases, Advantages, Disadvantages\n",
    "\n",
    "| Aspect          | Description                                                                                                           |\n",
    "| --------------- | --------------------------------------------------------------------------------------------------------------------- |\n",
    "| üí° Use Cases    | - Chatbots<br>- Text generation<br>- Document retrieval (embeddings)<br>- Summarization<br>- Structured output        |\n",
    "| ‚úÖ Advantages    | - Unified abstraction for various models<br>- Easily swappable providers<br>- Supports advanced configs (temp, top-p) |\n",
    "| ‚ùå Disadvantages | - External API cost<br>- Rate limits<br>- Model drift across versions                                                 |\n",
    "| ‚ö†Ô∏è Limitations  | - Requires internet/API access<br>- May not support every new model out of the box                                    |\n",
    "\n",
    "---\n",
    "\n",
    "### 4. ‚úÖ Best Model Integrations & When\n",
    "\n",
    "| Scenario                          | Best Model                                                         |\n",
    "| --------------------------------- | ------------------------------------------------------------------ |\n",
    "| Conversational AI                 | `ChatOpenAI`, `ChatAnthropic`                                      |\n",
    "| Cost-sensitive or quick prototype | `ChatOpenAI` with gpt-3.5                                          |\n",
    "| Private/Enterprise use            | `ChatGooglePalm`, `Cohere`, or local models                        |\n",
    "| Long context (>100k tokens)       | `Claude 2/3 (ChatAnthropic)`                                       |\n",
    "| Embedding for RAG                 | `OpenAIEmbeddings` for accuracy, `HuggingFaceEmbeddings` for local |\n",
    "\n",
    "---\n",
    "\n",
    "### 5. ‚úÖ Extras\n",
    "\n",
    "* üìå **Temperature (0‚Äì1)**: Lower = more deterministic\n",
    "* üìå **Top-p**: Controls sampling diversity (alternative to temperature)\n",
    "* üìå Use `.bind()` to fix certain values into the model for reuse:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# ‚ú≥Ô∏è **2. Prompts**\n",
    "\n",
    "---\n",
    "\n",
    "### 1. ‚úÖ Definition\n",
    "\n",
    "In LangChain, **prompts** are dynamic templates used to structure the input sent to LLMs. They allow you to combine static instructions with dynamic user data ‚Äî ensuring consistent, reusable, and controlled LLM inputs.\n",
    "\n",
    "LangChain supports:\n",
    "\n",
    "* Text prompts ‚Üí for `LLM` models\n",
    "* Chat prompts ‚Üí for `ChatModel` models using system, user, AI roles\n",
    "\n",
    "---\n",
    "\n",
    "### 2. ‚úÖ Types & Built-in Functions (with Definitions + Use Cases)\n",
    "\n",
    "#### üîπ A. `PromptTemplate`\n",
    "\n",
    "üìò **Definition**: A template for formatting string prompts with dynamic variables.\n",
    "\n",
    "```python\n",
    "from langchain.prompts import PromptTemplate\n",
    "```\n",
    "\n",
    "| Function / Class   | Purpose                                                         | When to Use                                 |\n",
    "| ------------------ | --------------------------------------------------------------- | ------------------------------------------- |\n",
    "| `PromptTemplate()` | Create a prompt with input variables and a template string      | For traditional LLM models (not chat-based) |\n",
    "| `from_template()`  | Alternative constructor to quickly define prompt with variables | Faster setup                                |\n",
    "\n",
    "```python\n",
    "template = PromptTemplate.from_template(\"What is the capital of {country}?\")\n",
    "prompt = template.format(country=\"France\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üîπ B. `ChatPromptTemplate`\n",
    "\n",
    "üìò **Definition**: Used to construct a sequence of chat messages for chat-based models (e.g., GPT-4).\n",
    "\n",
    "```python\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "```\n",
    "\n",
    "| Function / Class                     | Purpose                                                                    | When to Use                                           |\n",
    "| ------------------------------------ | -------------------------------------------------------------------------- | ----------------------------------------------------- |\n",
    "| `ChatPromptTemplate.from_messages()` | Build prompts using a list of role-based messages (`system`, `user`, `ai`) | Best for use with `ChatOpenAI`, `ChatAnthropic`, etc. |\n",
    "\n",
    "```python\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a smart tutor.\"),\n",
    "    (\"user\", \"Explain {topic} in 2 lines.\")\n",
    "])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üîπ C. `SystemMessagePromptTemplate`, `HumanMessagePromptTemplate`, `AIMessagePromptTemplate`\n",
    "\n",
    "üìò **Definition**: Specialized prompt blocks to structure chat messages by role.\n",
    "\n",
    "```python\n",
    "from langchain.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate\n",
    ")\n",
    "```\n",
    "\n",
    "| Class                                         | Purpose                                                    | When to Use                               |\n",
    "| --------------------------------------------- | ---------------------------------------------------------- | ----------------------------------------- |\n",
    "| `SystemMessagePromptTemplate.from_template()` | Define system role instructions (sets tone or role of LLM) | Use when you want to control LLM behavior |\n",
    "| `HumanMessagePromptTemplate.from_template()`  | Define user messages with placeholders                     | Use for user inputs                       |\n",
    "| `AIMessagePromptTemplate.from_template()`     | (Optional) Simulate previous AI responses                  | For chat memory simulation                |\n",
    "\n",
    "---\n",
    "\n",
    "#### üîπ D. `MessagesPlaceholder`\n",
    "\n",
    "üìò **Definition**: Placeholder for injecting past messages (used with memory).\n",
    "\n",
    "```python\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "```\n",
    "\n",
    "| Class                                               | Purpose                                                  | When to Use                  |\n",
    "| --------------------------------------------------- | -------------------------------------------------------- | ---------------------------- |\n",
    "| `MessagesPlaceholder(variable_name=\"chat_history\")` | Plug in dynamic memory-based message history into prompt | Use in memory-based chatbots |\n",
    "\n",
    "---\n",
    "\n",
    "#### üîπ E. `prompt.partial()`\n",
    "\n",
    "üìò **Definition**: Pre-fill or ‚Äúlock‚Äù certain variables in a prompt for reuse.\n",
    "\n",
    "| Method                      | Purpose                          | When to Use                                                       |\n",
    "| --------------------------- | -------------------------------- | ----------------------------------------------------------------- |\n",
    "| `prompt.partial(var=value)` | Set a fixed value for a variable | Useful when the same value is used repeatedly (e.g., system role) |\n",
    "\n",
    "```python\n",
    "partial_prompt = prompt.partial(topic=\"machine learning\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. ‚úÖ Use Cases, Advantages, Disadvantages, Limitations\n",
    "\n",
    "| Aspect          | Description                                                                                       |\n",
    "| --------------- | ------------------------------------------------------------------------------------------------- |\n",
    "| üí° Use Cases    | - Dynamic prompting<br>- Chat instruction flows<br>- Role-based responses<br>- Injecting memory   |\n",
    "| ‚úÖ Advantages    | - Reusable<br>- Cleaner logic<br>- Supports chat format + variable substitution                   |\n",
    "| ‚ùå Disadvantages | - Can become complex with too many variables<br>- Needs careful formatting                        |\n",
    "| ‚ö†Ô∏è Limitations  | - Not model-aware ‚Äî doesn‚Äôt prevent misuse (e.g., using ChatPromptTemplate with text-only models) |\n",
    "\n",
    "---\n",
    "\n",
    "### 4. ‚úÖ Best Model Integrations\n",
    "\n",
    "| Prompt Type                         | Best Paired Model                                     |\n",
    "| ----------------------------------- | ----------------------------------------------------- |\n",
    "| `PromptTemplate`                    | `OpenAI`, `Anthropic`, `TextGen models`               |\n",
    "| `ChatPromptTemplate`                | `ChatOpenAI`, `ChatAnthropic`, `ChatCohere`, `Claude` |\n",
    "| Role-based prompts (System/User/AI) | `ChatOpenAI`, `Claude`, `Gemini`                      |\n",
    "| `MessagesPlaceholder`               | Use with `Memory-enabled` chains or agents            |\n",
    "\n",
    "---\n",
    "\n",
    "### 5. ‚úÖ Extras / Best Practices\n",
    "\n",
    "| Tip                                              | Why It Matters                                 |\n",
    "| ------------------------------------------------ | ---------------------------------------------- |\n",
    "| Use `ChatPromptTemplate` with modern chat models | Better formatting and separation of roles      |\n",
    "| Favor `partial()` to reduce prompt complexity    | Improves reusability                           |\n",
    "| Combine prompts with `OutputParsers`             | Ensure output is structured & machine-readable |\n",
    "| Use `{variable}` instead of string interpolation | LangChain safely manages variable formatting   |\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Summary Code Example\n",
    "\n",
    "```python\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"user\", \"Tell me about {topic}\")\n",
    "])\n",
    "\n",
    "print(prompt.format_messages(topic=\"LangChain\"))\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# üß™ **3. Output Parsers**\n",
    "\n",
    "---\n",
    "\n",
    "### 1. ‚úÖ Definition\n",
    "\n",
    "**Output Parsers** in LangChain are components that convert the raw response from a model (usually a text string) into a **structured, usable format** like:\n",
    "\n",
    "* Plain text\n",
    "* JSON dict\n",
    "* Python objects (via Pydantic)\n",
    "\n",
    "They're essential when chaining prompts ‚Üí models ‚Üí usable outputs.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. ‚úÖ Types & Built-in Functions\n",
    "\n",
    "---\n",
    "\n",
    "#### üî∏ A. `StrOutputParser`\n",
    "\n",
    "üìò **Definition**: Parses model output into plain string by trimming and returning it directly.\n",
    "\n",
    "```python\n",
    "from langchain.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "```\n",
    "\n",
    "‚úÖ **When to Use**:\n",
    "\n",
    "* You just need plain text output (summaries, answers, explanations)\n",
    "* Default for most LLM chains\n",
    "\n",
    "üß† Example:\n",
    "\n",
    "```python\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "response = chain.invoke({\"question\": \"What is LangChain?\"})\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üî∏ B. `JsonOutputParser`\n",
    "\n",
    "üìò **Definition**: Parses model output into a Python `dict` assuming it's valid JSON.\n",
    "\n",
    "```python\n",
    "from langchain.output_parsers import JsonOutputParser\n",
    "\n",
    "parser = JsonOutputParser()\n",
    "```\n",
    "\n",
    "‚úÖ **When to Use**:\n",
    "\n",
    "* LLM is prompted to return structured JSON\n",
    "* Ideal for structured tasks like form-filling, RAG metadata, tabular Q\\&A\n",
    "\n",
    "üß† Example Prompt:\n",
    "\n",
    "```python\n",
    "\"Extract a JSON object with `name`, `age`, and `hobby`: {text}\"\n",
    "```\n",
    "\n",
    "üí° Pro Tip: GPT-4 + `response_format='json'` (OpenAI tools) ‚Üí works best here.\n",
    "\n",
    "---\n",
    "\n",
    "#### üî∏ C. `PydanticOutputParser`\n",
    "\n",
    "üìò **Definition**: Converts model output into a Python object based on a defined `Pydantic` schema ‚Äî validates field types.\n",
    "\n",
    "```python\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Person(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Person)\n",
    "```\n",
    "\n",
    "‚úÖ **When to Use**:\n",
    "\n",
    "* You want **strong type safety**\n",
    "* Output will be used in APIs, databases, or further processing\n",
    "\n",
    "üß† Example:\n",
    "\n",
    "```python\n",
    "chain = prompt | llm | parser\n",
    "result = chain.invoke({\"text\": \"name: Alice, age: 30\"})\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üî∏ D. `RetryOutputParser`\n",
    "\n",
    "üìò **Definition**: Automatically **retries** parsing if LLM output is invalid ‚Äî using feedback prompts.\n",
    "\n",
    "```python\n",
    "from langchain.output_parsers import RetryOutputParser\n",
    "```\n",
    "\n",
    "‚úÖ **When to Use**:\n",
    "\n",
    "* You're using `JsonOutputParser` or `PydanticOutputParser` and want to handle format errors gracefully\n",
    "* Mission-critical or production systems\n",
    "\n",
    "üß† Example:\n",
    "\n",
    "```python\n",
    "parser = RetryOutputParser.from_parser(parser=JsonOutputParser())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. ‚úÖ Use Cases, Advantages, Disadvantages\n",
    "\n",
    "| Use Case        | Examples                           |\n",
    "| --------------- | ---------------------------------- |\n",
    "| String output   | Answer generation, summarization   |\n",
    "| JSON extraction | Structured data, document metadata |\n",
    "| Typed output    | APIs, validators, downstream logic |\n",
    "\n",
    "| ‚úÖ Advantages         | ‚ùå Disadvantages               |\n",
    "| -------------------- | ----------------------------- |\n",
    "| Clean parsing logic  | LLM must follow strict format |\n",
    "| Modular & composable | Fails if output malformed     |\n",
    "| Works with all LLMs  | Retry parser adds latency     |\n",
    "\n",
    "---\n",
    "\n",
    "### 4. ‚úÖ Best Model Integrations\n",
    "\n",
    "| Parser                 | Best With       | Why                         |\n",
    "| ---------------------- | --------------- | --------------------------- |\n",
    "| `StrOutputParser`      | Any LLM         | Default output              |\n",
    "| `JsonOutputParser`     | GPT-4, Claude 3 | Structured output mode      |\n",
    "| `PydanticOutputParser` | GPT-4, Claude   | Typed schemas for APIs      |\n",
    "| `RetryOutputParser`    | OpenAI, Claude  | Handles bad format recovery |\n",
    "\n",
    "---\n",
    "\n",
    "### 5. ‚úÖ Tips & Best Practices\n",
    "\n",
    "‚úÖ Use `output_parser.get_format_instructions()` in your prompt\n",
    "‚û°Ô∏è Adds automatic formatting hints for the model.\n",
    "\n",
    "üß† Example with prompt:\n",
    "\n",
    "```python\n",
    "prompt = PromptTemplate.from_template(\n",
    "  \"Extract details:\\n{format_instructions}\\nText: {input_text}\"\n",
    ").partial(format_instructions=parser.get_format_instructions())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîó Common Pattern: Prompt ‚Üí Model ‚Üí Parser\n",
    "\n",
    "```python\n",
    "chain = prompt | llm | JsonOutputParser()\n",
    "result = chain.invoke({\"input_text\": \"My name is John and I am 22 years old.\"})\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# üîÅ **4. Chains (Runnable + LCEL)**\n",
    "\n",
    "---\n",
    "\n",
    "### 1. ‚úÖ Definition\n",
    "\n",
    "**Chains** are sequences of operations (prompt ‚Üí LLM ‚Üí output parser) that execute together. LangChain v0.3 uses a new composition system called **LCEL (LangChain Expression Language)** built on the `Runnable` interface ‚Äî making it modular, fast, and flexible.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. ‚úÖ Types & Built-in Functions\n",
    "\n",
    "---\n",
    "\n",
    "#### üî∏ A. `Runnable` (Base class)\n",
    "\n",
    "üìò **Definition**: Abstract interface for anything that can ‚Äúrun‚Äù ‚Äî a model, prompt, parser, or even function.\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import Runnable\n",
    "```\n",
    "\n",
    "| Function / Class         | Purpose                                   | When to Use                           |\n",
    "| ------------------------ | ----------------------------------------- | ------------------------------------- |\n",
    "| `Runnable.invoke(input)` | Runs a single input through the component | Default for most use cases            |\n",
    "| `Runnable.batch(inputs)` | Run multiple inputs at once               | Use for performance (batch LLM calls) |\n",
    "| `Runnable.map()`         | Runs on a stream of inputs in parallel    | Use with lists or streams             |\n",
    "\n",
    "---\n",
    "\n",
    "#### üî∏ B. `RunnableLambda`\n",
    "\n",
    "üìò **Definition**: Wraps any Python function as a `Runnable`.\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "```\n",
    "\n",
    "| Function             | Purpose                          | When to Use                   |\n",
    "| -------------------- | -------------------------------- | ----------------------------- |\n",
    "| `RunnableLambda(fn)` | Wrap function as a runnable step | Add pre/post processing logic |\n",
    "\n",
    "‚úÖ Example:\n",
    "\n",
    "```python\n",
    "strip_input = RunnableLambda(lambda x: x.strip())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üî∏ C. `RunnableSequence`\n",
    "\n",
    "üìò **Definition**: Chain multiple `Runnable` components manually (alternative to using `|`).\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "```\n",
    "\n",
    "‚úÖ Example:\n",
    "\n",
    "```python\n",
    "chain = RunnableSequence(first_step, second_step)\n",
    "```\n",
    "\n",
    "üìå **Tip**: `step1 | step2 | step3` is preferred for cleaner syntax.\n",
    "\n",
    "---\n",
    "\n",
    "#### üî∏ D. LCEL Composition (`|` operator)\n",
    "\n",
    "üìò **Definition**: Connect components using the pipe `|` operator.\n",
    "\n",
    "```python\n",
    "chain = prompt | llm | output_parser\n",
    "```\n",
    "\n",
    "‚úÖ This is the most modern and preferred chaining method in v0.3.\n",
    "\n",
    "---\n",
    "\n",
    "#### üî∏ E. `invoke`, `batch`, `stream`\n",
    "\n",
    "| Method      | Purpose                     | When to Use                   |\n",
    "| ----------- | --------------------------- | ----------------------------- |\n",
    "| `.invoke()` | For single inputs           | Most common use               |\n",
    "| `.batch()`  | For multiple inputs         | Performance-sensitive cases   |\n",
    "| `.stream()` | For streaming output tokens | Use with chat UI or CLI tools |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. ‚úÖ Use Cases, Advantages, Disadvantages\n",
    "\n",
    "| Use Case             | Example                   |\n",
    "| -------------------- | ------------------------- |\n",
    "| Prompt ‚Üí LLM ‚Üí Parse | Chatbot, completion, Q\\&A |\n",
    "| File ‚Üí Embed ‚Üí Store | RAG pipelines             |\n",
    "| Tool ‚Üí LLM ‚Üí Decide  | Agents                    |\n",
    "\n",
    "---\n",
    "\n",
    "| ‚úÖ Advantages                        | ‚ùå Disadvantages                              |                                    |\n",
    "| ----------------------------------- | -------------------------------------------- | ---------------------------------- |\n",
    "| Clean composition (\\`               | \\` style)                                    | Complex logic may need custom code |\n",
    "| Efficient + parallelizable          | Debugging deeply nested chains is harder     |                                    |\n",
    "| Works with all LangChain components | Requires understanding Runnable architecture |                                    |\n",
    "\n",
    "---\n",
    "\n",
    "### 4. ‚úÖ Best Model Integrations\n",
    "\n",
    "| Chain Type       | Best With      |                                    |                               |\n",
    "| ---------------- | -------------- | ---------------------------------- | ----------------------------- |\n",
    "| \\`Prompt         | ChatModel      | Parser\\`                           | `ChatOpenAI`, `ChatAnthropic` |\n",
    "| \\`Prompt         | LLM            | Parser\\`                           | `OpenAI`, `Claude`            |\n",
    "| \\`Embedding      | Vector Store\\` | `OpenAIEmbeddings`, `HFEmbeddings` |                               |\n",
    "| \\`RunnableLambda | LLM            | Parser\\`                           | For preprocessing pipelines   |\n",
    "\n",
    "---\n",
    "\n",
    "### 5. ‚úÖ Extras & Best Practices\n",
    "\n",
    "| Best Practice                              | Why It‚Äôs Useful                       |\n",
    "| ------------------------------------------ | ------------------------------------- |\n",
    "| Use `RunnableLambda` to preprocess data    | Trim, clean, enrich inputs before LLM |\n",
    "| Use `.bind()` for partial config injection | Reuse chains with static params       |\n",
    "| Compose reusable building blocks           | Clean logic & better modularity       |\n",
    "| Prefer `.invoke()` in notebooks / scripts  | Simple and synchronous                |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# üß∞ **5. Tools & Toolkits**\n",
    "\n",
    "---\n",
    "\n",
    "### 1. ‚úÖ Definition\n",
    "\n",
    "**Tools** in LangChain are modular, callable actions that LLM-based agents can use to interact with the outside world. Examples: web search, Wikipedia lookup, code execution, calculators, APIs.\n",
    "\n",
    "LangChain also allows you to define **custom tools** using the `@tool` decorator.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. ‚úÖ Types & Built-in Functions\n",
    "\n",
    "---\n",
    "\n",
    "#### üî∏ A. Built-in Tools\n",
    "\n",
    "| Tool                                  | Description                                   | When to Use                    |\n",
    "| ------------------------------------- | --------------------------------------------- | ------------------------------ |\n",
    "| `WikipediaQueryRun`                   | Search & extract content from Wikipedia       | Quick factual lookup           |\n",
    "| `GoogleSearchRun`                     | Perform Google search (via SerpAPI or Tavily) | Up-to-date information         |\n",
    "| `ArxivQueryRun`                       | Search academic papers                        | Research-based agents          |\n",
    "| `PythonREPLTool`                      | Run Python code snippets                      | Coding assistants, math agents |\n",
    "| `RequestsGetTool`, `RequestsPostTool` | Make HTTP requests                            | API-based toolchains           |\n",
    "| `TerminalTool`                        | Run shell commands (‚ö†Ô∏è risky)                 | DevOps, scripting workflows    |\n",
    "\n",
    "‚úÖ Example:\n",
    "\n",
    "```python\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "wiki_tool = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üî∏ B. Custom Tools with `@tool`\n",
    "\n",
    "üìò **Definition**: Use the `@tool` decorator to turn a Python function into a LangChain-compatible tool.\n",
    "\n",
    "```python\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def calculate_area(length: float, width: float) -> float:\n",
    "    \"\"\"Calculate area of a rectangle\"\"\"\n",
    "    return length * width\n",
    "```\n",
    "\n",
    "‚úÖ **When to Use**:\n",
    "\n",
    "* You want to add business logic\n",
    "* Integrate external APIs\n",
    "* Provide internal databases or calculations to agents\n",
    "\n",
    "üß† Tips:\n",
    "\n",
    "* Use docstrings! Agents rely on them to understand the tool's purpose.\n",
    "\n",
    "---\n",
    "\n",
    "#### üî∏ C. `Tool` Class (Manual Tool)\n",
    "\n",
    "```python\n",
    "from langchain_core.tools import Tool\n",
    "\n",
    "tool = Tool(\n",
    "    name=\"Echo\",\n",
    "    func=lambda x: x,\n",
    "    description=\"Repeats the input.\"\n",
    ")\n",
    "```\n",
    "\n",
    "‚úÖ Use this when you want full control (no decorator).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. ‚úÖ Use Cases, Advantages, Disadvantages\n",
    "\n",
    "| Use Case               | Example                               |\n",
    "| ---------------------- | ------------------------------------- |\n",
    "| Answer with web search | `GoogleSearchTool`, `WikipediaTool`   |\n",
    "| Math/code agents       | `PythonREPLTool`                      |\n",
    "| Custom logic           | `@tool` function for price prediction |\n",
    "\n",
    "| ‚úÖ Advantages                      | ‚ùå Disadvantages                          |\n",
    "| --------------------------------- | ---------------------------------------- |\n",
    "| Easy to add external capabilities | Tools must be deterministic              |\n",
    "| Integrate any API                 | Agents may misuse poorly described tools |\n",
    "| Works well with agent workflows   | Requires prompt clarity for usage        |\n",
    "\n",
    "---\n",
    "\n",
    "### 4. ‚úÖ Best Model Integrations\n",
    "\n",
    "| Tool Type    | Best With                                            |\n",
    "| ------------ | ---------------------------------------------------- |\n",
    "| Web tools    | `ChatOpenAI`, `ChatAnthropic` (with agent framework) |\n",
    "| Python tools | GPT-4 (for code reasoning)                           |\n",
    "| Custom APIs  | Any chat model used in agents                        |\n",
    "\n",
    "---\n",
    "\n",
    "### 5. ‚úÖ Tips & Best Practices\n",
    "\n",
    "| Tip                                                    | Why                                           |\n",
    "| ------------------------------------------------------ | --------------------------------------------- |\n",
    "| Always write clear `description` or docstring          | Agent uses it to decide when to call the tool |\n",
    "| Combine tools into a toolkit                           | Cleaner modular architecture                  |\n",
    "| Test tools independently                               | Prevent agent confusion during use            |\n",
    "| Use `Tool` or `@tool` ‚Äî both register tools for agents |                                               |\n",
    "\n",
    "---\n",
    "\n",
    "### üîó Tool Integration in Agent Workflows\n",
    "\n",
    "‚úÖ Once you define tools, you can pass them to agents like this:\n",
    "\n",
    "```python\n",
    "from langchain.agents import initialize_agent\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=[wiki_tool, calculate_area],\n",
    "    llm=ChatOpenAI(),\n",
    "    agent_type=\"openai-functions\"\n",
    ")\n",
    "```\n",
    "\n",
    "Then:\n",
    "\n",
    "```python\n",
    "agent.invoke(\"What is the area of a 10x5 rectangle?\")\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# üß† **6. Agents**\n",
    "\n",
    "LangChain v0.3 ‚Äî All About Smart, Tool-Using Agents üîßüßë‚Äçüíª\n",
    "\n",
    "---\n",
    "\n",
    "### 1. ‚úÖ Definition\n",
    "\n",
    "**Agents** in LangChain are intelligent LLM-powered decision-makers that dynamically **select tools**, plan tasks, call APIs, and remember past interactions.\n",
    "\n",
    "Unlike static chains, agents decide what steps to take based on user input and their internal reasoning.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. ‚úÖ Types & Built-in Agent Executors\n",
    "\n",
    "---\n",
    "\n",
    "#### üî∏ A. **ReAct Agent (Reasoning + Acting)**\n",
    "\n",
    "üìò Combines reasoning with tool usage ‚Äî the LLM thinks step-by-step before selecting a tool.\n",
    "\n",
    "```python\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent_type=AgentType.REACT_DESCRIPTION\n",
    ")\n",
    "```\n",
    "\n",
    "‚úÖ **When to Use**:\n",
    "\n",
    "* You want detailed intermediate steps (Thought ‚Üí Action ‚Üí Observation)\n",
    "* Transparent reasoning flow\n",
    "\n",
    "üß† Outputs:\n",
    "\n",
    "```\n",
    "Thought: I should look up the capital\n",
    "Action: WikipediaTool\n",
    "Observation: Capital is Paris\n",
    "Final Answer: Paris\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üî∏ B. **OpenAI Functions Agent**\n",
    "\n",
    "üìò Uses OpenAI‚Äôs `function_calling` capability (GPT-3.5/4) to **invoke tools like API functions** directly.\n",
    "\n",
    "```python\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=ChatOpenAI(model=\"gpt-4\", temperature=0),\n",
    "    agent_type=AgentType.OPENAI_FUNCTIONS\n",
    ")\n",
    "```\n",
    "\n",
    "‚úÖ **When to Use**:\n",
    "\n",
    "* You want function-calling interface (JSON-style interaction)\n",
    "* GPT-4 with structured outputs\n",
    "* Less verbose than ReAct\n",
    "\n",
    "---\n",
    "\n",
    "#### üî∏ C. **Tool-Using Agent (Zero-shot or Multi-tool)**\n",
    "\n",
    "üìò Uses tool descriptions only (no examples) to **decide which tool to call**.\n",
    "\n",
    "```python\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION\n",
    ")\n",
    "```\n",
    "\n",
    "‚úÖ **When to Use**:\n",
    "\n",
    "* You have several tools and want the agent to pick the best\n",
    "* Multi-tool workflows (weather, calculator, search, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "### 3. ‚úÖ Multi-Tool Orchestration\n",
    "\n",
    "LangChain agents can:\n",
    "\n",
    "* Choose **one tool** or multiple tools in sequence\n",
    "* Chain thoughts + tool use recursively\n",
    "* Combine with `@tool`, `Tool`, `Toolkit` classes\n",
    "\n",
    "‚úÖ Example:\n",
    "\n",
    "```python\n",
    "tools = [search_tool, calc_tool, code_tool]\n",
    "agent = initialize_agent(tools, llm, agent_type=\"openai-functions\")\n",
    "```\n",
    "\n",
    "Then invoke:\n",
    "\n",
    "```python\n",
    "agent.invoke(\"Search the weather and calculate what to wear for 15¬∞C.\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. ‚úÖ Memory-Integrated Agents\n",
    "\n",
    "Agents can **retain memory** of the conversation using `ConversationBufferMemory`, `SummaryMemory`, etc.\n",
    "\n",
    "üìò Add memory like this:\n",
    "\n",
    "```python\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent_type=\"openai-functions\",\n",
    "    memory=memory\n",
    ")\n",
    "```\n",
    "\n",
    "‚úÖ Agent will now remember past questions, facts, and its own responses.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. ‚úÖ Use Cases, Pros, Cons\n",
    "\n",
    "| Use Case                | Agent Type                     |\n",
    "| ----------------------- | ------------------------------ |\n",
    "| Complex reasoning       | ReAct Agent                    |\n",
    "| API automation          | OpenAI Functions               |\n",
    "| Many tools, no training | Zero-shot Tool Agent           |\n",
    "| Human-like assistant    | Memory agent with chat history |\n",
    "\n",
    "---\n",
    "\n",
    "| ‚úÖ Advantages                      | ‚ùå Disadvantages               |\n",
    "| --------------------------------- | ----------------------------- |\n",
    "| Dynamic, intelligent behavior     | Higher latency                |\n",
    "| Supports tools, memory, reasoning | Can hallucinate tool names    |\n",
    "| Extensible with any tool          | Needs clear tool descriptions |\n",
    "\n",
    "---\n",
    "\n",
    "### 6. ‚úÖ Best Model Integrations\n",
    "\n",
    "| Agent Type       | Best With                              |\n",
    "| ---------------- | -------------------------------------- |\n",
    "| ReAct            | OpenAI, Claude                         |\n",
    "| OpenAI Functions | GPT-4 with `function_calling`          |\n",
    "| Tool-Using       | All LLMs via LangChain                 |\n",
    "| Memory Agents    | Any ChatModel (`ChatOpenAI`, `Claude`) |\n",
    "\n",
    "---\n",
    "\n",
    "### 7. ‚úÖ Tips & Best Practices\n",
    "\n",
    "| Tip                                          | Why                             |\n",
    "| -------------------------------------------- | ------------------------------- |\n",
    "| Use `@tool` with good docstrings             | LLM uses them to choose tools   |\n",
    "| Prefer `OpenAI Functions` for clarity        | JSON format & API alignment     |\n",
    "| Add memory for chatbot-style agents          | Keeps context across steps      |\n",
    "| Use `AgentExecutor` for fine-grained control | Manage steps, timeouts, retries |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# üß† **7. Memory**\n",
    "\n",
    "---\n",
    "\n",
    "### 1. ‚úÖ Definition\n",
    "\n",
    "**Memory** in LangChain allows chains and agents to remember past interactions. This enables **contextual conversations**, follow-ups, and continuity ‚Äî making LLMs feel more intelligent and less repetitive.\n",
    "\n",
    "Memory stores previous inputs/outputs and feeds them back into prompts automatically.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. ‚úÖ Types & Built-in Memory Classes\n",
    "\n",
    "---\n",
    "\n",
    "#### üî∏ A. `ConversationBufferMemory`\n",
    "\n",
    "üìò **Stores** the entire raw conversation (input/output) as a buffer.\n",
    "\n",
    "```python\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "```\n",
    "\n",
    "‚úÖ **When to Use**:\n",
    "\n",
    "* You want full chat history preserved\n",
    "* Best for assistant/chatbots with short-to-mid term memory\n",
    "\n",
    "üß† Example:\n",
    "\n",
    "```python\n",
    "memory = ConversationBufferMemory()\n",
    "```\n",
    "\n",
    "üîÅ Feeds entire text history into the prompt like:\n",
    "\n",
    "```\n",
    "User: Hello  \n",
    "AI: Hi!  \n",
    "User: What‚Äôs my name?  \n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üî∏ B. `ConversationSummaryMemory`\n",
    "\n",
    "üìò **Summarizes** the previous messages to stay within context window limits.\n",
    "\n",
    "```python\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "```\n",
    "\n",
    "‚úÖ **When to Use**:\n",
    "\n",
    "* Long conversations where full history won't fit into context\n",
    "* You want compression + memory\n",
    "\n",
    "üß† How it works:\n",
    "\n",
    "* Uses an LLM to summarize past interactions into a single paragraph\n",
    "\n",
    "```python\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üî∏ C. `ConversationBufferWindowMemory`\n",
    "\n",
    "üìò Keeps only the **last N messages** in buffer.\n",
    "\n",
    "```python\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "```\n",
    "\n",
    "‚úÖ **When to Use**:\n",
    "\n",
    "* You want to limit token usage but preserve recent context (e.g., last 3 exchanges)\n",
    "\n",
    "üß† Example:\n",
    "\n",
    "```python\n",
    "memory = ConversationBufferWindowMemory(k=3)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üî∏ D. `ConversationKGMemory` (Knowledge Graph-based)\n",
    "\n",
    "üìò Tracks **entities and their relationships** from conversation history.\n",
    "\n",
    "```python\n",
    "from langchain.memory import ConversationKGMemory\n",
    "```\n",
    "\n",
    "‚úÖ **When to Use**:\n",
    "\n",
    "* You want to track subjects like people, places, and objects\n",
    "* Especially useful for QA bots or assistants handling structured data\n",
    "\n",
    "---\n",
    "\n",
    "#### üî∏ E. `EntityMemory`\n",
    "\n",
    "üìò Extracts and tracks **named entities** across the conversation.\n",
    "\n",
    "```python\n",
    "from langchain.memory import ConversationEntityMemory\n",
    "```\n",
    "\n",
    "‚úÖ **When to Use**:\n",
    "\n",
    "* Chatbot needs to remember people, organizations, etc. by name\n",
    "* Personal assistants, customer service bots\n",
    "\n",
    "---\n",
    "\n",
    "### 3. ‚úÖ Adding Memory to Chains & Agents\n",
    "\n",
    "---\n",
    "\n",
    "#### üîπ A. With Agents\n",
    "\n",
    "```python\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=ChatOpenAI(),\n",
    "    agent_type=\"openai-functions\",\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "```\n",
    "\n",
    "#### üîπ B. With Chains\n",
    "\n",
    "```python\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "chain = ConversationChain(\n",
    "    llm=ChatOpenAI(),\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. ‚úÖ Custom Memory Types\n",
    "\n",
    "You can also build your **own memory class** by subclassing:\n",
    "\n",
    "```python\n",
    "from langchain_core.memory import BaseMemory\n",
    "\n",
    "class MyCustomMemory(BaseMemory):\n",
    "    def load_memory_variables(self, inputs):\n",
    "        ...\n",
    "    def save_context(self, inputs, outputs):\n",
    "        ...\n",
    "```\n",
    "\n",
    "‚úÖ Use this when:\n",
    "\n",
    "* You want to store memory in a DB\n",
    "* Or fetch memories based on semantic similarity\n",
    "\n",
    "---\n",
    "\n",
    "### 5. ‚úÖ Use Cases, Advantages, Limitations\n",
    "\n",
    "| Use Case                | Best Memory                 |\n",
    "| ----------------------- | --------------------------- |\n",
    "| Chatbot (short context) | `ConversationBufferMemory`  |\n",
    "| Long dialogue           | `ConversationSummaryMemory` |\n",
    "| Entity tracking         | `EntityMemory`              |\n",
    "| Structured info         | `KGMemory`                  |\n",
    "\n",
    "| ‚úÖ Advantages                                | ‚ùå Disadvantages                         |\n",
    "| ------------------------------------------- | --------------------------------------- |\n",
    "| Retains memory without you managing context | Can leak irrelevant history into prompt |\n",
    "| Supports summarization and scaling          | Summarizers may hallucinate facts       |\n",
    "| Multiple plug-and-play types                | Some memory types increase token usage  |\n",
    "\n",
    "---\n",
    "\n",
    "### 6. ‚úÖ Best Model Integrations\n",
    "\n",
    "| Memory Type                | Best With                               |\n",
    "| -------------------------- | --------------------------------------- |\n",
    "| `BufferMemory`             | Any chat model (`ChatOpenAI`, `Claude`) |\n",
    "| `SummaryMemory`            | GPT-4 (for better summaries)            |\n",
    "| `KGMemory`, `EntityMemory` | Claude, GPT-4 (structured extraction)   |\n",
    "\n",
    "---\n",
    "\n",
    "### 7. ‚úÖ Tips & Best Practices\n",
    "\n",
    "| Tip                                         | Why                                   |\n",
    "| ------------------------------------------- | ------------------------------------- |\n",
    "| Use `BufferMemory` to start                 | Easiest and transparent               |\n",
    "| For long sessions, use `SummaryMemory`      | Avoids context overflow               |\n",
    "| Combine with `MessagesPlaceholder`          | Plug memory into `ChatPromptTemplate` |\n",
    "| Use in agents to enable multi-step planning | Maintains logical continuity          |\n",
    "\n",
    "---\n",
    "\n",
    "üß† Example: Integrating Memory in a Chain\n",
    "\n",
    "```python\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant.\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"ai\", \"{chat_history}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "chain.invoke({\"input\": \"Hi\", \"chat_history\": memory.load_memory_variables({})})\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# üìÑ **1. Document Loaders**\n",
    "\n",
    "*(Part of üü† III. Data-Aware / RAG Systems)*\n",
    "\n",
    "---\n",
    "\n",
    "### 1. ‚úÖ Definition\n",
    "\n",
    "**Document Loaders** are LangChain components that **ingest and convert raw data sources** (like PDFs, text files, websites, emails, databases) into a standard `Document` format (containing `page_content` and `metadata`).\n",
    "\n",
    "These are essential in any **RAG** pipeline, where LLMs answer questions based on external knowledge.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. ‚úÖ Common Loader Types & Usage\n",
    "\n",
    "---\n",
    "\n",
    "#### üî∏ A. `TextLoader`\n",
    "\n",
    "üìò Loads plain `.txt` files into documents.\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"example.txt\")\n",
    "docs = loader.load()\n",
    "```\n",
    "\n",
    "‚úÖ When to Use:\n",
    "\n",
    "* Simple plain-text files\n",
    "* CLI data or code exports\n",
    "\n",
    "---\n",
    "\n",
    "#### üî∏ B. `PDFLoader`\n",
    "\n",
    "üìò Loads text from PDFs (using `pdfplumber`, `PyMuPDF`, or `pdfminer`).\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"sample.pdf\")\n",
    "docs = loader.load()\n",
    "```\n",
    "\n",
    "‚úÖ When to Use:\n",
    "\n",
    "* Academic papers\n",
    "* Reports, manuals, scanned documents\n",
    "\n",
    "üìå Alternative:\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import PDFMinerLoader, PDFPlumberLoader\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üî∏ C. `WebBaseLoader`\n",
    "\n",
    "üìò Loads content from a web page URL.\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://en.wikipedia.org/wiki/LangChain\")\n",
    "docs = loader.load()\n",
    "```\n",
    "\n",
    "‚úÖ When to Use:\n",
    "\n",
    "* Knowledge scraping from websites\n",
    "* Wikipedia, blogs, documentation\n",
    "\n",
    "üìå Uses `requests + BeautifulSoup` for HTML parsing.\n",
    "\n",
    "---\n",
    "\n",
    "#### üî∏ D. `UnstructuredLoader` (via Unstructured.io)\n",
    "\n",
    "üìò Parses complex layouts like tables, headers, lists, multi-column PDFs.\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader\n",
    "\n",
    "loader = UnstructuredFileLoader(\"invoice.pdf\")\n",
    "docs = loader.load()\n",
    "```\n",
    "\n",
    "‚úÖ When to Use:\n",
    "\n",
    "* You need semantic parsing beyond plain text\n",
    "* Ideal for enterprise docs, forms, and tables\n",
    "\n",
    "üîÅ Requires: `pip install unstructured`\n",
    "\n",
    "---\n",
    "\n",
    "#### üî∏ E. `PlaywrightURLLoader`\n",
    "\n",
    "üìò Uses a headless browser to load and render dynamic JavaScript-heavy websites (like React apps).\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import PlaywrightURLLoader\n",
    "\n",
    "loader = PlaywrightURLLoader([\"https://example.com\"])\n",
    "docs = loader.load()\n",
    "```\n",
    "\n",
    "‚úÖ When to Use:\n",
    "\n",
    "* Pages that require scrolling, clicking, or dynamic rendering\n",
    "* Websites with SPA (single-page apps)\n",
    "\n",
    "üîÅ Requires: `playwright install`\n",
    "\n",
    "---\n",
    "\n",
    "### 3. ‚úÖ Document Format\n",
    "\n",
    "All loaders output:\n",
    "\n",
    "```python\n",
    "[\n",
    "  Document(\n",
    "    page_content=\"Text goes here...\",\n",
    "    metadata={\"source\": \"example.pdf\", ...}\n",
    "  )\n",
    "]\n",
    "```\n",
    "\n",
    "‚úÖ You can access both the **text** and **where it came from** ‚Äî important for retrieval and citation.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. ‚úÖ Use Cases, Pros, Cons\n",
    "\n",
    "| Use Case            | Loader                            |\n",
    "| ------------------- | --------------------------------- |\n",
    "| Ingest web articles | `WebBaseLoader`                   |\n",
    "| Academic PDFs       | `PDFLoader`, `UnstructuredLoader` |\n",
    "| Dynamic JS content  | `PlaywrightLoader`                |\n",
    "| Plain files         | `TextLoader`                      |\n",
    "\n",
    "---\n",
    "\n",
    "| ‚úÖ Advantages                | ‚ùå Disadvantages                   |\n",
    "| --------------------------- | --------------------------------- |\n",
    "| Supports all common formats | Parsing quality depends on loader |\n",
    "| Pluggable and chainable     | Some need extra dependencies      |\n",
    "| Includes source metadata    | Dynamic content can break parsers |\n",
    "\n",
    "---\n",
    "\n",
    "### 5. ‚úÖ Best Practices\n",
    "\n",
    "| Tip                                                       | Why                            |\n",
    "| --------------------------------------------------------- | ------------------------------ |\n",
    "| Use metadata fields like `source`                         | Helps retrieval/citation later |\n",
    "| Preprocess long PDFs with splitters                       | Avoids context overflow        |\n",
    "| Use `UnstructuredLoader` for quality layout-aware parsing | Better than naive loaders      |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# ‚úÇÔ∏è **2. Text Splitters**\n",
    "\n",
    "*(Part of üü† III. Data-Aware / RAG Systems)*\n",
    "\n",
    "---\n",
    "\n",
    "### 1. ‚úÖ Definition\n",
    "\n",
    "**Text Splitters** in LangChain break large texts into smaller, manageable **chunks** that fit into LLM context limits and can be indexed into vector databases.\n",
    "\n",
    "They preserve semantic meaning while optimizing for token length, chunk overlap, and relevance.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. ‚úÖ Types & Built-in Splitters\n",
    "\n",
    "---\n",
    "\n",
    "#### üî∏ A. `RecursiveCharacterTextSplitter` ‚úÖ *(Most Recommended)*\n",
    "\n",
    "üìò **Smart splitting** that recursively splits text by paragraph ‚Üí sentence ‚Üí word ‚Üí character boundaries until it fits desired chunk size.\n",
    "\n",
    "```python\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "chunks = splitter.split_documents(documents)\n",
    "```\n",
    "\n",
    "‚úÖ Best for:\n",
    "\n",
    "* Most RAG tasks\n",
    "* Unstructured docs with mixed content\n",
    "* Balances chunk size + coherence\n",
    "\n",
    "---\n",
    "\n",
    "#### üî∏ B. `CharacterTextSplitter`\n",
    "\n",
    "üìò Naively splits text by a single character (like `\\\\n` or space).\n",
    "\n",
    "```python\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "splitter = CharacterTextSplitter(\n",
    "    separator=\"\\\\n\\\\n\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "```\n",
    "\n",
    "‚úÖ Best for:\n",
    "\n",
    "* Simple use cases\n",
    "* Controlled formats (e.g., logs, articles)\n",
    "\n",
    "‚ö†Ô∏è Limitation: No semantic fallback if text isn't cleanly separated.\n",
    "\n",
    "---\n",
    "\n",
    "#### üî∏ C. `SentenceTransformersTokenTextSplitter`\n",
    "\n",
    "üìò Splits text based on **tokens**, not characters ‚Äî uses HuggingFace tokenizer.\n",
    "\n",
    "```python\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "\n",
    "splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=50, tokens_per_chunk=400)\n",
    "```\n",
    "\n",
    "‚úÖ Best for:\n",
    "\n",
    "* Fine-tuned control by token count\n",
    "* Embedding-quality-sensitive tasks\n",
    "\n",
    "---\n",
    "\n",
    "#### üî∏ D. `NLTKTextSplitter` or `SpacyTextSplitter`\n",
    "\n",
    "üìò Splits using **natural language rules**: paragraphs, sentences (requires NLTK or spaCy).\n",
    "\n",
    "```python\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "\n",
    "splitter = NLTKTextSplitter()\n",
    "```\n",
    "\n",
    "‚úÖ Best for:\n",
    "\n",
    "* Academic documents\n",
    "* News, legal, medical use cases\n",
    "\n",
    "---\n",
    "\n",
    "### 3. ‚úÖ Chunk Overlap (Best Practices)\n",
    "\n",
    "| Parameter       | Why It Matters                                                               |\n",
    "| --------------- | ---------------------------------------------------------------------------- |\n",
    "| `chunk_size`    | How much content fits into one chunk ‚Äî balance between detail and token cost |\n",
    "| `chunk_overlap` | How much overlap between chunks ‚Äî helps with context continuity              |\n",
    "\n",
    "‚úÖ **Recommended Settings**:\n",
    "\n",
    "```python\n",
    "chunk_size = 500\n",
    "chunk_overlap = 100\n",
    "```\n",
    "\n",
    "üìå Why overlap?\n",
    "\n",
    "> To ensure that related information isn't split between chunks and lost during retrieval.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. ‚úÖ Use Cases, Advantages, Limitations\n",
    "\n",
    "| Use Case            | Splitter                         |\n",
    "| ------------------- | -------------------------------- |\n",
    "| Raw PDFs, web pages | `RecursiveCharacterTextSplitter` |\n",
    "| Logs, clean text    | `CharacterTextSplitter`          |\n",
    "| Token-precise tasks | `TokenTextSplitter`              |\n",
    "| Sentence integrity  | `NLTK`, `SpaCy`                  |\n",
    "\n",
    "| ‚úÖ Advantages                    | ‚ùå Disadvantages                           |\n",
    "| ------------------------------- | ----------------------------------------- |\n",
    "| Optimizes for LLM limits        | Poorly tuned parameters = loss of context |\n",
    "| Preserves semantic meaning      | Token-aware splitting needs setup         |\n",
    "| Supports overlap for better RAG | Needs experimentation for best chunking   |\n",
    "\n",
    "---\n",
    "\n",
    "### 5. ‚úÖ Example: Load ‚Üí Split ‚Üí Store\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load\n",
    "docs = TextLoader(\"example.txt\").load()\n",
    "\n",
    "# Split\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "chunks = splitter.split_documents(docs)\n",
    "```\n",
    "\n",
    "Each `chunk` will have:\n",
    "\n",
    "```python\n",
    "Document(\n",
    "  page_content=\"This is a small section of the full doc...\",\n",
    "  metadata={\"source\": \"example.txt\"}\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# üß¨ **3. Embedding Models**\n",
    "\n",
    "*(Part of üü† III. Data-Aware / RAG Systems)*\n",
    "\n",
    "---\n",
    "\n",
    "### 1. ‚úÖ Definition\n",
    "\n",
    "**Embedding Models** convert text (chunks, queries, documents) into **high-dimensional vectors** (lists of floats) that capture semantic meaning. These vectors are used for **similarity search** in vector databases.\n",
    "\n",
    "LangChain supports several pluggable embedding providers (OpenAI, Cohere, HuggingFace, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. ‚úÖ Common Embedding Providers & Imports\n",
    "\n",
    "---\n",
    "\n",
    "| Model                      | Import                                                             | Use Case                          |\n",
    "| -------------------------- | ------------------------------------------------------------------ | --------------------------------- |\n",
    "| üîπ `OpenAIEmbeddings`      | `from langchain_openai import OpenAIEmbeddings`                    | Cloud, accurate, default for GPT  |\n",
    "| üîπ `HuggingFaceEmbeddings` | `from langchain_community.embeddings import HuggingFaceEmbeddings` | Local/offline use, open models    |\n",
    "| üîπ `CohereEmbeddings`      | `from langchain_cohere import CohereEmbeddings`                    | Alternative to OpenAI, commercial |\n",
    "| üîπ `GooglePalmEmbeddings`  | `from langchain_google_genai import GooglePalmEmbeddings`          | Google‚Äôs embedding APIs           |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. ‚úÖ How to Create Embeddings\n",
    "\n",
    "---\n",
    "\n",
    "#### üî∏ A. OpenAI (Default & Most Common)\n",
    "\n",
    "```python\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embedder = OpenAIEmbeddings()\n",
    "vectors = embedder.embed_documents([\"Apple is a fruit\", \"Google is a company\"])\n",
    "```\n",
    "\n",
    "‚úÖ Best for:\n",
    "\n",
    "* Plug-and-play with GPT LLMs\n",
    "* Supports OpenAI vector DBs like Pinecone, Weaviate, etc.\n",
    "\n",
    "---\n",
    "\n",
    "#### üî∏ B. HuggingFace (Local Embeddings)\n",
    "\n",
    "```python\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectors = embedder.embed_documents([\"Apple is a fruit\", \"Google is a company\"])\n",
    "```\n",
    "\n",
    "‚úÖ Best for:\n",
    "\n",
    "* Local/offline inference\n",
    "* Fine-tuned open-source tasks\n",
    "\n",
    "üì¶ Requires: `transformers`, `sentence-transformers`\n",
    "\n",
    "---\n",
    "\n",
    "#### üî∏ C. Cohere\n",
    "\n",
    "```python\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "\n",
    "embedder = CohereEmbeddings()\n",
    "vectors = embedder.embed_documents([\"LangChain is great\"])\n",
    "```\n",
    "\n",
    "‚úÖ Best for:\n",
    "\n",
    "* Commercial alternatives to OpenAI\n",
    "* Good multilingual support\n",
    "\n",
    "---\n",
    "\n",
    "### 4. ‚úÖ Normalization (Best Practice)\n",
    "\n",
    "To ensure **consistent cosine similarity** search, **normalize** vectors.\n",
    "\n",
    "LangChain auto-normalizes in some vector stores (like FAISS), but you can also do:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "normalized = [v / np.linalg.norm(v) for v in vectors]\n",
    "```\n",
    "\n",
    "üìå Helps with:\n",
    "\n",
    "* Consistent similarity ranking\n",
    "* Avoiding dimensional bias\n",
    "\n",
    "---\n",
    "\n",
    "### 5. ‚úÖ Use Cases, Pros, Cons\n",
    "\n",
    "| Use Case                | Example                              |\n",
    "| ----------------------- | ------------------------------------ |\n",
    "| RAG Search              | Embed + store chunks in a vector DB  |\n",
    "| Semantic Matching       | Search docs, FAQs, knowledge base    |\n",
    "| Cross-lingual retrieval | Multilingual embeddings (Cohere, HF) |\n",
    "\n",
    "| ‚úÖ Advantages                  | ‚ùå Disadvantages                           |\n",
    "| ----------------------------- | ----------------------------------------- |\n",
    "| Encodes semantic meaning      | Quality depends on model                  |\n",
    "| Plug-and-play with many LLMs  | Embeddings are opaque (hard to interpret) |\n",
    "| Supports both cloud and local | Some models require GPU locally           |\n",
    "\n",
    "---\n",
    "\n",
    "### 6. ‚úÖ Best Practices\n",
    "\n",
    "| Practice                                     | Why It‚Äôs Useful                            |\n",
    "| -------------------------------------------- | ------------------------------------------ |\n",
    "| Use the **same embedder** for docs & queries | Ensures vector space alignment             |\n",
    "| Use **chunk metadata** with vectors          | Helps with filtering (e.g., source, topic) |\n",
    "| Normalize vectors for cosine distance        | Prevents score bias                        |\n",
    "\n",
    "---\n",
    "\n",
    "### 7. ‚úÖ Example: Embed and Prepare for Vector Store\n",
    "\n",
    "```python\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "embedder = OpenAIEmbeddings()\n",
    "\n",
    "# Embed text chunks\n",
    "vectors = embedder.embed_documents([\"chunk 1\", \"chunk 2\", \"chunk 3\"])\n",
    "\n",
    "# Save into vector DB (e.g., FAISS)\n",
    "db = FAISS.from_texts([\"chunk 1\", \"chunk 2\", \"chunk 3\"], embedding=embedder)\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# üß≤ **4. Vector Stores**\n",
    "\n",
    "*(Part of üü† III. Data-Aware / RAG Systems)*\n",
    "\n",
    "---\n",
    "\n",
    "### 1. ‚úÖ Definition\n",
    "\n",
    "**Vector Stores** are specialized databases for storing and retrieving **embedding vectors** based on similarity (cosine, dot product, etc.).\n",
    "\n",
    "In LangChain, they store chunked documents and allow fast **semantic search** ‚Äî finding the most relevant chunks for a query.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. ‚úÖ Supported Vector Stores & Imports\n",
    "\n",
    "| Vector Store  | Import                                        | Key Trait                                        |\n",
    "| ------------- | --------------------------------------------- | ------------------------------------------------ |\n",
    "| üîπ `FAISS`    | `from langchain.vectorstores import FAISS`    | Local, lightweight, fast                         |\n",
    "| üîπ `Chroma`   | `from langchain.vectorstores import Chroma`   | Local/embedded DB, persistent                    |\n",
    "| üîπ `Qdrant`   | `from langchain.vectorstores import Qdrant`   | Open-source, scalable, metadata filtering        |\n",
    "| üîπ `Weaviate` | `from langchain.vectorstores import Weaviate` | Cloud/local, hybrid search                       |\n",
    "| üîπ `Pinecone` | `from langchain.vectorstores import Pinecone` | Managed service, powerful filtering, cloud-scale |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. ‚úÖ Adding Documents to Vector Store\n",
    "\n",
    "All vector stores follow the same pattern:\n",
    "\n",
    "```python\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "texts = [\"Doc1 content\", \"Doc2 content\"]\n",
    "metadatas = [{\"source\": \"pdf1\"}, {\"source\": \"pdf2\"}]\n",
    "embedding = OpenAIEmbeddings()\n",
    "\n",
    "db = FAISS.from_texts(texts, embedding=embedding, metadatas=metadatas)\n",
    "```\n",
    "\n",
    "‚úÖ Each `Document` contains:\n",
    "\n",
    "* `page_content` (the chunk text)\n",
    "* `metadata` (source, id, topic, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "### 4. ‚úÖ Searching (Retrieval)\n",
    "\n",
    "```python\n",
    "retrieved_docs = db.similarity_search(\"What is LangChain?\", k=3)\n",
    "```\n",
    "\n",
    "* `similarity_search(query, k)`: Basic nearest neighbor search\n",
    "* `similarity_search_with_score()`: Returns results + similarity scores\n",
    "\n",
    "---\n",
    "\n",
    "### 5. ‚úÖ Metadata Filtering (Advanced)\n",
    "\n",
    "Only supported by vector stores like **Chroma**, **Qdrant**, **Pinecone**, and **Weaviate**.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "results = db.similarity_search(\n",
    "  query=\"Show me AWS docs\",\n",
    "  filter={\"source\": \"aws_docs\"},\n",
    "  k=2\n",
    ")\n",
    "```\n",
    "\n",
    "‚úÖ Use this to:\n",
    "\n",
    "* Filter by document source\n",
    "* Restrict by author, domain, timestamp, etc.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. ‚úÖ Persistence (Save & Load)\n",
    "\n",
    "#### üîπ FAISS:\n",
    "\n",
    "```python\n",
    "db.save_local(\"faiss_index/\")\n",
    "db = FAISS.load_local(\"faiss_index/\", embedding=OpenAIEmbeddings())\n",
    "```\n",
    "\n",
    "#### üîπ Chroma:\n",
    "\n",
    "```python\n",
    "db = Chroma(persist_directory=\"db/\", embedding_function=embedding)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 7. ‚úÖ When to Use Which?\n",
    "\n",
    "| Store      | Best For                    | Offline?  | Metadata Filtering? |\n",
    "| ---------- | --------------------------- | --------- | ------------------- |\n",
    "| `FAISS`    | Local dev, fast, simple     | ‚úÖ         | ‚ùå                   |\n",
    "| `Chroma`   | Lightweight, persistent     | ‚úÖ         | ‚úÖ                   |\n",
    "| `Qdrant`   | Scalable + filters          | ‚úÖ/‚òÅÔ∏è      | ‚úÖ                   |\n",
    "| `Weaviate` | Hybrid (semantic + keyword) | ‚úÖ/‚òÅÔ∏è      | ‚úÖ                   |\n",
    "| `Pinecone` | Enterprise RAG at scale     | ‚ùå (cloud) | ‚úÖ (rich support)    |\n",
    "\n",
    "---\n",
    "\n",
    "### 8. ‚úÖ Use Cases, Pros, Cons\n",
    "\n",
    "| Use Case               | Store                            |\n",
    "| ---------------------- | -------------------------------- |\n",
    "| Local notebooks, demos | `FAISS`, `Chroma`                |\n",
    "| Long-term persistence  | `Chroma`, `Qdrant`               |\n",
    "| Filtered search        | `Qdrant`, `Weaviate`, `Pinecone` |\n",
    "\n",
    "| ‚úÖ Advantages               | ‚ùå Limitations                     |\n",
    "| -------------------------- | --------------------------------- |\n",
    "| Fast semantic retrieval    | Most don‚Äôt support hybrid ranking |\n",
    "| Scalable + pluggable       | Must tune chunking for quality    |\n",
    "| Filters + metadata support | FAISS lacks filtering             |\n",
    "\n",
    "---\n",
    "\n",
    "### 9. ‚úÖ Example: Full Pipeline\n",
    "\n",
    "```python\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# Load + Split\n",
    "docs = TextLoader(\"book.txt\").load()\n",
    "chunks = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50).split_documents(docs)\n",
    "\n",
    "# Embed + Store\n",
    "db = FAISS.from_documents(chunks, OpenAIEmbeddings())\n",
    "\n",
    "# Search\n",
    "results = db.similarity_search(\"What is the main idea?\", k=2)\n",
    "print(results[0].page_content)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# üîç **5. Retrievers**\n",
    "\n",
    "*(Part of üü† III. Data-Aware / RAG Systems)*\n",
    "\n",
    "---\n",
    "\n",
    "### 1. ‚úÖ Definition\n",
    "\n",
    "**Retrievers** are LangChain wrappers around vector stores (or other sources) that **fetch the most relevant chunks** for a user query, based on semantic similarity or enhanced logic.\n",
    "\n",
    "They‚Äôre used in **RAG pipelines** to dynamically retrieve the right documents for the LLM to generate grounded answers.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. ‚úÖ Built-in Retriever Types & Usage\n",
    "\n",
    "---\n",
    "\n",
    "#### üî∏ A. `VectorStoreRetriever`\n",
    "\n",
    "üìò Wraps a vector store like FAISS, Chroma, etc., for basic similarity search.\n",
    "\n",
    "```python\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
    "docs = retriever.invoke(\"What is LangChain?\")\n",
    "```\n",
    "\n",
    "‚úÖ Use when:\n",
    "\n",
    "* You want simple top-k vector search\n",
    "* No additional filtering or logic required\n",
    "\n",
    "---\n",
    "\n",
    "#### üî∏ B. `MultiQueryRetriever`\n",
    "\n",
    "üìò Generates **multiple reworded queries** using an LLM ‚Üí retrieves documents using each ‚Üí aggregates results.\n",
    "\n",
    "```python\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=db.as_retriever(),\n",
    "    llm=ChatOpenAI()\n",
    ")\n",
    "docs = retriever.invoke(\"Tell me about LangChain agents\")\n",
    "```\n",
    "\n",
    "‚úÖ Use when:\n",
    "\n",
    "* Your query might be ambiguous or have multiple angles\n",
    "* You want to maximize recall (get more relevant info)\n",
    "\n",
    "üß† Example:\n",
    "For `\"LangChain memory\"`, it might generate:\n",
    "\n",
    "* \"How does LangChain store history?\"\n",
    "* \"What is ConversationMemory in LangChain?\"\n",
    "\n",
    "---\n",
    "\n",
    "#### üî∏ C. `ContextualCompressionRetriever`\n",
    "\n",
    "üìò Retrieves documents ‚Üí compresses them using an LLM ‚Üí returns only the **most relevant parts** of each chunk.\n",
    "\n",
    "```python\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "compressor = LLMChainExtractor.from_llm(ChatOpenAI())\n",
    "retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=db.as_retriever()\n",
    ")\n",
    "```\n",
    "\n",
    "‚úÖ Use when:\n",
    "\n",
    "* Chunks are too long, noisy, or redundant\n",
    "* You want to **refine retrieved data** before sending it to the LLM\n",
    "\n",
    "---\n",
    "\n",
    "#### üî∏ D. `BM25Retriever`, `ParentDocumentRetriever`, etc.\n",
    "\n",
    "Other advanced retrievers (optional):\n",
    "\n",
    "* `BM25Retriever`: Keyword-based retrieval using TF-IDF\n",
    "* `ParentDocumentRetriever`: Maps chunk ‚Üí parent doc for full context\n",
    "* `EnsembleRetriever`: Combines multiple retrievers (e.g., BM25 + vector)\n",
    "\n",
    "---\n",
    "\n",
    "### 3. ‚úÖ Use Cases, Pros, and Cons\n",
    "\n",
    "| Use Case                   | Retriever                        |\n",
    "| -------------------------- | -------------------------------- |\n",
    "| Simple FAQ / docs          | `VectorStoreRetriever`           |\n",
    "| Complex or broad questions | `MultiQueryRetriever`            |\n",
    "| Large, verbose chunks      | `ContextualCompressionRetriever` |\n",
    "| Legal/long docs            | `ParentDocumentRetriever`        |\n",
    "\n",
    "| ‚úÖ Advantages                                  | ‚ùå Limitations                           |\n",
    "| --------------------------------------------- | --------------------------------------- |\n",
    "| Simple API to connect LLMs to relevant data   | Overlap/redundancy if not chunked well  |\n",
    "| Easy to combine, extend, compose              | LLM-based retrievers increase latency   |\n",
    "| Contextual compression = smaller prompt sizes | Multi-query can retrieve too much noise |\n",
    "\n",
    "---\n",
    "\n",
    "### 4. ‚úÖ Best Practices\n",
    "\n",
    "| Practice                                        | Why                                                   |\n",
    "| ----------------------------------------------- | ----------------------------------------------------- |\n",
    "| Tune `chunk_size` + `k`                         | Prevents under/over-retrieving                        |\n",
    "| Use `MultiQueryRetriever` in open-ended QA      | Improves recall                                       |\n",
    "| Use `ContextualCompressionRetriever` with GPT-4 | Gives focused context                                 |\n",
    "| Combine `metadata filters`                      | Useful for document segmentation by source/topic/date |\n",
    "\n",
    "---\n",
    "\n",
    "### 5. ‚úÖ Example RAG Flow with Retriever\n",
    "\n",
    "```python\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "# Connect retriever to chain\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(),\n",
    "    retriever=retriever\n",
    ")\n",
    "\n",
    "response = qa_chain.invoke({\"query\": \"Explain memory in LangChain\"})\n",
    "print(response['result'])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6. ‚úÖ Bonus: When to Use What?\n",
    "\n",
    "| Retriever                        | Best With               | Key Strength          |\n",
    "| -------------------------------- | ----------------------- | --------------------- |\n",
    "| `VectorStoreRetriever`           | All vector stores       | Fast, simple          |\n",
    "| `MultiQueryRetriever`            | GPT-4/Claude            | Deep recall           |\n",
    "| `ContextualCompressionRetriever` | GPT-4                   | Clean + short results |\n",
    "| `BM25Retriever`                  | Legal, code, structured | Exact matches         |\n",
    "| `ParentDocumentRetriever`        | Large docs              | Full-document context |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# üîó **6. Retrieval Chains**\n",
    "\n",
    "*(Part of üü† III. Data-Aware / RAG Systems)*\n",
    "\n",
    "---\n",
    "\n",
    "### 1. ‚úÖ Definition\n",
    "\n",
    "**Retrieval Chains** are LangChain pipelines that combine:\n",
    "\n",
    "* üîç **Retriever** (fetch relevant documents)\n",
    "* üí¨ **LLM** (answer question using those docs)\n",
    "* üß† Optionally: **memory, search tools, filters**\n",
    "\n",
    "This forms a complete **RAG (Retrieval-Augmented Generation)** system.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. ‚úÖ Built-in Retrieval Chains\n",
    "\n",
    "---\n",
    "\n",
    "#### üî∏ A. `RetrievalQA`\n",
    "\n",
    "üìò Basic Question Answering over documents using retrieval.\n",
    "\n",
    "```python\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(),\n",
    "    retriever=retriever,        # any retriever\n",
    "    chain_type=\"stuff\",         # how docs are passed into prompt\n",
    ")\n",
    "```\n",
    "\n",
    "‚úÖ Use when:\n",
    "\n",
    "* You need simple, fast document QA\n",
    "* No chat memory required\n",
    "\n",
    "---\n",
    "\n",
    "#### üî∏ B. `ConversationalRetrievalChain`\n",
    "\n",
    "üìò A multi-turn chat-style retrieval chain with **memory support**.\n",
    "\n",
    "```python\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "chat_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=ChatOpenAI(),\n",
    "    retriever=retriever,\n",
    "    memory=ConversationBufferMemory(return_messages=True)\n",
    ")\n",
    "```\n",
    "\n",
    "‚úÖ Use when:\n",
    "\n",
    "* You want a chatbot experience with history\n",
    "* Follow-up questions, clarification, etc.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. ‚úÖ How Chains Work Internally\n",
    "\n",
    "For both chains:\n",
    "\n",
    "1. User asks a question\n",
    "2. Retriever fetches relevant chunks\n",
    "3. Prompt template fills in question + context\n",
    "4. LLM answers using only the retrieved data\n",
    "\n",
    "> üìå Prompt style depends on `chain_type`:\n",
    "\n",
    "* `\"stuff\"` ‚Äì simply concatenates all docs\n",
    "* `\"map_reduce\"` ‚Äì processes chunks individually then summarizes\n",
    "* `\"refine\"` ‚Äì incrementally improves answer using each doc\n",
    "\n",
    "---\n",
    "\n",
    "### 4. ‚úÖ Adding Tools, Memory, Filters\n",
    "\n",
    "| Feature                | How                                                                              |\n",
    "| ---------------------- | -------------------------------------------------------------------------------- |\n",
    "| ‚úÖ Memory               | Use `ConversationalRetrievalChain(memory=...)`                                   |\n",
    "| ‚úÖ Metadata filters     | Use `retriever = db.as_retriever(search_kwargs={\"filter\": {\"source\": \"file1\"}})` |\n",
    "| ‚úÖ Custom prompts       | Use `RetrievalQA.from_chain_type(..., chain_type_kwargs={\"prompt\": my_prompt})`  |\n",
    "| ‚úÖ Tool-enhanced agents | Combine `retriever` with agent tools in `initialize_agent()`                     |\n",
    "\n",
    "---\n",
    "\n",
    "### 5. ‚úÖ Use Cases, Pros & Cons\n",
    "\n",
    "| Use Case                      | Chain                          |\n",
    "| ----------------------------- | ------------------------------ |\n",
    "| Static document QA            | `RetrievalQA`                  |\n",
    "| Long-form chatbot             | `ConversationalRetrievalChain` |\n",
    "| Code, legal, or PDF assistant | Either + memory                |\n",
    "\n",
    "| ‚úÖ Advantages                        | ‚ùå Limitations                               |\n",
    "| ----------------------------------- | ------------------------------------------- |\n",
    "| Easy to plug into any retriever     | RAG quality depends on retrieval relevance  |\n",
    "| Flexible (LLM + retriever + prompt) | Need to manage chunk size, overlap, filters |\n",
    "| Supports memory & tools             | Some chain types (e.g. refine) are slow     |\n",
    "\n",
    "---\n",
    "\n",
    "### 6. ‚úÖ Example: Build a RAG Chatbot\n",
    "\n",
    "```python\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=ChatOpenAI(),\n",
    "    retriever=retriever,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "chat_history = []\n",
    "question = \"What is LangChain memory?\"\n",
    "\n",
    "result = chain.invoke({\"question\": question, \"chat_history\": chat_history})\n",
    "print(result['answer'])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 7. ‚úÖ Best Practices\n",
    "\n",
    "| Tip                                             | Why                           |\n",
    "| ----------------------------------------------- | ----------------------------- |\n",
    "| Tune `chunk_size`, `k`, and `chain_type`        | Impacts retrieval relevance   |\n",
    "| Use `ConversationalRetrievalChain` for chatbots | Tracks conversation           |\n",
    "| Use metadata filters with retrievers            | More precise control          |\n",
    "| Try `map_reduce` for long docs                  | Better summarization accuracy |\n",
    "\n",
    "---\n",
    "\n",
    "### 8. ‚úÖ When to Use What\n",
    "\n",
    "| Chain                          | Use When              | Memory   |\n",
    "| ------------------------------ | --------------------- | -------- |\n",
    "| `RetrievalQA`                  | One-shot Q\\&A         | ‚ùå        |\n",
    "| `ConversationalRetrievalChain` | Chatbot w/ follow-ups | ‚úÖ        |\n",
    "| `Agent + Retriever`            | Tools + search + QA   | Optional |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# üîµ **1. LangGraph (Stateful Flows)**\n",
    "\n",
    "*(Part of IV. Advanced Topics)*\n",
    "\n",
    "---\n",
    "\n",
    "### 1. ‚úÖ Definition\n",
    "\n",
    "**LangGraph** is a LangChain-native library for building **stateful, multi-step, and multi-agent workflows** using a **graph-based architecture**.\n",
    "\n",
    "It‚Äôs ideal for:\n",
    "\n",
    "* üß† **Multi-agent systems**\n",
    "* üîÅ **Dynamic tool/step routing**\n",
    "* üóÇÔ∏è **Persistent state & memory**\n",
    "* üîß **Workflow orchestration with logic control**\n",
    "\n",
    "LangGraph = `LangChain + NetworkX + Async FSM (finite-state machine)`\n",
    "\n",
    "---\n",
    "\n",
    "### 2. ‚úÖ Key Concepts\n",
    "\n",
    "| Concept   | Meaning                             |\n",
    "| --------- | ----------------------------------- |\n",
    "| **Nodes** | LLMs, tools, functions, agents      |\n",
    "| **Edges** | Transitions between nodes           |\n",
    "| **Graph** | A full pipeline with multiple paths |\n",
    "| **State** | Memory/context passed along edges   |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. ‚úÖ Installation\n",
    "\n",
    "```bash\n",
    "pip install langgraph\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. ‚úÖ Example Use Case: Multi-Agent Collaboration\n",
    "\n",
    "> Build a **multi-agent research assistant**:\n",
    "> One agent searches the web, one summarizes, one critiques.\n",
    "\n",
    "#### Step-by-Step\n",
    "\n",
    "```python\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# 1. Define state schema\n",
    "class ResearchState(TypedDict):\n",
    "    topic: str\n",
    "    result: str\n",
    "\n",
    "# 2. Define nodes (steps)\n",
    "search_node = RunnableLambda(lambda state: {\"result\": f\"Searching for {state['topic']}...\"})\n",
    "summarize_node = RunnableLambda(lambda state: {\"result\": f\"Summarized: {state['result']}\"})\n",
    "end_node = RunnableLambda(lambda state: state)\n",
    "\n",
    "# 3. Create graph\n",
    "graph = StateGraph(ResearchState)\n",
    "graph.add_node(\"search\", search_node)\n",
    "graph.add_node(\"summarize\", summarize_node)\n",
    "graph.set_entry_point(\"search\")\n",
    "graph.add_edge(\"search\", \"summarize\")\n",
    "graph.add_edge(\"summarize\", END)\n",
    "\n",
    "# 4. Compile and invoke\n",
    "app = graph.compile()\n",
    "output = app.invoke({\"topic\": \"LangChain agents\"})\n",
    "print(output)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. ‚úÖ Advanced: Conditional Routing (Dynamic Graphs)\n",
    "\n",
    "```python\n",
    "def route(state):\n",
    "    return \"summarize\" if \"summary\" not in state else END\n",
    "\n",
    "graph.add_conditional_edges(\"search\", route)\n",
    "```\n",
    "\n",
    "‚úÖ Useful for:\n",
    "\n",
    "* Deciding next step based on content\n",
    "* Building feedback loops, retries, approvals\n",
    "\n",
    "---\n",
    "\n",
    "### 6. ‚úÖ Multi-Agent Use Case\n",
    "\n",
    "You can wrap agents or chains as graph nodes:\n",
    "\n",
    "```python\n",
    "graph.add_node(\"coding_agent\", code_generation_chain)\n",
    "graph.add_node(\"reviewer\", reviewer_chain)\n",
    "```\n",
    "\n",
    "‚úÖ Example:\n",
    "\n",
    "* `coding_agent` ‚Üí generates code\n",
    "* `reviewer` ‚Üí checks logic & style\n",
    "* If rejected, loop back to `coding_agent`\n",
    "\n",
    "---\n",
    "\n",
    "### 7. ‚úÖ Use Cases, Pros, Cons\n",
    "\n",
    "| Use Case                  | Description                               |\n",
    "| ------------------------- | ----------------------------------------- |\n",
    "| Multi-agent collaboration | Researcher ‚Üî Summarizer ‚Üî Critic          |\n",
    "| Tool routers              | Route query to calculator, API, SQL, etc. |\n",
    "| Conditional task flows    | Based on metadata, user input, memory     |\n",
    "| Complex workflows         | Like Airflow but LLM-aware                |\n",
    "\n",
    "| ‚úÖ Pros                       | ‚ùå Cons                        |\n",
    "| ---------------------------- | ----------------------------- |\n",
    "| Precise control of LLM flows | Slightly complex setup        |\n",
    "| Real-time dynamic routing    | Async required for full power |\n",
    "| Easy to debug & visualize    | Limited built-in visualizers  |\n",
    "\n",
    "---\n",
    "\n",
    "### 8. ‚úÖ LangGraph + Memory\n",
    "\n",
    "LangGraph supports **custom shared state** across nodes ‚Äî like agent memories, counters, task logs, etc.\n",
    "\n",
    "You define a `TypedDict` or `BaseModel` for state and pass it through.\n",
    "\n",
    "```python\n",
    "class TaskState(TypedDict):\n",
    "    query: str\n",
    "    history: List[str]\n",
    "```\n",
    "\n",
    "‚úÖ Keeps LLMs **stateful and aware** in multi-step settings.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. ‚úÖ Best Practices\n",
    "\n",
    "| Tip                                  | Reason                               |\n",
    "| ------------------------------------ | ------------------------------------ |\n",
    "| Use `RunnableLambda` for small steps | Quick test/debug logic               |\n",
    "| Define a clear state schema          | Prevents bugs + improves readability |\n",
    "| Add retries & fallback nodes         | Improves robustness                  |\n",
    "| Mix tools + chains + agents          | Build real-world pipelines           |\n",
    "\n",
    "---\n",
    "\n",
    "### 10. ‚úÖ Summary\n",
    "\n",
    "| Feature                     | Benefit                          |\n",
    "| --------------------------- | -------------------------------- |\n",
    "| üåê Graph-based routing      | Complex flows made easy          |\n",
    "| üß† Shared memory            | Real multi-agent reasoning       |\n",
    "| üîÅ Loops & conditionals     | Flexible, intelligent automation |\n",
    "| ‚öôÔ∏è Plug & play chains/tools | Modular workflows                |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# ‚ö° **2. Async & Streaming**\n",
    "\n",
    "*(Part of üîµ IV. Advanced Topics)*\n",
    "\n",
    "---\n",
    "\n",
    "### 1. ‚úÖ Definition\n",
    "\n",
    "LangChain supports:\n",
    "\n",
    "* **Async LLM calls** ‚Üí handle multiple LLM operations concurrently\n",
    "* **Streaming outputs** ‚Üí get tokens as they‚Äôre generated in real time\n",
    "* **Event-driven workflows** ‚Üí trigger updates/UI responses as the model \"thinks\"\n",
    "\n",
    "These improve **speed**, **interactivity**, and **real-time user experience** ‚Äî especially in agents, chatbots, and UI-integrated apps (like Streamlit, Gradio).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. ‚úÖ Async LLM Calls\n",
    "\n",
    "---\n",
    "\n",
    "#### üî∏ Why Use Async?\n",
    "\n",
    "* Traditional sync (`.invoke()`) is blocking\n",
    "* Async allows parallel LLM calls (e.g., agents, tools, retrievals)\n",
    "\n",
    "#### üî∏ Example\n",
    "\n",
    "```python\n",
    "import asyncio\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "\n",
    "async def get_response():\n",
    "    result = await llm.ainvoke(\"What is LangChain?\")\n",
    "    print(result.content)\n",
    "\n",
    "asyncio.run(get_response())\n",
    "```\n",
    "\n",
    "‚úÖ Useful for:\n",
    "\n",
    "* **Concurrent** tool execution in agents\n",
    "* **Multiple user queries** in parallel\n",
    "\n",
    "---\n",
    "\n",
    "### 3. ‚úÖ Streaming Outputs (Token-by-Token Generation)\n",
    "\n",
    "---\n",
    "\n",
    "#### üî∏ Why Use Streaming?\n",
    "\n",
    "* Immediate feedback to the user\n",
    "* Great for chat interfaces and CLI tools\n",
    "* More engaging UX\n",
    "\n",
    "#### üî∏ With OpenAI Chat Models\n",
    "\n",
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatOpenAI(streaming=True)\n",
    "\n",
    "def stream_tokens():\n",
    "    stream = llm.pipe(StrOutputParser()).stream(\"Tell me a joke\")\n",
    "    for token in stream:\n",
    "        print(token, end=\"\", flush=True)\n",
    "\n",
    "stream_tokens()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. ‚úÖ Async + Streaming (Together)\n",
    "\n",
    "```python\n",
    "async def stream_async():\n",
    "    async for chunk in llm.astream(\"Give me a summary of LangChain\"):\n",
    "        print(chunk.content, end=\"\")\n",
    "\n",
    "asyncio.run(stream_async())\n",
    "```\n",
    "\n",
    "‚úÖ Best for:\n",
    "\n",
    "* Chatbots with typing effect\n",
    "* UI apps with progress animations\n",
    "* LLM dashboards\n",
    "\n",
    "---\n",
    "\n",
    "### 5. ‚úÖ Event-Driven Architecture in LangChain\n",
    "\n",
    "---\n",
    "\n",
    "LangChain supports **event hooks** using the `CallbackManager` system ‚Äî letting you capture and handle events like:\n",
    "\n",
    "* Token generation\n",
    "* Tool invocation\n",
    "* Chain start/end\n",
    "* Errors\n",
    "\n",
    "#### üî∏ Example: Custom Callback\n",
    "\n",
    "```python\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "\n",
    "class PrintCallbackHandler(BaseCallbackHandler):\n",
    "    def on_llm_new_token(self, token, **kwargs):\n",
    "        print(token, end=\"\")\n",
    "\n",
    "llm = ChatOpenAI(streaming=True, callbacks=[PrintCallbackHandler()])\n",
    "llm.invoke(\"Tell me a short poem about AI\")\n",
    "```\n",
    "\n",
    "‚úÖ Useful for:\n",
    "\n",
    "* Logging\n",
    "* Live dashboards\n",
    "* Tool monitoring\n",
    "\n",
    "---\n",
    "\n",
    "### 6. ‚úÖ Real-World Use Cases\n",
    "\n",
    "| Use Case                             | Feature Used                        |\n",
    "| ------------------------------------ | ----------------------------------- |\n",
    "| Chat app with token-by-token display | `stream=True`, `on_llm_new_token()` |\n",
    "| Dashboard that logs all actions      | Custom callbacks                    |\n",
    "| Agent with parallel tools            | `async/await` LLM + tools           |\n",
    "| Voice assistant                      | Streaming + async                   |\n",
    "\n",
    "---\n",
    "\n",
    "### 7. ‚úÖ Pros & Limitations\n",
    "\n",
    "| ‚úÖ Advantages                 | ‚ùå Limitations                     |\n",
    "| ---------------------------- | --------------------------------- |\n",
    "| Faster, responsive UX        | Some LLMs don‚Äôt support streaming |\n",
    "| Enables UI interactivity     | Async syntax more complex         |\n",
    "| Better tool/agent throughput | Callbacks require setup & testing |\n",
    "\n",
    "---\n",
    "\n",
    "### 8. ‚úÖ Best Practices\n",
    "\n",
    "| Tip                                | Why                              |\n",
    "| ---------------------------------- | -------------------------------- |\n",
    "| Use `stream=True` with UIs         | Feels live & responsive          |\n",
    "| Wrap chains in `async def`         | Enables concurrency              |\n",
    "| Combine with LangServe or FastAPI  | Build scalable APIs              |\n",
    "| Use callbacks for monitoring tools | Full observability over pipeline |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Summary Table\n",
    "\n",
    "| Feature           | Usage                        |\n",
    "| ----------------- | ---------------------------- |\n",
    "| `ainvoke()`       | Async LLM call               |\n",
    "| `astream()`       | Async + Streaming            |\n",
    "| `stream=True`     | Streaming in sync mode       |\n",
    "| `CallbackHandler` | Custom token/tool monitoring |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# üåê **3. LangServe (API Deployment)**\n",
    "\n",
    "*(Part of üîµ IV. Advanced Topics)*\n",
    "\n",
    "---\n",
    "\n",
    "### 1. ‚úÖ Definition\n",
    "\n",
    "**LangServe** is a LangChain-native framework for **deploying chains, agents, tools, or entire apps as RESTful APIs** using **FastAPI**.\n",
    "\n",
    "It also integrates tightly with **LangSmith** for:\n",
    "\n",
    "* Monitoring\n",
    "* Debugging\n",
    "* Tracing\n",
    "* Evaluation\n",
    "\n",
    "> ‚öôÔ∏è ‚ÄúLangServe = LangChain + FastAPI + Observability + AutoDocs‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "### 2. ‚úÖ Key Features\n",
    "\n",
    "| Feature                           | Description                      |\n",
    "| --------------------------------- | -------------------------------- |\n",
    "| üîÅ Auto-wraps any LangChain chain | into a FastAPI REST server       |\n",
    "| üìä Logs, traces, and metadata     | via LangSmith                    |\n",
    "| ‚ö° Async-ready & scalable          | for production deployment        |\n",
    "| üß™ Built-in testing/debug UI      | via `http://localhost:8000/docs` |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. ‚úÖ Installation\n",
    "\n",
    "```bash\n",
    "pip install langserve\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. ‚úÖ Quickstart: Serve a Chain as API\n",
    "\n",
    "```python\n",
    "# serve.py\n",
    "\n",
    "from langserve import add_routes\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from fastapi import FastAPI\n",
    "\n",
    "# 1. Define chain\n",
    "prompt = PromptTemplate.from_template(\"Write a poem about {topic}\")\n",
    "chain = LLMChain(llm=ChatOpenAI(), prompt=prompt)\n",
    "\n",
    "# 2. Create API app\n",
    "app = FastAPI()\n",
    "add_routes(app, chain, path=\"/poem\")\n",
    "\n",
    "# 3. Run server\n",
    "# ‚û§ uvicorn serve:app --reload\n",
    "```\n",
    "\n",
    "üåê Go to:\n",
    "`http://localhost:8000/poem/invoke`\n",
    "with body:\n",
    "\n",
    "```json\n",
    "{\"input\": {\"topic\": \"AI\"}}\n",
    "```\n",
    "\n",
    "‚úÖ It responds with an LLM-generated poem üéâ\n",
    "\n",
    "---\n",
    "\n",
    "### 5. ‚úÖ Supported Endpoints\n",
    "\n",
    "| Endpoint                      | Purpose                            |\n",
    "| ----------------------------- | ---------------------------------- |\n",
    "| `/invoke`                     | Run the chain                      |\n",
    "| `/stream`                     | Stream the result (token-by-token) |\n",
    "| `/batch`                      | Run a batch of inputs              |\n",
    "| `/config`, `/schema`, `/docs` | Auto-generated OpenAPI UI          |\n",
    "\n",
    "---\n",
    "\n",
    "### 6. ‚úÖ Example with Tool/Agent\n",
    "\n",
    "```python\n",
    "from langchain.agents import initialize_agent, load_tools\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "tools = load_tools([\"serpapi\"])  # e.g. Google Search\n",
    "agent = initialize_agent(tools, ChatOpenAI(), agent_type=\"zero-shot-react-description\")\n",
    "\n",
    "app = FastAPI()\n",
    "add_routes(app, agent, path=\"/agent\")\n",
    "```\n",
    "\n",
    "‚úÖ You‚Äôve now deployed a **multi-tool reasoning agent** as a fully usable REST API.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. ‚úÖ LangSmith + LangServe Integration\n",
    "\n",
    "LangServe supports LangSmith **by default** ‚Äî if your environment includes:\n",
    "\n",
    "```bash\n",
    "export LANGCHAIN_API_KEY=\"your_langsmith_api_key\"\n",
    "export LANGCHAIN_PROJECT=\"my-deployment\"\n",
    "```\n",
    "\n",
    "‚úÖ This enables:\n",
    "\n",
    "* Request/response tracing\n",
    "* Token usage stats\n",
    "* Latency metrics\n",
    "* Chain visualization\n",
    "\n",
    "---\n",
    "\n",
    "### 8. ‚úÖ Use Cases\n",
    "\n",
    "| Use Case                    | How LangServe Helps                        |\n",
    "| --------------------------- | ------------------------------------------ |\n",
    "| Deploy chatbot for frontend | Wrap `ConversationalRetrievalChain` as API |\n",
    "| Create semantic search API  | Wrap RAG retriever + LLM                   |\n",
    "| Deploy agent with tools     | Serve as `/agent` route                    |\n",
    "| QA over private docs        | Load vector store + chain ‚Üí serve it       |\n",
    "\n",
    "---\n",
    "\n",
    "### 9. ‚úÖ Pros & Cons\n",
    "\n",
    "| ‚úÖ Pros                        | ‚ùå Cons                                   |\n",
    "| ----------------------------- | ---------------------------------------- |\n",
    "| Very easy to deploy any chain | Not as customizable as full FastAPI apps |\n",
    "| Built-in async + streaming    | Still maturing (API shape may evolve)    |\n",
    "| Observability via LangSmith   | Requires good design to scale            |\n",
    "\n",
    "---\n",
    "\n",
    "### 10. ‚úÖ Example Full Workflow\n",
    "\n",
    "```bash\n",
    "uvicorn serve:app --reload\n",
    "```\n",
    "\n",
    "Request:\n",
    "\n",
    "```bash\n",
    "curl -X POST http://localhost:8000/poem/invoke \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"input\": {\"topic\": \"robots\"}}'\n",
    "```\n",
    "\n",
    "Response:\n",
    "\n",
    "```json\n",
    "{\"output\": \"A poem about robots and AI... ü§ñ\"}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 11. ‚úÖ Best Practices\n",
    "\n",
    "| Tip                                    | Reason                      |\n",
    "| -------------------------------------- | --------------------------- |\n",
    "| Use environment variables for API keys | Secure your deployment      |\n",
    "| Use `LangSmith` for observability      | See exactly what went wrong |\n",
    "| Add custom endpoints for status/logs   | Production-grade needs      |\n",
    "| Use `stream` endpoint in chat apps     | For smooth UX               |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Summary Table\n",
    "\n",
    "| Component                      | Purpose                       |\n",
    "| ------------------------------ | ----------------------------- |\n",
    "| `add_routes()`                 | Turns any LangChain into API  |\n",
    "| `/invoke`, `/stream`, `/batch` | Built-in endpoints            |\n",
    "| LangSmith integration          | Full tracing & logs           |\n",
    "| FastAPI-compatible             | Add your own routes if needed |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# üß† **4. Tracing with LangSmith**\n",
    "\n",
    "*(Part of üîµ IV. Advanced Topics)*\n",
    "\n",
    "---\n",
    "\n",
    "### 1. ‚úÖ Definition\n",
    "\n",
    "**LangSmith** is LangChain‚Äôs observability and evaluation platform. It helps developers:\n",
    "\n",
    "* ü™µ **Trace** every LLM call, tool, and chain step\n",
    "* üß™ **Debug** and replay errors\n",
    "* üìä **Analyze** token usage, latency, outputs\n",
    "* üß∑ **Log metadata** and inputs/outputs for QA or production\n",
    "\n",
    "LangSmith = **‚ÄúDebug Console + Profiler + Analytics + Evaluation Suite‚Äù** for LangChain apps.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. ‚úÖ Core Features\n",
    "\n",
    "| Feature                     | Description                                |\n",
    "| --------------------------- | ------------------------------------------ |\n",
    "| üîé Traces                   | Every step: prompt ‚Üí LLM ‚Üí tool ‚Üí response |\n",
    "| ü™ô Token & latency tracking | See cost & time per call                   |\n",
    "| üõ†Ô∏è Replay & inspect        | Fix errors without reruns                  |\n",
    "| üìò Dataset evals            | Run model comparisons                      |\n",
    "| üè∑Ô∏è Custom metadata         | Add tags, versions, session IDs            |\n",
    "\n",
    "---\n",
    "\n",
    "### 3. ‚úÖ Setup LangSmith\n",
    "\n",
    "#### Step 1: Install SDK\n",
    "\n",
    "```bash\n",
    "pip install langsmith\n",
    "```\n",
    "\n",
    "#### Step 2: Set Environment Variables\n",
    "\n",
    "```bash\n",
    "export LANGCHAIN_API_KEY=\"your_langsmith_key\"\n",
    "export LANGCHAIN_PROJECT=\"MyRAGApp\"\n",
    "```\n",
    "\n",
    "> You‚Äôll find the API key at [https://smith.langchain.com](https://smith.langchain.com) under ‚öôÔ∏è ‚Üí **Account Settings**.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. ‚úÖ Basic Usage in Code\n",
    "\n",
    "LangChain automatically logs everything **if LangSmith is enabled**:\n",
    "\n",
    "```python\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "llm_chain = LLMChain(llm=ChatOpenAI(), prompt=prompt)\n",
    "\n",
    "response = llm_chain.invoke({\"topic\": \"robots\"})\n",
    "```\n",
    "\n",
    "‚úÖ This will now appear in your LangSmith dashboard:\n",
    "\n",
    "```\n",
    "Project: MyRAGApp\n",
    "Trace:  ChatOpenAI ‚Üí PromptTemplate ‚Üí Response\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 5. ‚úÖ Adding Custom Metadata (Logging)\n",
    "\n",
    "You can tag runs with metadata for filtering and tracking:\n",
    "\n",
    "```python\n",
    "llm_chain.invoke(\n",
    "  {\"topic\": \"robots\"},\n",
    "  config={\"metadata\": {\"session_id\": \"123\", \"user\": \"mukesh\"}}\n",
    ")\n",
    "```\n",
    "\n",
    "‚úÖ Use cases:\n",
    "\n",
    "* Multi-user app logging\n",
    "* Trace-specific debugging\n",
    "* Version tracking (e.g., model version, prompt version)\n",
    "\n",
    "---\n",
    "\n",
    "### 6. ‚úÖ Visual Trace Example\n",
    "\n",
    "In the LangSmith UI, you‚Äôll see:\n",
    "\n",
    "```\n",
    "üìÑ Root: LLMChain\n",
    " ‚îú‚îÄ‚îÄ üß† PromptTemplate\n",
    " ‚îú‚îÄ‚îÄ ü§ñ ChatOpenAI\n",
    " ‚îî‚îÄ‚îÄ üìù Output\n",
    "```\n",
    "\n",
    "Each step includes:\n",
    "\n",
    "* Inputs\n",
    "* Prompt used\n",
    "* Output\n",
    "* Token usage\n",
    "* Time taken\n",
    "\n",
    "---\n",
    "\n",
    "### 7. ‚úÖ Tracing Tools, Agents, Memory\n",
    "\n",
    "LangSmith also traces:\n",
    "\n",
    "* üîß Tool calls in agents\n",
    "* üß† Memory variables in chat\n",
    "* üîÅ Retry logic in chains\n",
    "* üß± Nested chains (e.g., inside LangGraph)\n",
    "\n",
    "‚úÖ This means **entire LangChain pipelines**, no matter how complex, are fully traceable.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. ‚úÖ Datasets & Evaluation (Bonus)\n",
    "\n",
    "You can create **test datasets** and evaluate chains:\n",
    "\n",
    "```python\n",
    "from langsmith.evaluation import RunEvaluator\n",
    "\n",
    "evaluator = RunEvaluator(\"qa\")\n",
    "evaluator.evaluate_run(run_id=\"abc123\")\n",
    "```\n",
    "\n",
    "Or from UI:\n",
    "\n",
    "* Create dataset ‚Üí Add input/output pairs\n",
    "* Evaluate different chain versions\n",
    "* Track accuracy, latency, token cost\n",
    "\n",
    "---\n",
    "\n",
    "### 9. ‚úÖ Best Practices\n",
    "\n",
    "| Practice                          | Why                          |\n",
    "| --------------------------------- | ---------------------------- |\n",
    "| Always set `LANGCHAIN_PROJECT`    | Groups related runs together |\n",
    "| Use `metadata` tags               | Enables powerful filtering   |\n",
    "| Trace agents + RAG + LangGraph    | Debug even the deepest stack |\n",
    "| Use datasets for regression tests | LLM eval at scale            |\n",
    "\n",
    "---\n",
    "\n",
    "### 10. ‚úÖ Use Cases\n",
    "\n",
    "| Use Case            | LangSmith Benefit              |\n",
    "| ------------------- | ------------------------------ |\n",
    "| Debug broken prompt | See the full prompt + response |\n",
    "| Compare models      | Use trace diffing + evals      |\n",
    "| Optimize cost       | Check token usage per call     |\n",
    "| Fix agent routing   | View tool traces step by step  |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Summary Table\n",
    "\n",
    "| Feature        | Value                       |\n",
    "| -------------- | --------------------------- |\n",
    "| üîç Tracing     | Prompt-to-output visibility |\n",
    "| üìä Analytics   | Token + time cost           |\n",
    "| üß∑ Metadata    | Tags, versions, sessions    |\n",
    "| üß™ Evaluations | Ground truth testing        |\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
