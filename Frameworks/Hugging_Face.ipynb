{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMhpZ5VHar7mc9cYwL1WgAo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mukeshrock7897/GenerativeAI/blob/main/Hugging_Face.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Beginner Level**\n",
        "\n",
        "1. **Introduction to Hugging Face**\n",
        "   * Overview of Hugging Face\n",
        "   * Key features and benefits\n",
        "   * Installation and setup\n",
        "\n",
        "2. **Hugging Face Transformers Library**\n",
        "   * Overview of the Transformers library\n",
        "   * Basic concepts and terminology\n",
        "   * Installing the Transformers library\n",
        "\n",
        "3. **Getting Started with Pre-trained Models**\n",
        "   * Loading a pre-trained model\n",
        "   * Tokenization\n",
        "   * Performing basic NLP tasks (e.g., text classification, named entity recognition)\n",
        "\n",
        "4. **Pipeline API**\n",
        "   * Introduction to the Pipeline API\n",
        "   * Using pipelines for text classification, text generation, translation, etc.\n",
        "   * Customizing pipelines\n",
        "\n",
        "5. **Working with Tokenizers**\n",
        "   * Understanding tokenizers\n",
        "   * Tokenization process\n",
        "   * Using different tokenizers (e.g., BERT, GPT-2)\n",
        "\n",
        "# **Intermediate Level**\n",
        "\n",
        "1. **Model Fine-tuning**\n",
        "   * Fine-tuning pre-trained models on custom datasets\n",
        "   * Preparing datasets for fine-tuning\n",
        "   * Fine-tuning for text classification, sequence tagging, and other tasks\n",
        "\n",
        "2. **Advanced Tokenization**\n",
        "   * Understanding special tokens\n",
        "   * Customizing tokenization\n",
        "   * Handling special cases in tokenization\n",
        "\n",
        "3. **Using Datasets Library**\n",
        "   * Overview of the Datasets library\n",
        "   * Loading and preprocessing datasets\n",
        "   * Creating custom datasets\n",
        "\n",
        "4. **Custom Models and Architectures**\n",
        "   * Creating custom transformer models\n",
        "   * Modifying existing architectures\n",
        "   * Implementing new architectures\n",
        "\n",
        "5. **Distributed Training and Optimization**\n",
        "   * Using the Trainer API\n",
        "   * Optimizing model performance\n",
        "   * Distributed training with multiple GPUs\n",
        "\n",
        "6. **Model Hub**\n",
        "   * Exploring the Model Hub\n",
        "   * Uploading and sharing models\n",
        "   * Using community models\n",
        "\n",
        "# **Advanced Level**\n",
        "\n",
        "1. **Advanced Fine-tuning Techniques**\n",
        "   * Transfer learning\n",
        "   * Multi-task learning\n",
        "   * Fine-tuning on multi-lingual datasets\n",
        "\n",
        "2. **Specialized Architectures**\n",
        "   * Exploring specialized transformer architectures (e.g., T5, BART, Longformer)\n",
        "   * Understanding the differences and use cases\n",
        "\n",
        "3. **Advanced Model Deployment**\n",
        "   * Deploying models with FastAPI\n",
        "   * Using Hugging Face Inference API\n",
        "   * Deploying models on cloud platforms (AWS, GCP, Azure)\n",
        "\n",
        "4. **Research and Experimentation**\n",
        "   * Implementing cutting-edge research papers\n",
        "   * Experimenting with new architectures and techniques\n",
        "   * Contributing to Hugging Face's open-source projects\n",
        "\n",
        "5. **Case Studies and Real-world Applications**\n",
        "   * In-depth case studies of Hugging Face implementations\n",
        "   * Best practices and lessons learned from large-scale deployments\n",
        "\n",
        "6. **Future Trends and Developments**\n",
        "   * Emerging trends in NLP and transformer models\n",
        "   * Research directions and open challenges\n",
        "   * Community and ecosystem development\n",
        "\n",
        "# **Frameworks and Libraries**\n",
        "\n",
        "1. **Transformers Library**\n",
        "   * Overview and key features\n",
        "   * Installation and usage\n",
        "\n",
        "2. **Datasets Library**\n",
        "   * Overview and key features\n",
        "   * Installation and usage\n",
        "\n",
        "3. **Tokenizers Library**\n",
        "   * Overview and key features\n",
        "   * Installation and usage\n",
        "\n",
        "4. **Trainer API**\n",
        "   * Overview and key features\n",
        "   * Installation and usage\n",
        "\n",
        "5. **Hugging Face Hub**\n",
        "   * Exploring the Model Hub\n",
        "   * Uploading and sharing models\n",
        "   * Using community models\n",
        "\n",
        "# **Applications of Hugging Face**\n",
        "\n",
        "1. **Text Classification**\n",
        "   * Sentiment analysis\n",
        "   * Spam detection\n",
        "   * Topic classification\n",
        "\n",
        "2. **Named Entity Recognition (NER)**\n",
        "   * Extracting entities from text\n",
        "   * Applications in information extraction\n",
        "\n",
        "3. **Question Answering**\n",
        "   * Building QA systems\n",
        "   * Applications in chatbots and virtual assistants\n",
        "\n",
        "4. **Text Generation**\n",
        "   * Generating coherent and contextually relevant text\n",
        "   * Applications in content creation\n",
        "\n",
        "5. **Translation**\n",
        "   * Translating text between languages\n",
        "   * Applications in localization and multilingual communication\n",
        "\n",
        "# **Advantages of Hugging Face**\n",
        "\n",
        "1. **Ease of Use**\n",
        "   * User-friendly APIs\n",
        "   * Extensive documentation and tutorials\n",
        "\n",
        "2. **Flexibility**\n",
        "   * Supports a wide range of NLP tasks\n",
        "   * Customizable and extensible\n",
        "\n",
        "3. **Community and Ecosystem**\n",
        "   * Active community and open-source contributions\n",
        "   * Wide range of pre-trained models and datasets\n",
        "\n",
        "# **Disadvantages of Hugging Face**\n",
        "\n",
        "1. **Resource Intensive**\n",
        "   * High computational requirements for training and fine-tuning\n",
        "   * Large models and datasets can be memory intensive\n",
        "\n",
        "2. **Complexity**\n",
        "   * Advanced customization and optimization can be complex\n",
        "   * Steeper learning curve for advanced features\n"
      ],
      "metadata": {
        "id": "WJglZO0KhW6f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZ-jO7qIhSXl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction to Hugging Face**\n",
        "\n",
        "**Overview of Hugging Face**\n",
        "* Hugging Face is a company that has created a library of state-of-the-art NLP models and tools. Their Transformers library provides easy access to many pre-trained transformer models for tasks like text classification, named entity recognition, text generation, and more.\n",
        "\n",
        "**Key Features and Benefits**\n",
        "* **Ease of Use:** User-friendly APIs for working with transformer models.\n",
        "* **Wide Range of Models:** Access to many pre-trained models for various NLP tasks.\n",
        "* **Community and Support:** Active community and extensive documentation.\n",
        "* **Flexibility:** Ability to fine-tune models for specific tasks.\n",
        "\n",
        "**Installation and Setup**\n",
        "* To get started with Hugging Face, install the Transformers library using pip:"
      ],
      "metadata": {
        "id": "CfSftGsRh4PC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "IbhVsNBap-UO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hugging Face Transformers Library**\n",
        "\n",
        "**Overview of the Transformers Library**\n",
        "* The Transformers library by Hugging Face provides implementations of various transformer models (e.g., BERT, GPT-2, T5) and tools to work with them. It allows users to easily download and use pre-trained models, fine-tune them, and perform various NLP tasks.\n",
        "\n",
        "**Basic Concepts and Terminology**\n",
        "* **Model:** A pre-trained neural network for a specific task.\n",
        "* **Tokenizer:** A tool to convert text into tokens that the model can understand.\n",
        "* **Pipeline:** A high-level API to perform specific tasks with a few lines of code.\n",
        "\n",
        "**Installing the Transformers Library**\n",
        "Install the library using pip:\n",
        "\n"
      ],
      "metadata": {
        "id": "ZPWiTFJ3qLNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "fM-QROgDqa6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Getting Started with Pre-trained Models**\n",
        "\n",
        "**Loading a Pre-trained Model**\n",
        "* Here's how to load a pre-trained BERT model for text classification:"
      ],
      "metadata": {
        "id": "zVlIzXlSqeN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "model_name = \"bert-base-uncased\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "7QhzZ47KqkA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenization**\n",
        "* Tokenization is the process of converting text into tokens that the model can understand."
      ],
      "metadata": {
        "id": "2TSkbwFhqmin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hugging Face is a great library for NLP tasks.\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "print(inputs)"
      ],
      "metadata": {
        "id": "Ot-IvshIqq-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Performing Basic NLP Tasks**\n",
        "**Text Classification**"
      ],
      "metadata": {
        "id": "k5C5-rDBqs8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "result = classifier(\"Hugging Face is a great library for NLP tasks.\")\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "E9K-dJtWqx55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Named Entity Recognition**"
      ],
      "metadata": {
        "id": "CH-xE-Pbqz6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ner = pipeline(\"ner\")\n",
        "result = ner(\"Hugging Face Inc. is based in New York City.\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "rpOev61Wq2aC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Pipeline API**\n",
        "\n",
        "**Introduction to the Pipeline API**\n",
        "* The pipeline API is a high-level interface that allows you to perform various NLP tasks with minimal code.\n",
        "\n",
        "**Using Pipelines for Text Classification, Text Generation, Translation, etc.**\n",
        "\n",
        "**Text Generation**"
      ],
      "metadata": {
        "id": "0luuiygJq4fb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = pipeline(\"text-generation\")\n",
        "result = generator(\"Once upon a time,\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "L0vw-ZTdrAsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Translation**"
      ],
      "metadata": {
        "id": "fElV_Ja-rDqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "translator = pipeline(\"translation_en_to_fr\")\n",
        "result = translator(\"Hugging Face is a great library for NLP tasks.\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "Y0ELrlz8rM-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Customizing Pipelines**\n",
        "* You can customize pipelines by specifying model and tokenizer parameters."
      ],
      "metadata": {
        "id": "qXKRdRLLrO4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "custom_classifier = pipeline(\"sentiment-analysis\", model=model_name, tokenizer=tokenizer)\n",
        "result = custom_classifier(\"I love using Hugging Face!\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "RuMGBEL0rS5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Working with Tokenizers**\n",
        "\n",
        "**Understanding Tokenizers**\n",
        "* Tokenizers convert text into tokens that models can process. They handle tasks like splitting text into words or subwords, adding special tokens, and converting tokens to IDs.\n",
        "\n",
        "**Tokenization Process**"
      ],
      "metadata": {
        "id": "bjhZkhyhrU2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(\"Hugging Face is awesome!\", return_tensors=\"pt\")\n",
        "print(inputs)"
      ],
      "metadata": {
        "id": "O4x0gk7DrcsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Using Different Tokenizers (e.g., BERT, GPT-2)**\n",
        "\n",
        "**BERT Tokenizer**"
      ],
      "metadata": {
        "id": "i9MrLzMYroO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "inputs = bert_tokenizer(\"Hugging Face is awesome!\", return_tensors=\"pt\")\n",
        "print(inputs)\n"
      ],
      "metadata": {
        "id": "jqc2Kwi3rsmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**GPT-2 Tokenizer**"
      ],
      "metadata": {
        "id": "Y2ui4j-LruZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "inputs = gpt2_tokenizer(\"Hugging Face is awesome!\", return_tensors=\"pt\")\n",
        "print(inputs)"
      ],
      "metadata": {
        "id": "_w9D-gDRrxS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EaGxuE2nr1-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Fine-tuning**\n",
        "\n",
        "**Fine-tuning Pre-trained Models on Custom Datasets**\n",
        "* Fine-tuning involves taking a pre-trained model and training it further on a custom dataset to adapt it to a specific task.\n",
        "\n",
        "**Preparing Datasets for Fine-tuning**"
      ],
      "metadata": {
        "id": "N_owqT_pr3B6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"imdb\")\n",
        "\n",
        "# Prepare the data for training\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples['text'], truncation=True, padding=True)\n",
        "\n",
        "encoded_dataset = dataset.map(preprocess_function, batched=True)\n"
      ],
      "metadata": {
        "id": "eTzYCoD6sAHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fine-tuning for Text Classification**"
      ],
      "metadata": {
        "id": "W9mpsiM4sBve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=encoded_dataset[\"train\"],\n",
        "    eval_dataset=encoded_dataset[\"test\"],\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "7DAipKYHsEWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Advanced Tokenization**\n",
        "\n",
        "**Understanding Special Tokens**\n",
        "* Special tokens like [CLS], [SEP], [PAD] are used for specific purposes in tokenization. They help models understand the structure and segments of the input.\n",
        "\n",
        "**Customizing Tokenization**"
      ],
      "metadata": {
        "id": "j6S7ROXDsGy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "tokens = tokenizer.tokenize(\"Hugging Face is awesome!\")\n",
        "print(tokens)\n",
        "\n",
        "# Adding special tokens\n",
        "tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
        "print(tokens)\n"
      ],
      "metadata": {
        "id": "046RBFYysNrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Handling Special Cases in Tokenization**"
      ],
      "metadata": {
        "id": "Cc-dvVcVsPkd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "special_tokens_dict = {'additional_special_tokens': ['<custom_token>']}\n",
        "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "print(tokenizer.tokenize(\"Using a <custom_token> in text.\"))\n"
      ],
      "metadata": {
        "id": "AzUSJv2GsSdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Using Datasets Library**\n",
        "\n",
        "**Overview of the Datasets Library**\n",
        "* The Datasets library by Hugging Face is a collection of ready-to-use datasets and tools to work with them.\n",
        "\n",
        "**Loading and Preprocessing Datasets**"
      ],
      "metadata": {
        "id": "oYEjnazLsWFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset('glue', 'mrpc')\n",
        "\n",
        "# Preprocessing\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples['sentence1'], examples['sentence2'], truncation=True, padding=True)\n",
        "\n",
        "encoded_dataset = dataset.map(preprocess_function, batched=True)\n"
      ],
      "metadata": {
        "id": "OkF7zabssd2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating Custom Datasets**"
      ],
      "metadata": {
        "id": "BhwUMCQosgTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "data = {\"text\": [\"I love NLP\", \"Transformers are great\"], \"label\": [1, 0]}\n",
        "custom_dataset = Dataset.from_dict(data)\n",
        "\n",
        "print(custom_dataset)\n"
      ],
      "metadata": {
        "id": "tnB2gjIJsitz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Custom Models and Architectures**\n",
        "\n",
        "**Creating Custom Transformer Models**"
      ],
      "metadata": {
        "id": "L78fXFp0skIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel, BertConfig\n",
        "\n",
        "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
        "custom_model = BertModel(config)\n",
        "\n",
        "print(custom_model)\n"
      ],
      "metadata": {
        "id": "nBY1g6_VspRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modifying Existing Architectures**"
      ],
      "metadata": {
        "id": "7pr64L9WsrAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomBertModel(BertModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.custom_layer = torch.nn.Linear(config.hidden_size, config.hidden_size)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
        "        outputs = super().forward(input_ids, attention_mask, token_type_ids)\n",
        "        sequence_output = outputs[0]\n",
        "        custom_output = self.custom_layer(sequence_output)\n",
        "        return custom_output\n",
        "\n",
        "custom_model = CustomBertModel(config)\n"
      ],
      "metadata": {
        "id": "nVZaH4cTstl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementing New Architectures**"
      ],
      "metadata": {
        "id": "2TyXehr0svLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import PreTrainedModel, PretrainedConfig\n",
        "\n",
        "class CustomConfig(PretrainedConfig):\n",
        "    model_type = \"custom_model\"\n",
        "    def __init__(self, vocab_size=30522, hidden_size=768, num_hidden_layers=12, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "\n",
        "class CustomModel(PreTrainedModel):\n",
        "    config_class = CustomConfig\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.embeddings = torch.nn.Embedding(config.vocab_size, config.hidden_size)\n",
        "        self.layers = torch.nn.ModuleList([torch.nn.Linear(config.hidden_size, config.hidden_size) for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        output = self.embeddings(input_ids)\n",
        "        for layer in self.layers:\n",
        "            output = layer(output)\n",
        "        return output\n",
        "\n",
        "custom_config = CustomConfig()\n",
        "custom_model = CustomModel(custom_config)\n"
      ],
      "metadata": {
        "id": "fx_iFnWfsybC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Distributed Training and Optimization**\n",
        "\n",
        "**Using the Trainer API**\n",
        "* The Trainer API simplifies the training and evaluation of models."
      ],
      "metadata": {
        "id": "jXl1TxqRs0bw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=encoded_dataset[\"train\"],\n",
        "    eval_dataset=encoded_dataset[\"test\"]\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "O4zoR-Pos7AJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimizing Model Performance**"
      ],
      "metadata": {
        "id": "jvUpCSEas9Ke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=500,\n",
        "    logging_dir='./logs',\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=encoded_dataset[\"train\"],\n",
        "    eval_dataset=encoded_dataset[\"test\"]\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "-ik1ZUFTs_lS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Distributed Training with Multiple GPUs**"
      ],
      "metadata": {
        "id": "GOIAUt6ptBGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    gradient_accumulation_steps=2,\n",
        "    fp16=True,\n",
        "    logging_dir='./logs',\n",
        "    local_rank=-1\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=encoded_dataset[\"train\"],\n",
        "    eval_dataset=encoded_dataset[\"test\"]\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "gbRPUyY5tDXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Hub**\n",
        "\n",
        "**Exploring the Model Hub**\n",
        "* Hugging Face's Model Hub hosts thousands of pre-trained models shared by the community."
      ],
      "metadata": {
        "id": "VqO2x3IItE3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "model = pipeline('sentiment-analysis', model='nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "result = model(\"I love Hugging Face!\")\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "GVU_hTEltKhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Uploading and Sharing Models**"
      ],
      "metadata": {
        "id": "zvTBTHlltMug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi, HfFolder\n",
        "\n",
        "api = HfApi()\n",
        "token = HfFolder.get_token()\n",
        "\n",
        "api.upload_file(\n",
        "    path_or_fileobj=\"path/to/your/model\",\n",
        "    path_in_repo=\"model\",\n",
        "    repo_id=\"your-username/your-model\",\n",
        "    token=token\n",
        ")\n"
      ],
      "metadata": {
        "id": "n5KpRvdCtO9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using Community Models**"
      ],
      "metadata": {
        "id": "12DBM_GitQPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "inputs = tokenizer(\"I love Hugging Face!\", return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "print(outputs)\n"
      ],
      "metadata": {
        "id": "ey2jJ_8MtSbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "38e5hMrGtWSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Advanced Fine-tuning Techniques**\n",
        "\n",
        "**Transfer Learning**\n",
        "* Transfer learning leverages knowledge from a pre-trained model on a new, related task.\n",
        "\n",
        "**Example: Transfer Learning for Text Classification**"
      ],
      "metadata": {
        "id": "R23VS_swtW16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load dataset and tokenizer\n",
        "dataset = load_dataset('imdb')\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples['text'], truncation=True, padding=True)\n",
        "\n",
        "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Load pre-trained model\n",
        "model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=encoded_dataset['train'],\n",
        "    eval_dataset=encoded_dataset['test'],\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "XFsGyPkr3-o1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multi-task Learning**\n",
        "* Multi-task learning involves training a model on multiple tasks simultaneously.\n",
        "\n",
        "**Example: Multi-task Learning with Transformers**"
      ],
      "metadata": {
        "id": "ktVZUjtn4CeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Load pre-trained T5 model and tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "\n",
        "# Example tasks\n",
        "tasks = [\n",
        "    {\"task\": \"summarize\", \"text\": \"The quick brown fox jumps over the lazy dog.\"},\n",
        "    {\"task\": \"translate English to French\", \"text\": \"The quick brown fox jumps over the lazy dog.\"}\n",
        "]\n",
        "\n",
        "# Prepare inputs\n",
        "inputs = tokenizer([f\"{task['task']}: {task['text']}\" for task in tasks], return_tensors=\"pt\", padding=True)\n",
        "\n",
        "# Generate outputs\n",
        "outputs = model.generate(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
        "\n",
        "# Decode outputs\n",
        "decoded_outputs = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
        "print(decoded_outputs)\n"
      ],
      "metadata": {
        "id": "JVEZkfbd4Ihh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fine-tuning on Multi-lingual Datasets**\n",
        "* Fine-tuning on datasets in multiple languages enhances model performance across different languages.\n",
        "\n",
        "**Example: Fine-tuning XLM-R on a Multi-lingual Dataset**"
      ],
      "metadata": {
        "id": "kMu6ivPb4QA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\n",
        "\n",
        "# Load dataset and tokenizer\n",
        "dataset = load_dataset('xnli')\n",
        "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples['premise'], examples['hypothesis'], truncation=True, padding=True)\n",
        "\n",
        "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Load pre-trained model\n",
        "model = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=3)\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=encoded_dataset['train'],\n",
        "    eval_dataset=encoded_dataset['validation'],\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "SGYJh-JT4UVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Specialized Architectures**\n",
        "\n",
        "**Exploring Specialized Transformer Architectures**\n",
        "* Specialized architectures like T5, BART, and Longformer have unique structures and applications.\n",
        "\n",
        "**Example: Using T5 for Text-to-Text Tasks**"
      ],
      "metadata": {
        "id": "zDXnx3D94WKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Load pre-trained T5 model and tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "\n",
        "# Prepare input\n",
        "input_text = \"translate English to German: How are you?\"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# Generate output\n",
        "output_ids = model.generate(input_ids)\n",
        "output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "print(output_text)\n"
      ],
      "metadata": {
        "id": "-4VjNx0i4cmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Advanced Model Deployment**\n",
        "\n",
        "**Deploying Models with FastAPI**\n",
        "* Deploying a model using FastAPI allows for creating REST APIs for model inference.\n",
        "\n",
        "**Example: Deploying a Sentiment Analysis Model**"
      ],
      "metadata": {
        "id": "NJrCP6534enA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn transformers\n",
        "\n",
        "from fastapi import FastAPI\n",
        "from transformers import pipeline\n",
        "\n",
        "app = FastAPI()\n",
        "classifier = pipeline('sentiment-analysis')\n",
        "\n",
        "@app.post(\"/predict/\")\n",
        "async def predict(text: str):\n",
        "    return classifier(text)\n",
        "\n",
        "# To run the server:\n",
        "# !uvicorn myapp:app --reload\n"
      ],
      "metadata": {
        "id": "9G3UGIGa4mMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using Hugging Face Inference API**\n",
        "* Hugging Face offers an Inference API to deploy models with minimal setup.\n",
        "\n",
        "**Example: Using the Inference API**\n"
      ],
      "metadata": {
        "id": "Ja6J3aK74oL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline('sentiment-analysis', model='nlptown/bert-base-multilingual-uncased-sentiment')\n",
        "\n",
        "result = classifier(\"I love Hugging Face!\")\n",
        "print(result)\n"
      ],
      "metadata": {
        "id": "qRy-OjAt4tdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deploying Models on Cloud Platforms**\n",
        "* Deploying models on cloud platforms like AWS, GCP, and Azure for scalable inference.\n",
        "\n",
        "**Example: Deploying on AWS Sagemaker**"
      ],
      "metadata": {
        "id": "QACQJaH_4vUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sagemaker\n",
        "from sagemaker.huggingface import HuggingFaceModel\n",
        "\n",
        "# Initialize the HuggingFaceModel object\n",
        "huggingface_model = HuggingFaceModel(\n",
        "    model_data='s3://path/to/model.tar.gz',\n",
        "    role='arn:aws:iam::account-id:role/role-name',\n",
        "    transformers_version='4.6.1',\n",
        "    pytorch_version='1.7.1',\n",
        "    py_version='py36',\n",
        ")\n",
        "\n",
        "# Deploy the model\n",
        "predictor = huggingface_model.deploy(\n",
        "    initial_instance_count=1,\n",
        "    instance_type='ml.m5.xlarge'\n",
        ")\n",
        "\n",
        "# Predict\n",
        "data = {\"inputs\": \"I love Hugging Face!\"}\n",
        "prediction = predictor.predict(data)\n",
        "print(prediction)\n"
      ],
      "metadata": {
        "id": "B0JR9Tiw40ZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Research and Experimentation**\n",
        "\n",
        "**Implementing Cutting-edge Research Papers**\n",
        "* Reproducing state-of-the-art models and techniques from recent research papers.\n",
        "\n",
        "**Example: Implementing a New Attention Mechanism**"
      ],
      "metadata": {
        "id": "gF99oavj42Ws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel, BertConfig\n",
        "\n",
        "class CustomBertModel(BertModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.custom_attention = torch.nn.MultiheadAttention(config.hidden_size, config.num_attention_heads)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
        "        outputs = super().forward(input_ids, attention_mask, token_type_ids)\n",
        "        sequence_output = outputs[0]\n",
        "        custom_output, _ = self.custom_attention(sequence_output, sequence_output, sequence_output)\n",
        "        return custom_output\n",
        "\n",
        "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
        "custom_model = CustomBertModel(config)\n"
      ],
      "metadata": {
        "id": "f18F7JtT4_Yl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Experimenting with New Architectures and Techniques**\n",
        "* Trying out novel ideas and architectural changes to improve model performance.\n",
        "\n",
        "**Contributing to Hugging Face's Open-source Projects**\n",
        "* Engage with the Hugging Face community by contributing code, documentation, or bug fixes.\n",
        "\n",
        "# **Case Studies and Real-world Applications**\n",
        "**In-depth Case Studies of Hugging Face Implementations**\n",
        "* Analyzing real-world use cases of Hugging Face models in various industries.\n",
        "\n",
        "**Best Practices and Lessons Learned from Large-scale Deployments**\n",
        "* Insights and strategies from deploying Hugging Face models at scale.\n",
        "\n",
        "# **Future Trends and Developments**\n",
        "**Emerging Trends in NLP and Transformer Models**\n",
        "* Keeping up with the latest advancements and trends in the field of NLP.\n",
        "\n",
        "**Research Directions and Open Challenges**\n",
        "* Identifying areas for further research and development in transformer models.\n",
        "\n",
        "**Community and Ecosystem Development**\n",
        "* Participating in the Hugging Face community and contributing to its growth."
      ],
      "metadata": {
        "id": "1bBWPEsn5CCc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u3U7KPgL5jvN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}