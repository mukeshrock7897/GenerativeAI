{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOa4HiAyhQCE1kv4oSryqc/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mukeshrock7897/GenerativeAI/blob/main/XGBoost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Introduction to XGBoost\n",
        "   - **Definition**: XGBoost stands for eXtreme Gradient Boosting. It is a scalable and accurate implementation of gradient boosting machines designed to optimize performance and computational speed. It is particularly effective for structured/tabular data and is used in various machine learning competitions due to its accuracy and efficiency.\n",
        "   - **Use Cases**:\n",
        "     - Data science competitions (e.g., Kaggle)\n",
        "     - Customer churn prediction\n",
        "     - Anomaly detection\n",
        "     - Risk management in finance\n",
        "\n",
        "### 2. Installation and Setup\n",
        "   - **Installation**:\n",
        "     ```python\n",
        "     pip install xgboost\n",
        "     ```\n",
        "     Installs the XGBoost library.\n",
        "   - **Setting up the environment**:\n",
        "     Importing necessary libraries and datasets, and preparing the data for use with XGBoost.\n",
        "\n",
        "### 3. Key Features of XGBoost\n",
        "   - **Regularization**:\n",
        "     - Prevents overfitting by adding penalties to the loss function for increasing model complexity.\n",
        "     - Parameters like `alpha` (L1 regularization term on weights) and `lambda` (L2 regularization term on weights) help control overfitting.\n",
        "   - **Parallel Processing**:\n",
        "     - Uses multiple CPU cores for training, which speeds up the computation significantly.\n",
        "     - The `nthread` parameter specifies the number of threads to use.\n",
        "   - **Handling Missing Values**:\n",
        "     - Automatically learns the best way to handle missing data during training.\n",
        "     - This is useful when dealing with incomplete datasets.\n",
        "   - **Tree Pruning**:\n",
        "     - Uses a depth-first approach to build trees.\n",
        "     - The `max_depth` parameter controls the maximum depth of a tree, helping to reduce overfitting and memory consumption.\n",
        "\n",
        "### 4. Basic Usage\n",
        "   - **Loading Dataset**:\n",
        "     ```python\n",
        "     import xgboost as xgb\n",
        "     from sklearn.datasets import fetch_california_housing\n",
        "     from sklearn.model_selection import train_test_split\n",
        "     from sklearn.metrics import mean_squared_error\n",
        "\n",
        "     # Load data\n",
        "     housing = fetch_california_housing()\n",
        "     X, y = housing.data, housing.target\n",
        "     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "     ```\n",
        "     - Fetches the California Housing dataset, splits it into training and testing sets.\n",
        "   - **DMatrix**:\n",
        "     - A specialized data structure in XGBoost optimized for both memory efficiency and training speed.\n",
        "     ```python\n",
        "     dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "     dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "     ```\n",
        "\n",
        "### 5. Training the Model\n",
        "   - **Parameters**:\n",
        "     - Defines hyperparameters to control the training process.\n",
        "     ```python\n",
        "     params = {\n",
        "         'objective': 'reg:squarederror',  # for regression task\n",
        "         'max_depth': 6,\n",
        "         'eta': 0.3,  # learning rate\n",
        "         'subsample': 0.7,\n",
        "         'colsample_bytree': 0.7\n",
        "     }\n",
        "     ```\n",
        "   - **Training**:\n",
        "     - Trains the model using the specified parameters and dataset.\n",
        "     ```python\n",
        "     num_round = 100\n",
        "     bst = xgb.train(params, dtrain, num_round)\n",
        "     ```\n",
        "\n",
        "### 6. Making Predictions\n",
        "   - **Predicting and Evaluating**:\n",
        "     - Generates predictions on the test dataset and evaluates the performance using Root Mean Squared Error (RMSE).\n",
        "     ```python\n",
        "     preds = bst.predict(dtest)\n",
        "     rmse = mean_squared_error(y_test, preds, squared=False)\n",
        "     print(f'RMSE: {rmse}')\n",
        "     ```\n",
        "\n",
        "### 7. Advanced Features\n",
        "   - **Early Stopping**:\n",
        "     - Stops training when the evaluation metric on a validation set does not improve after a specified number of rounds.\n",
        "     ```python\n",
        "     evallist = [(dtest, 'eval'), (dtrain, 'train')]\n",
        "     bst = xgb.train(params, dtrain, num_round, evallist, early_stopping_rounds=10)\n",
        "     ```\n",
        "   - **Cross-Validation**:\n",
        "     - Performs cross-validation to tune hyperparameters and prevent overfitting.\n",
        "     ```python\n",
        "     cv_results = xgb.cv(params, dtrain, num_boost_round=100, nfold=5, metrics={'rmse'}, early_stopping_rounds=10)\n",
        "     print(cv_results)\n",
        "     ```\n",
        "   - **Feature Importance**:\n",
        "     - Visualizes the importance of each feature in making predictions.\n",
        "     ```python\n",
        "     import matplotlib.pyplot as plt\n",
        "     xgb.plot_importance(bst)\n",
        "     plt.show()\n",
        "     ```\n",
        "\n",
        "### 8. Hyperparameter Tuning\n",
        "   - **Grid Search**:\n",
        "     - Uses grid search to find the optimal hyperparameters for the model.\n",
        "     ```python\n",
        "     from sklearn.model_selection import GridSearchCV\n",
        "     param_grid = {\n",
        "         'max_depth': [3, 5, 7],\n",
        "         'learning_rate': [0.01, 0.1, 0.3],\n",
        "         'subsample': [0.5, 0.7, 1.0]\n",
        "     }\n",
        "     grid_search = GridSearchCV(estimator=xgb.XGBRegressor(objective='reg:squarederror'), param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n",
        "     grid_search.fit(X_train, y_train)\n",
        "     print(grid_search.best_params_)\n",
        "     ```\n",
        "\n",
        "### 9. Model Interpretation\n",
        "   - **SHAP Values**:\n",
        "     - SHAP (SHapley Additive exPlanations) values are used to interpret the output of machine learning models.\n",
        "     ```python\n",
        "     import shap\n",
        "     explainer = shap.Explainer(bst)\n",
        "     shap_values = explainer(X_test)\n",
        "     shap.summary_plot(shap_values, X_test)\n",
        "     ```\n",
        "\n",
        "### 10. Saving and Loading Models\n",
        "   - **Saving Model**:\n",
        "     - Saves the trained model to a file for later use.\n",
        "     ```python\n",
        "     bst.save_model('xgboost_model.json')\n",
        "     ```\n",
        "   - **Loading Model**:\n",
        "     - Loads a saved model from a file.\n",
        "     ```python\n",
        "     loaded_bst = xgb.Booster()\n",
        "     loaded_bst.load_model('xgboost_model.json')\n",
        "     ```\n",
        "\n",
        "### 11. Integration with Other Libraries\n",
        "   - **Scikit-learn Integration**:\n",
        "     - Integrates XGBoost with scikit-learn for a more familiar interface and additional functionalities like pipelines.\n",
        "     ```python\n",
        "     from xgboost import XGBRegressor\n",
        "     model = XGBRegressor(objective='reg:squarederror')\n",
        "     model.fit(X_train, y_train)\n",
        "     preds = model.predict(X_test)\n",
        "     ```\n",
        "\n",
        "### 12. Dealing with Imbalanced Datasets\n",
        "   - **Parameter Adjustment**:\n",
        "     - Adjusts the `scale_pos_weight` parameter to handle imbalanced datasets, especially in binary classification tasks.\n",
        "     ```python\n",
        "     params['scale_pos_weight'] = sum(y_train == 0) / sum(y_train == 1)  # For binary classification\n",
        "     ```\n",
        "\n",
        "### 13. Custom Objective and Evaluation Functions\n",
        "   - **Custom Objective**:\n",
        "     - Defines a custom objective function for specific needs.\n",
        "     ```python\n",
        "     def custom_obj(preds, dtrain):\n",
        "         labels = dtrain.get_label()\n",
        "         return 'custom_obj', sum((preds - labels) ** 2)\n",
        "\n",
        "     bst = xgb.train(params, dtrain, num_round, obj=custom_obj)\n",
        "     ```\n",
        "\n",
        "### 14. GPU Support\n",
        "   - **Enabling GPU**:\n",
        "     - Utilizes GPU acceleration for faster computation.\n",
        "     ```python\n",
        "     params['tree_method'] = 'gpu_hist'\n",
        "     bst = xgb.train(params, dtrain, num_round)\n",
        "     ```\n",
        "\n",
        "### 15. XGBoost in a Distributed Setting\n",
        "   - **Running on Multiple Machines**:\n",
        "     - Uses Dask or Spark to handle large datasets distributed across multiple machines for parallel processing and distributed computing.\n"
      ],
      "metadata": {
        "id": "-dVYJSe2B_Se"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GO7k3EQIB7xs"
      },
      "outputs": [],
      "source": []
    }
  ]
}