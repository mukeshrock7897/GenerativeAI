{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# üöÄ **NLP for Machine Learning, Deep Learning & Generative AI**\n",
    "---\n",
    "---\n",
    "\n",
    "### üå± **1. Text Preprocessing & Cleaning**\n",
    "\n",
    "> üßº Core for all ML/DL pipelines\n",
    "\n",
    "* üî° Tokenization (word, subword, sentence)\n",
    "* üî§ Lowercasing, Stopword Removal\n",
    "* üîç Regex-based Cleaning\n",
    "* üåø Lemmatization (preferred over stemming)\n",
    "* üî† POS Tagging (important for feature engineering)\n",
    "* üß± Named Entity Recognition (NER)\n",
    "\n",
    "---\n",
    "\n",
    "### üìä **2. Text Representation**\n",
    "\n",
    "> üî¢ Core for feature engineering & transformer models\n",
    "\n",
    "* üì¶ Bag of Words (BoW) *(ML only)*\n",
    "* üìà TF-IDF *(Still used in non-DL models)*\n",
    "* üß† Word Embeddings\n",
    "\n",
    "  * üìå Word2Vec, FastText *(classical)*\n",
    "  * üî° GloVe *(for pretrained static vectors)*\n",
    "* üß† Sentence Embeddings\n",
    "\n",
    "  * üîç Sentence-BERT, Universal Sentence Encoder *(modern)*\n",
    "* üîç Document Embeddings *(Doc2Vec or avg of word embeddings)*\n",
    "\n",
    "---\n",
    "\n",
    "### üß† **3. Machine Learning for NLP**\n",
    "\n",
    "> üõ†Ô∏è Still widely used in industry for small-to-medium scale tasks\n",
    "\n",
    "* ‚úÖ Text Classification (spam detection, intent classification)\n",
    "* üîç Sentiment Analysis (BoW/TF-IDF + ML)\n",
    "* üóÇÔ∏è Topic Modeling\n",
    "\n",
    "  * üìö LDA, NMF *(exploratory analysis or unsupervised insights)*\n",
    "* üì• Text Similarity (cosine, Jaccard, with embeddings)\n",
    "\n",
    "---\n",
    "\n",
    "### ü§ñ **4. Deep Learning for NLP**\n",
    "\n",
    "> üî• Used in complex pipelines and modern ML stacks\n",
    "\n",
    "* üß± Embedding Layers (`tf.keras.layers.Embedding`)\n",
    "* üîÅ LSTM, GRU *(legacy DL, still used for time-aware text data)*\n",
    "* üîÑ Bi-LSTM + Attention *(NER, Seq Labeling)*\n",
    "* üìê CNN for Text Classification *(fast & effective baseline)*\n",
    "* üß† Seq2Seq (Encoder-Decoder models) *(translation, summarization)*\n",
    "\n",
    "---\n",
    "\n",
    "### üîÅ **5. Transformers and Foundation Models**\n",
    "\n",
    "> üí• Core of **modern** NLP applications\n",
    "\n",
    "* ü§ñ BERT, RoBERTa, DistilBERT *(classification, NER, QA)*\n",
    "* üß† GPT family *(GPT-2, GPT-3, GPT-4)* ‚Äî **Text Generation, Chatbots**\n",
    "* üîÑ T5, BART ‚Äî **Text-to-Text tasks (summarization, translation)**\n",
    "* üß™ LLaMA, Falcon, Mistral ‚Äî **Open LLMs**\n",
    "* üì¶ Hugging Face Transformers (üöÄ Industry-standard library)\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ **6. Key NLP Tasks in Industry**\n",
    "\n",
    "> üîç Directly tied to business impact\n",
    "\n",
    "* üßæ Sentiment Analysis (retail, finance, healthcare)\n",
    "* üóÇÔ∏è Document Classification (support tickets, resumes)\n",
    "* üîê NER (legal, medical, financial documents)\n",
    "* ‚ú® Keyword & Keyphrase Extraction\n",
    "* üß† Semantic Search (e.g., embedding-based search engines)\n",
    "* üí¨ Chatbots and Virtual Assistants (GPT-based)\n",
    "* üìÑ Text Summarization (customer reviews, reports)\n",
    "* üì§ Question Answering (FAQ bots, support tools)\n",
    "* üìö Topic Modeling (for unsupervised business insights)\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è **7. Essential NLP Libraries**\n",
    "\n",
    "> üõ†Ô∏è Use these in real production pipelines\n",
    "\n",
    "* üêç NLTK & SpaCy (Preprocessing, POS, NER)\n",
    "* üî• Hugging Face Transformers (BERT, GPT, T5, QA)\n",
    "* üìö Gensim (LDA, Word2Vec)\n",
    "* üß™ SentenceTransformers (`sentence-transformers` for S-BERT)\n",
    "* üß† LangChain (for LLM integration + agentic NLP apps)\n",
    "* üéôÔ∏è OpenAI Whisper (for speech-to-text transcription)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Text Preprocessing & Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### üî° **Tokenization**\n",
    "\n",
    "### üìå Definitions:\n",
    "\n",
    "* Tokenization is the process of breaking a **text into smaller units** called tokens.\n",
    "* Tokens can be **words**, **subwords**, **characters**, or **sentences**.\n",
    "* It is the **first and essential step** in almost every NLP pipeline.\n",
    "\n",
    "### ‚úÖ Real-time Use Case:\n",
    "\n",
    "* Preprocessing user reviews in e-commerce for sentiment analysis.\n",
    "* Feeding input to models like BERT or GPT which require tokenized input.\n",
    "\n",
    "### ‚ùå When Not to Use:\n",
    "\n",
    "* For structured numerical data (e.g., stock prices, sensor data).\n",
    "* When raw text is already represented via embeddings or IDs.\n",
    "\n",
    "### ‚öôÔ∏è Code Implementation:\n",
    "\n",
    "```python\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "text = \"NLP is powerful. It drives many AI applications.\"\n",
    "word_tokens = word_tokenize(text)\n",
    "sent_tokens = sent_tokenize(text)\n",
    "print(word_tokens)  # ['NLP', 'is', 'powerful', '.', 'It', 'drives', 'many', 'AI', 'applications', '.']\n",
    "```\n",
    "\n",
    "For deep learning:\n",
    "\n",
    "```python\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokens = tokenizer.tokenize(\"Natural Language Processing is amazing!\")\n",
    "print(tokens)\n",
    "```\n",
    "\n",
    "### ‚úÖ Advantages:\n",
    "\n",
    "* Essential for converting unstructured text into structured data.\n",
    "* Forms the basis for downstream processing and modeling.\n",
    "\n",
    "### ‚ùå Disadvantages:\n",
    "\n",
    "* Token boundaries can vary across languages (e.g., Chinese vs English).\n",
    "* Requires language-specific rules or pretrained tokenizers.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### üî§ **Lowercasing & Stopword Removal**\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Definitions:\n",
    "\n",
    "* **Lowercasing**: Converting all characters in text to lowercase to reduce vocabulary size and normalize data.\n",
    "* **Stopword Removal**: Eliminating common words (e.g., ‚Äúthe‚Äù, ‚Äúis‚Äù, ‚Äúand‚Äù) that do not carry significant meaning for analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Real-time Use Case:\n",
    "\n",
    "* Used in **text classification** (e.g., spam detection) to prevent treating \"Spam\" and \"spam\" as different words.\n",
    "* Improves accuracy in **TF-IDF** and **BoW** models by removing low-information words.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå When Not to Use:\n",
    "\n",
    "* In **sentiment analysis**, sometimes stopwords like ‚Äúnot‚Äù and ‚Äúnever‚Äù carry crucial polarity.\n",
    "* For **case-sensitive tasks** like NER or analyzing proper nouns (e.g., ‚ÄúApple‚Äù vs ‚Äúapple‚Äù).\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Code Implementation:\n",
    "\n",
    "```python\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"The Quick brown fox jumps over the lazy Dog.\"\n",
    "words = word_tokenize(text.lower())  # lowercasing\n",
    "filtered = [w for w in words if w not in stopwords.words('english')]\n",
    "print(filtered)  # ['quick', 'brown', 'fox', 'jumps', 'lazy', 'dog']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Advantages:\n",
    "\n",
    "* Reduces dimensionality of text features.\n",
    "* Increases model generalization by eliminating irrelevant noise.\n",
    "* Helps in faster training for ML algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Disadvantages:\n",
    "\n",
    "* Might remove contextually important words (e.g., ‚Äúnot‚Äù in ‚Äúnot happy‚Äù).\n",
    "* Can be language-specific ‚Äî requires the right stopword list per language.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### üßπ **Regex-based Cleaning (Text Normalization)**\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Definitions:\n",
    "\n",
    "* **Regex (Regular Expressions)**: A pattern-matching technique to **identify and clean specific text patterns** such as URLs, emails, special characters, or digits.\n",
    "* Commonly used to **sanitize text** by removing noise like HTML tags, extra spaces, emojis, mentions, hashtags, etc.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Real-time Use Case:\n",
    "\n",
    "* Cleaning tweets or product reviews by removing URLs, hashtags, mentions before feeding to ML models.\n",
    "* Preprocessing **OCR** text to remove unwanted symbols.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå When Not to Use:\n",
    "\n",
    "* For text sources where **raw symbols carry meaning** (e.g., code, logs, financial tickers like `$AAPL`).\n",
    "* If regex patterns are too aggressive, they may **remove valid text** (e.g., decimal points, punctuation).\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Code Implementation:\n",
    "\n",
    "```python\n",
    "import re\n",
    "\n",
    "text = \"Check out our new product! üòç https://example.com #Launch @company\"\n",
    "\n",
    "# Remove URLs\n",
    "text = re.sub(r\"http\\S+\", \"\", text)\n",
    "\n",
    "# Remove mentions and hashtags\n",
    "text = re.sub(r\"[@#]\\w+\", \"\", text)\n",
    "\n",
    "# Remove emojis and special characters\n",
    "text = re.sub(r\"[^A-Za-z0-9\\s]\", \"\", text)\n",
    "\n",
    "# Normalize whitespace\n",
    "text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "print(text)  # Output: 'Check out our new product'\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Advantages:\n",
    "\n",
    "* Makes text more consistent for tokenization and vectorization.\n",
    "* Improves accuracy of downstream NLP models by reducing irrelevant noise.\n",
    "* Fully customizable using patterns for domain-specific cleaning.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Disadvantages:\n",
    "\n",
    "* Writing regex patterns requires expertise and can be error-prone.\n",
    "* Over-cleaning might lead to **loss of context**, such as removing hashtags that carry semantic meaning (e.g., `#BlackLivesMatter`).\n",
    "* May not generalize well across different datasets or domains.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### üåø **Lemmatization**\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Definitions:\n",
    "\n",
    "* **Lemmatization** reduces a word to its **base or dictionary form** (called a *lemma*), considering **context and part-of-speech**.\n",
    "* Unlike stemming, it returns **real words**.\n",
    "  Example:\n",
    "\n",
    "  * ‚Äúam‚Äù, ‚Äúare‚Äù, ‚Äúis‚Äù ‚Üí ‚Äúbe‚Äù\n",
    "  * ‚Äúrunning‚Äù, ‚Äúran‚Äù ‚Üí ‚Äúrun‚Äù\n",
    "  * ‚Äúbetter‚Äù ‚Üí ‚Äúgood‚Äù (semantic lemmatization)\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Real-time Use Case:\n",
    "\n",
    "* **Text classification** and **topic modeling**, where different word forms need to be unified.\n",
    "* Helps in reducing the vocabulary without losing semantic meaning, improving model performance.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå When Not to Use:\n",
    "\n",
    "* In **speed-sensitive pipelines**, since lemmatization is slower than stemming.\n",
    "* When exact word form is important (e.g., **language generation**, **translation**, or **legal text**).\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Code Implementation:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Mapping NLTK POS tags to WordNet POS tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'): return wordnet.ADJ\n",
    "    elif tag.startswith('V'): return wordnet.VERB\n",
    "    elif tag.startswith('N'): return wordnet.NOUN\n",
    "    elif tag.startswith('R'): return wordnet.ADV\n",
    "    else: return wordnet.NOUN\n",
    "\n",
    "text = \"The striped bats were hanging on their feet and ate best.\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "lemmatized = [lemmatizer.lemmatize(w, get_wordnet_pos(t)) for w, t in pos_tags]\n",
    "print(lemmatized)\n",
    "# ['The', 'striped', 'bat', 'be', 'hang', 'on', 'their', 'foot', 'and', 'eat', 'good']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Advantages:\n",
    "\n",
    "* More accurate than stemming ‚Äî preserves word meaning.\n",
    "* Reduces vocabulary size while retaining **semantic correctness**.\n",
    "* Useful in downstream tasks like classification, summarization, topic modeling.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Disadvantages:\n",
    "\n",
    "* Slower than stemming ‚Äî involves **POS tagging** and **dictionary lookup**.\n",
    "* Doesn‚Äôt always handle **morphologically complex words** correctly.\n",
    "* May require tuning for specific domains (medical, legal, etc.).\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üî† **POS Tagging (Part-of-Speech Tagging)**\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Definitions:\n",
    "\n",
    "* **POS Tagging** is the process of assigning a **part of speech label** (noun, verb, adjective, etc.) to each word in a sentence.\n",
    "* Helps in understanding the **syntactic structure** of the sentence.\n",
    "* POS tags like **NN (noun), VB (verb), JJ (adjective), RB (adverb)** are commonly used.\n",
    "* It‚Äôs a foundational step for **lemmatization**, **NER**, and **dependency parsing**.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Real-time Use Case:\n",
    "\n",
    "* **Feature Engineering** for ML models ‚Äî e.g., creating features like ‚Äúpercentage of nouns‚Äù in a document.\n",
    "* Used in **sentiment analysis**, where adjectives/adverbs matter.\n",
    "* Crucial for **question answering** systems and **grammar correction** tools.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå When Not to Use:\n",
    "\n",
    "* In very **short texts** (e.g., search keywords or emojis) where context is limited.\n",
    "* When using transformer-based models like BERT, which already learn context without explicit POS tags.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Code Implementation:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "tokens = word_tokenize(text)\n",
    "tags = pos_tag(tokens)\n",
    "\n",
    "print(tags)\n",
    "# [('The', 'DT'), ('quick', 'JJ'), ('brown', 'JJ'), ('fox', 'NN'),\n",
    "#  ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n",
    "```\n",
    "\n",
    "üëâ Example Feature Extraction for ML:\n",
    "\n",
    "```python\n",
    "# Count % of adjectives\n",
    "num_adj = sum(1 for word, tag in tags if tag.startswith('JJ'))\n",
    "adj_ratio = num_adj / len(tags)\n",
    "print(f\"Adjective Ratio: {adj_ratio:.2f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Advantages:\n",
    "\n",
    "* Adds **syntactic understanding** to raw text.\n",
    "* Boosts the performance of ML models when used as engineered features.\n",
    "* Essential for downstream NLP tasks like **NER**, **coreference resolution**, and **lemmatization**.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Disadvantages:\n",
    "\n",
    "* **Language-specific** ‚Äî requires different taggers and rules for each language.\n",
    "* May be **inaccurate** with slang, typos, or very informal text.\n",
    "* Not always necessary when using **deep contextual embeddings** (e.g., from BERT).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üß± **Named Entity Recognition (NER)**\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Definitions:\n",
    "\n",
    "* **Named Entity Recognition (NER)** is the process of **detecting and classifying named entities** in text into predefined categories such as:\n",
    "\n",
    "  * üßë Person\n",
    "  * üåç Location\n",
    "  * üè¢ Organization\n",
    "  * üóìÔ∏è Date / Time\n",
    "  * üí∞ Money / Quantity / Percent\n",
    "\n",
    "* It helps extract **structured data from unstructured text**, crucial for downstream analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Real-time Use Case:\n",
    "\n",
    "* üîç **Resume parsing**: Extract names, skills, companies, dates.\n",
    "* üí¨ **Customer support**: Identify product names, complaint categories, places.\n",
    "* üßë‚Äç‚öñÔ∏è **Legal/Medical documents**: Tag dates, people, cases, medical terms.\n",
    "* üìà **Financial news**: Detect company names, stock tickers, monetary values.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå When Not to Use:\n",
    "\n",
    "* For **informal text** with heavy slang, code-switching, or poor grammar ‚Äî may confuse pre-trained NER models.\n",
    "* When only **general topics** are needed (e.g., spam detection), NER might be overkill.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Code Implementation:\n",
    "\n",
    "**Using SpaCy (industry-preferred NER library):**\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "# Load English NER model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"Elon Musk is the CEO of SpaceX, founded in 2002 in California.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# Print Named Entities\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "    \n",
    "# Output:\n",
    "# Elon Musk PERSON\n",
    "# SpaceX ORG\n",
    "# 2002 DATE\n",
    "# California GPE\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Advantages:\n",
    "\n",
    "* Extracts **high-value structured data** from raw text.\n",
    "* Pretrained models (like SpaCy, BERT NER) are **highly accurate**.\n",
    "* Can be used to **tag domain-specific entities** (e.g., drugs, diseases, products) with custom training.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Disadvantages:\n",
    "\n",
    "* **Limited to known entity types** unless retrained on custom data.\n",
    "* Pretrained models may struggle with **non-English**, **noisy**, or **short text**.\n",
    "* **Annotation-heavy** if building a custom NER model from scratch.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Text Representation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üì¶ **1. Bag of Words (BoW)**\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Definitions:\n",
    "\n",
    "* **BoW** converts text into a fixed-length vector based on **word frequency** in a document.\n",
    "* It **ignores grammar and word order**, only counts how often each word appears.\n",
    "* Often combined with **CountVectorizer** from `scikit-learn`.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Real-time Use Case:\n",
    "\n",
    "* Used in **text classification** tasks like:\n",
    "\n",
    "  * Spam Detection\n",
    "  * Product Review Classification\n",
    "  * News Category Classification\n",
    "\n",
    "Especially effective for **small datasets** and **simple ML models**.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå When Not to Use:\n",
    "\n",
    "* When word **order or semantics** matter (e.g., machine translation, summarization).\n",
    "* For **deep learning models**, which require dense and contextual input.\n",
    "* On **very large vocabularies**, as BoW becomes sparse and inefficient.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Code Implementation:\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"ChatGPT is amazing for NLP.\",\n",
    "    \"NLP includes ChatGPT, BERT, and more models.\"\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "# ['and' 'amazing' 'bert' 'chatgpt' 'for' 'includes' 'models' 'more' 'nlp']\n",
    "\n",
    "print(X.toarray())\n",
    "# [[0 1 0 1 1 0 0 0 1],\n",
    "#  [1 0 1 1 0 1 1 1 1]]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Advantages:\n",
    "\n",
    "* Very **simple and fast** to implement.\n",
    "* Works well with **linear classifiers** (Logistic Regression, Naive Bayes).\n",
    "* Good baseline for many text classification problems.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Disadvantages:\n",
    "\n",
    "* Results in **sparse** matrices with high memory usage.\n",
    "* **No semantic meaning** ‚Äî \"car\" and \"automobile\" treated as different.\n",
    "* Ignores **word order**, grammar, or context.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üìà **2. TF-IDF (Term Frequency‚ÄìInverse Document Frequency)**\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Definitions:\n",
    "\n",
    "* **TF-IDF** is a statistical measure used to evaluate how **important a word is** in a document relative to the entire corpus.\n",
    "* It combines:\n",
    "\n",
    "  * **TF** (Term Frequency): Frequency of a word in a document.\n",
    "  * **IDF** (Inverse Document Frequency): Rarity of the word across all documents.\n",
    "* Formula:\n",
    "  `TF-IDF(w, d) = TF(w, d) * log(N / DF(w))`\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Real-time Use Case:\n",
    "\n",
    "* Used in:\n",
    "\n",
    "  * **Information Retrieval Systems** (search engines)\n",
    "  * **Keyword extraction**\n",
    "  * **Text classification** (better than BoW in many scenarios)\n",
    "* Helps highlight **important domain-specific terms**.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå When Not to Use:\n",
    "\n",
    "* For **semantic tasks** requiring understanding of meaning (e.g., QA, summarization).\n",
    "* For **deep learning models** ‚Äî they perform better with embeddings.\n",
    "* On **very short texts**, TF-IDF may not differentiate terms effectively.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Code Implementation:\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"ChatGPT is amazing for NLP.\",\n",
    "    \"NLP includes ChatGPT, BERT, and more models.\"\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "# ['and' 'amazing' 'bert' 'chatgpt' 'for' 'includes' 'models' 'more' 'nlp']\n",
    "\n",
    "print(X.toarray())\n",
    "# TF-IDF values for each word in each document\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Advantages:\n",
    "\n",
    "* Reduces the **weight of common words** (like ‚Äúthe‚Äù, ‚Äúand‚Äù).\n",
    "* Highlights **discriminative keywords** in each document.\n",
    "* Better performance than BoW for **classification & retrieval** tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Disadvantages:\n",
    "\n",
    "* Still results in **sparse matrices**.\n",
    "* Ignores **context and word meaning**.\n",
    "* Doesn‚Äôt work well when documents are **too short** or highly unbalanced.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üß† **3. Word Embeddings (Word2Vec, GloVe, FastText)**\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Definitions:\n",
    "\n",
    "* **Word Embeddings** are dense, real-valued vector representations of words where **similar words have similar vectors**.\n",
    "* Unlike BoW/TF-IDF, embeddings **capture context, semantics, and relationships**.\n",
    "* Popular pretrained embedding models:\n",
    "\n",
    "  * üß± **Word2Vec** ‚Äì predicts surrounding words (Skip-gram/CBOW)\n",
    "  * üß± **GloVe** ‚Äì based on word co-occurrence statistics\n",
    "  * üß± **FastText** ‚Äì improves on Word2Vec using subword (character n-grams)\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Real-time Use Case:\n",
    "\n",
    "* Used in:\n",
    "\n",
    "  * üí¨ Sentiment Analysis\n",
    "  * üìÇ Document Similarity & Clustering\n",
    "  * üìå Semantic Search\n",
    "  * üí¨ Chatbots & Virtual Assistants\n",
    "* Perfect for **feeding into deep learning models (RNNs, CNNs, Transformers)**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå When Not to Use:\n",
    "\n",
    "* For **lightweight ML models** (like Logistic Regression), BoW/TF-IDF might be faster/simpler.\n",
    "* When **training time is limited** ‚Äî training embeddings from scratch can be slow.\n",
    "* Word2Vec/GloVe don‚Äôt handle **out-of-vocabulary** (OOV) words unless extended (like FastText).\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Code Implementation:\n",
    "\n",
    "**Using Gensim‚Äôs Word2Vec (training on your own data):**\n",
    "\n",
    "```python\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [\n",
    "    ['nlp', 'is', 'fun'],\n",
    "    ['chatgpt', 'is', 'a', 'powerful', 'language', 'model']\n",
    "]\n",
    "\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=1)\n",
    "\n",
    "vector = model.wv['chatgpt']\n",
    "print(vector[:5])  # Shows first 5 values of the 100-dim vector\n",
    "```\n",
    "\n",
    "**Using pretrained GloVe embeddings:**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def load_glove(path=\"glove.6B.100d.txt\"):\n",
    "    embeddings = {}\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            word = values[0]\n",
    "            vector = np.array(values[1:], dtype=\"float32\")\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "glove = load_glove()\n",
    "print(glove[\"king\"][:5])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Advantages:\n",
    "\n",
    "* Captures **semantic and syntactic relationships** (e.g., *king - man + woman ‚âà queen*).\n",
    "* Improves accuracy in **downstream tasks** like classification, similarity, and clustering.\n",
    "* **Pretrained models** reduce training time and can generalize well.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Disadvantages:\n",
    "\n",
    "* Word2Vec & GloVe create **static embeddings** (same vector regardless of context).\n",
    "* Require **large corpus** for training from scratch.\n",
    "* Struggles with **polysemy** (same word, different meanings ‚Äî e.g., ‚Äúbank‚Äù).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üß† **4. Sentence Embeddings (e.g., Sentence-BERT, USE)**\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Definitions:\n",
    "\n",
    "* **Sentence Embeddings** represent **entire sentences or paragraphs** as dense vectors that capture **semantic meaning**, not just word composition.\n",
    "* Unlike word embeddings, they encode **context, intent, and structure** of the full sentence.\n",
    "* Popular models:\n",
    "\n",
    "  * üß† **Sentence-BERT (SBERT)** ‚Äì adds a pooling layer to BERT for producing sentence-level embeddings.\n",
    "  * üåê **Universal Sentence Encoder (USE)** ‚Äì Google's model for general-purpose sentence representation.\n",
    "  * üß† **MPNet, MiniLM** ‚Äì lightweight transformer variants for fast embedding generation.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Real-time Use Case:\n",
    "\n",
    "* üìö **Semantic search**: Retrieve documents that semantically match a query.\n",
    "* üß† **Duplicate question detection**: (e.g., Quora or forums)\n",
    "* üìÑ **Legal/medical similarity checks**: Identify similar clauses or cases.\n",
    "* üì• **Embedding-based clustering or classification** of user comments, chats, feedback.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå When Not to Use:\n",
    "\n",
    "* When working with **very short keywords** ‚Äî word embeddings may suffice.\n",
    "* For **rule-based pipelines** or **symbolic NLP**, where context isn't needed.\n",
    "* In **low-resource environments** where transformers are too heavy.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Code Implementation:\n",
    "\n",
    "**Using Sentence-BERT via `sentence-transformers`:**\n",
    "\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "sentences = [\n",
    "    \"I love playing with AI models.\",\n",
    "    \"Natural Language Processing is fascinating!\"\n",
    "]\n",
    "\n",
    "embeddings = model.encode(sentences)\n",
    "print(embeddings[0][:5])  # Show first 5 values of first sentence vector\n",
    "```\n",
    "\n",
    "**Cosine Similarity for Semantic Comparison:**\n",
    "\n",
    "```python\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity = cosine_similarity([embeddings[0]], [embeddings[1]])\n",
    "print(f\"Similarity Score: {similarity[0][0]:.2f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Advantages:\n",
    "\n",
    "* Captures **sentence-level meaning**, perfect for modern applications.\n",
    "* **High accuracy** in semantic tasks (ranking, retrieval, clustering).\n",
    "* Pretrained models available ‚Äî no need to train from scratch.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Disadvantages:\n",
    "\n",
    "* Computationally heavier than word embeddings.\n",
    "* Needs a **transformer model** under the hood ‚Äî can be slow in large-scale real-time apps.\n",
    "* May not work well on **non-standard sentence formats** (e.g., bullet points, commands).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üìò **5. Document Embeddings**\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Definitions:\n",
    "\n",
    "* **Document Embeddings** represent an **entire document or paragraph** as a single dense vector.\n",
    "* Unlike sentence embeddings that work on a sentence level, document embeddings handle **longer texts** and capture **global context**.\n",
    "* Can be obtained by:\n",
    "\n",
    "  * üìä Averaging word or sentence embeddings.\n",
    "  * üß† Using specialized models like **Doc2Vec** or **Transformer-based pooling**.\n",
    "  * üì¶ Pretrained encoders (e.g., Sentence-BERT for multi-sentence input).\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Real-time Use Case:\n",
    "\n",
    "* üìÇ Document classification (e.g., contracts, medical reports).\n",
    "* üìë Legal document similarity (e.g., matching clauses).\n",
    "* üìö Semantic search over long-form text (news, research papers).\n",
    "* üß† Input representation for LLM pipelines (e.g., RAG, retrieval-augmented generation).\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå When Not to Use:\n",
    "\n",
    "* When **fine-grained sentence-level understanding** is needed (e.g., QA).\n",
    "* In low-resource or **edge environments** ‚Äî document embeddings may be too large or slow.\n",
    "* When document lengths **exceed model max tokens** (common with BERT-based encoders).\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Code Implementation:\n",
    "\n",
    "**Approach 1: Average of Sentence Embeddings (using SBERT)**\n",
    "\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Break doc into sentences\n",
    "document = [\n",
    "    \"ChatGPT is great at NLP.\",\n",
    "    \"It can summarize, answer questions, and more.\",\n",
    "    \"The models are based on transformers.\"\n",
    "]\n",
    "\n",
    "sentence_vectors = model.encode(document)\n",
    "document_vector = np.mean(sentence_vectors, axis=0)\n",
    "\n",
    "print(document_vector[:5])  # First 5 values of document vector\n",
    "```\n",
    "\n",
    "**Approach 2: Using Doc2Vec (via Gensim):**\n",
    "\n",
    "```python\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "docs = [\n",
    "    \"Deep learning is revolutionizing NLP.\",\n",
    "    \"Transformers have replaced RNNs.\"\n",
    "]\n",
    "\n",
    "tagged_data = [TaggedDocument(words=doc.lower().split(), tags=[str(i)]) for i, doc in enumerate(docs)]\n",
    "model = Doc2Vec(tagged_data, vector_size=100, window=5, min_count=1, epochs=20)\n",
    "\n",
    "vector = model.infer_vector(\"NLP models like BERT are powerful\".lower().split())\n",
    "print(vector[:5])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Advantages:\n",
    "\n",
    "* Encodes **longer context** than single-sentence models.\n",
    "* Ideal for **document-level classification, clustering, retrieval**.\n",
    "* Can be used in combination with **vector databases** for large-scale search (e.g., FAISS, ChromaDB, Pinecone).\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Disadvantages:\n",
    "\n",
    "* May lose important **sentence-level granularity**.\n",
    "* Transformer-based encoders may **truncate long documents** (e.g., BERT limits to 512 tokens).\n",
    "* Doc2Vec has **weaker performance** than newer transformer-based methods.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Machine Learning for NLP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ‚úÖ **1. Text Classification (Spam Detection, Intent Classification)**\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Definitions:\n",
    "\n",
    "* **Text Classification** is the process of assigning **labels or categories** to text data based on its content.\n",
    "* It uses NLP features (like TF-IDF, embeddings) combined with **ML models** like:\n",
    "\n",
    "  * Logistic Regression\n",
    "  * Naive Bayes\n",
    "  * SVM\n",
    "  * Random Forest\n",
    "  * Gradient Boosting (e.g., XGBoost)\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Real-time Use Case:\n",
    "\n",
    "* üì© **Spam detection** in emails.\n",
    "* üìû **Intent detection** in customer service chatbots (e.g., ‚Äúrefund‚Äù, ‚Äúorder status‚Äù).\n",
    "* üß† **Topic classification** of news articles or support tickets.\n",
    "* üîç **Toxic comment classification** in social media moderation.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå When Not to Use:\n",
    "\n",
    "* When the task requires **contextual understanding** or **long dependencies** ‚Üí prefer deep learning (RNNs, BERT).\n",
    "* For **zero-shot** or **multi-label** classification ‚Äî ML models need fixed labels.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Code Implementation:\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "texts = [\"I need a refund\", \"Your product is great\", \"Where is my order?\"]\n",
    "labels = [\"refund\", \"praise\", \"order_status\"]\n",
    "\n",
    "# Define ML pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipeline.fit(texts, labels)\n",
    "\n",
    "# Predict\n",
    "test_text = [\"How do I return my item?\"]\n",
    "predicted = pipeline.predict(test_text)\n",
    "print(predicted)  # Output: ['refund']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Advantages:\n",
    "\n",
    "* **Fast and interpretable** with models like Naive Bayes or Logistic Regression.\n",
    "* Works well on **small to medium datasets**.\n",
    "* Easy to **train, evaluate, and deploy** using `scikit-learn`.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Disadvantages:\n",
    "\n",
    "* Requires **manual feature engineering** (BoW, TF-IDF, POS tags, etc.).\n",
    "* Struggles with **complex syntax** or **long-range dependencies**.\n",
    "* Doesn‚Äôt support **contextual understanding** ‚Äî ‚Äúbank‚Äù (river vs finance) confusion.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üîç **2. Sentiment Analysis (BoW/TF-IDF + ML)**\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Definitions:\n",
    "\n",
    "* **Sentiment Analysis** determines whether a piece of text expresses a **positive, negative, or neutral** sentiment.\n",
    "* Often modeled as a **binary** or **multi-class classification** problem.\n",
    "* Features like **TF-IDF**, **BoW**, or **lexicon scores** are used with:\n",
    "\n",
    "  * Logistic Regression / SVM\n",
    "  * Naive Bayes\n",
    "  * Random Forest / XGBoost\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Real-time Use Case:\n",
    "\n",
    "* üõçÔ∏è **Customer review analysis** (e.g., Amazon, Yelp, TripAdvisor)\n",
    "* üìà **Social media monitoring** (e.g., brand sentiment on Twitter)\n",
    "* üí¨ **User feedback classification** in product surveys\n",
    "* üé¨ **Movie or product rating predictions**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå When Not to Use:\n",
    "\n",
    "* In texts with **ambiguous or sarcastic language** (e.g., ‚ÄúGreat, it broke in a day üôÑ‚Äù)\n",
    "* If you require **fine-grained emotion detection** (e.g., anger, fear, joy) ‚Äî go for deep learning\n",
    "* When **context shifts meaning** (e.g., ‚Äúnot bad‚Äù means good)\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Code Implementation (TF-IDF + Logistic Regression):\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "texts = [\n",
    "    \"I love this product!\",\n",
    "    \"Worst experience ever.\",\n",
    "    \"Not bad, could be better.\",\n",
    "    \"Amazing quality and service.\",\n",
    "    \"I hate it.\"\n",
    "]\n",
    "labels = [1, 0, 1, 1, 0]  # 1 = positive, 0 = negative\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "# Predict sentiment\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "pred = model.predict(X_test_vec)\n",
    "print(pred)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Advantages:\n",
    "\n",
    "* Works well for **short and direct text** (e.g., product reviews, tweets).\n",
    "* **Fast to train** and doesn‚Äôt require large hardware resources.\n",
    "* TF-IDF-based models are **interpretable** (you can inspect important terms).\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Disadvantages:\n",
    "\n",
    "* **Misses context, sarcasm, negation** (e.g., ‚Äúnot good‚Äù ‚â† ‚Äúgood‚Äù).\n",
    "* Struggles with **long-form opinions** or **mixed sentiments**.\n",
    "* Needs **large and well-labeled datasets** to generalize well.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üóÇÔ∏è **3. Topic Modeling (LDA, NMF)**\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Definitions:\n",
    "\n",
    "* **Topic Modeling** is an unsupervised learning method to **automatically discover hidden themes (topics)** in a collection of documents.\n",
    "* Each document is modeled as a **distribution of topics**, and each topic as a **distribution of words**.\n",
    "* Most commonly used algorithms:\n",
    "\n",
    "  * üìö **LDA (Latent Dirichlet Allocation)**\n",
    "  * üß† **NMF (Non-negative Matrix Factorization)**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Real-time Use Case:\n",
    "\n",
    "* üìö **News/media**: Group articles into categories (e.g., sports, politics).\n",
    "* üí¨ **Support tickets**: Identify major pain points or common issues.\n",
    "* üìà **Market research**: Analyze large-scale customer reviews or survey feedback.\n",
    "* üèõÔ∏è **Legal & academic corpora**: Discover recurring themes in case files or research papers.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå When Not to Use:\n",
    "\n",
    "* When you need **specific labels** ‚Äî topic modeling is not supervised.\n",
    "* On **very short documents** (like tweets or titles) ‚Äî lacks word co-occurrence strength.\n",
    "* When real-world topics don‚Äôt **align with mathematical topics** ‚Äî manual tuning/labeling is still needed.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Code Implementation (LDA via `scikit-learn`):\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "docs = [\n",
    "    \"The government passed a new law in the parliament.\",\n",
    "    \"The match between India and Australia was thrilling.\",\n",
    "    \"Politics and governance are critical in democracy.\",\n",
    "    \"The cricket world cup was watched by millions.\",\n",
    "]\n",
    "\n",
    "# Convert to BoW\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(docs)\n",
    "\n",
    "# Apply LDA\n",
    "lda = LatentDirichletAllocation(n_components=2, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "# Show top words per topic\n",
    "words = vectorizer.get_feature_names_out()\n",
    "for i, topic in enumerate(lda.components_):\n",
    "    top_words = [words[i] for i in topic.argsort()[-5:]]\n",
    "    print(f\"Topic {i+1}: {top_words}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Advantages:\n",
    "\n",
    "* Reveals **hidden patterns** in large corpora without labels.\n",
    "* Helps with **document clustering, summarization, search enhancement**.\n",
    "* LDA produces **interpretable** topic-word and document-topic distributions.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Disadvantages:\n",
    "\n",
    "* Topics are **abstract** and may not align with human understanding.\n",
    "* Need to pre-define number of topics (`n_components`) ‚Äî tricky to tune.\n",
    "* Less effective on **short texts**, noisy or sparse data.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üì• **4. Text Similarity (Cosine, Jaccard, Embeddings)**\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Definitions:\n",
    "\n",
    "* **Text Similarity** measures how close two pieces of text are in meaning or structure.\n",
    "* Techniques can be:\n",
    "\n",
    "  * üìè **Cosine Similarity** ‚Äì angle between two vector representations.\n",
    "  * üßÆ **Jaccard Similarity** ‚Äì overlap of sets (e.g., common words).\n",
    "  * üß† **Embedding-based Similarity** ‚Äì compare semantic meaning using Word2Vec, BERT, SBERT, etc.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Real-time Use Case:\n",
    "\n",
    "* üìÑ **Plagiarism detection** in assignments or documents.\n",
    "* üí¨ **Duplicate question detection** (e.g., Quora, Stack Overflow).\n",
    "* üîç **Semantic search** ‚Äì match queries with relevant documents.\n",
    "* üõçÔ∏è **Product deduplication** or **review similarity** analysis in e-commerce.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå When Not to Use:\n",
    "\n",
    "* When exact **string match** is sufficient (use Levenshtein instead).\n",
    "* When comparing **non-textual or symbolic data** (e.g., math formulas).\n",
    "* Embedding methods may not work well on **out-of-domain text** without finetuning.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Code Implementation\n",
    "\n",
    "**Method 1: Cosine Similarity using TF-IDF**\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "texts = [\"AI is transforming the world.\", \"The world is being transformed by AI.\"]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "print(f\"Cosine Similarity: {similarity[0][0]:.2f}\")  # Output: ~1.00\n",
    "```\n",
    "\n",
    "**Method 2: Embedding-based Similarity using Sentence-BERT**\n",
    "\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "embeddings = model.encode(texts)\n",
    "cos_sim = util.cos_sim(embeddings[0], embeddings[1])\n",
    "\n",
    "print(f\"Semantic Similarity: {cos_sim.item():.2f}\")\n",
    "```\n",
    "\n",
    "**Method 3: Jaccard Similarity**\n",
    "\n",
    "```python\n",
    "def jaccard_similarity(a, b):\n",
    "    a_set, b_set = set(a.lower().split()), set(b.lower().split())\n",
    "    return len(a_set & b_set) / len(a_set | b_set)\n",
    "\n",
    "score = jaccard_similarity(texts[0], texts[1])\n",
    "print(f\"Jaccard Similarity: {score:.2f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Advantages:\n",
    "\n",
    "* Supports both **shallow (BoW/TF-IDF)** and **deep (SBERT)** similarity.\n",
    "* Versatile for tasks like **duplicate detection, search, and clustering**.\n",
    "* Pretrained models (e.g., SBERT) give **excellent semantic accuracy**.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Disadvantages:\n",
    "\n",
    "* BoW/TF-IDF methods **miss semantic meaning** (e.g., ‚Äúcar‚Äù ‚â† ‚Äúautomobile‚Äù).\n",
    "* Embedding-based approaches can be **slow and memory-heavy**.\n",
    "* Sensitive to **domain mismatch** unless embeddings are fine-tuned.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Deep Learning for NLP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üß± **1. Embedding Layers (`tf.keras.layers.Embedding`)**\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Definitions:\n",
    "\n",
    "* An **Embedding Layer** is a trainable layer in deep learning that converts **integer-encoded words** into **dense vector representations** (embeddings).\n",
    "* Typically used as the **first layer** in text-based neural networks.\n",
    "* It **learns semantic relationships** between words during model training.\n",
    "* Each word index maps to a vector of fixed size (e.g., 100-dim, 300-dim).\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Real-time Use Case:\n",
    "\n",
    "* üåê Used in **text classification**, **sentiment analysis**, **NER**, and **sequence labeling**.\n",
    "* Enables deep learning models to work directly on **raw text input** by converting it to meaningful vectors.\n",
    "* Fine-tuned embeddings used in **chatbots, recommendation systems**, and **RNNs/transformers**.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå When Not to Use:\n",
    "\n",
    "* When using **pretrained embeddings** (e.g., GloVe, Word2Vec) outside the training loop.\n",
    "* For **rule-based** NLP tasks or pipelines not involving deep learning.\n",
    "* If text is already encoded using **BERT or SBERT** (contextual embeddings).\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Code Implementation (with TensorFlow/Keras):\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "# Example settings\n",
    "vocab_size = 5000  # total number of unique tokens\n",
    "embedding_dim = 100  # size of each embedding vector\n",
    "input_length = 20  # length of input sequences\n",
    "\n",
    "# Create a simple embedding model\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=input_length),\n",
    "    Flatten(),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Advantages:\n",
    "\n",
    "* Learns task-specific embeddings **during training**.\n",
    "* Reduces high-dimensional sparse data into **compact, dense representations**.\n",
    "* Easy to integrate in **end-to-end DL pipelines** (LSTM, CNN, Transformers).\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Disadvantages:\n",
    "\n",
    "* Needs a **large amount of labeled data** to learn good embeddings.\n",
    "* Doesn't capture **contextual meaning** ‚Äî same vector for \"bank\" in both \"river bank\" and \"money bank\".\n",
    "* Requires proper **padding, masking**, and **tokenization setup**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üîÅ **2. LSTM & GRU (Legacy Deep Learning for Sequences)**\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Definitions:\n",
    "\n",
    "* **LSTM (Long Short-Term Memory)** and **GRU (Gated Recurrent Unit)** are types of **Recurrent Neural Networks (RNNs)** designed to model **sequential data** by maintaining memory over time.\n",
    "* They solve the **vanishing gradient problem** that plagued vanilla RNNs, making them effective for longer sequences.\n",
    "* Widely used before Transformers for tasks like:\n",
    "\n",
    "  * Text classification\n",
    "  * Named Entity Recognition (NER)\n",
    "  * Sentiment analysis\n",
    "  * Sequence prediction\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Real-time Use Case:\n",
    "\n",
    "* üì± Chatbots that **understand conversation history**\n",
    "* üßæ Predicting **next words or characters**\n",
    "* üí¨ **Sequence labeling** like NER, POS tagging\n",
    "* üìù **Text generation** or **sentence completion**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå When Not to Use:\n",
    "\n",
    "* On **very long sequences** (LSTMs are still limited by time-step memory)\n",
    "* When **parallel training** is important (LSTMs/GRUs are sequential ‚Üí slow)\n",
    "* In place of **modern Transformer-based models** (BERT, GPT) when high accuracy is needed\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Code Implementation (with TensorFlow/Keras):\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense\n",
    "\n",
    "vocab_size = 5000\n",
    "embedding_dim = 100\n",
    "input_length = 100\n",
    "\n",
    "# LSTM model\n",
    "lstm_model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=input_length),\n",
    "    LSTM(64),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# GRU model\n",
    "gru_model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=input_length),\n",
    "    GRU(64),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "gru_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "lstm_model.summary()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Advantages:\n",
    "\n",
    "* Handles **sequence dependencies** better than vanilla RNNs.\n",
    "* GRUs are **faster and simpler** than LSTMs (fewer parameters).\n",
    "* Still effective for **moderate-length** sequence tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Disadvantages:\n",
    "\n",
    "* Hard to parallelize due to **sequential processing**.\n",
    "* Can't fully capture **very long-term dependencies** like Transformers.\n",
    "* Needs **careful tuning** (e.g., dropout, learning rate, sequence length).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üîÑ **3. Bi-LSTM + Attention (For NER, Sequence Labeling)**\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Definitions:\n",
    "\n",
    "* **Bi-LSTM** (Bidirectional LSTM) processes input sequences in **both forward and backward** directions to capture **past and future context**.\n",
    "* **Attention Mechanism** learns to **focus** on the most relevant parts of the input sequence for each output step.\n",
    "* Together, they form a strong architecture for:\n",
    "\n",
    "  * üîñ Named Entity Recognition (NER)\n",
    "  * üß© Part-of-Speech Tagging\n",
    "  * üß† Text summarization\n",
    "  * üóÉÔ∏è Sequence classification with interpretable attention weights\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Real-time Use Case:\n",
    "\n",
    "* üßæ **NER models** for resumes, invoices, legal docs.\n",
    "* üí¨ **Question answering** over context paragraphs.\n",
    "* üìã **Medical/clinical text labeling**.\n",
    "* üß† **Custom attention-based entity extraction** in financial or domain-specific texts.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå When Not to Use:\n",
    "\n",
    "* For **very long documents** ‚Äî Transformer models (e.g., BERT) scale better.\n",
    "* In **low-resource environments**, as attention + BiLSTM increases complexity.\n",
    "* When using **pretrained transformer embeddings** already capturing global context.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Code Implementation (Simplified Bi-LSTM + Attention):\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, Attention\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "vocab_size = 5000\n",
    "embedding_dim = 100\n",
    "input_length = 100\n",
    "\n",
    "# Input layer\n",
    "inputs = Input(shape=(input_length,))\n",
    "embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(inputs)\n",
    "\n",
    "# Bi-LSTM layer\n",
    "bi_lstm = Bidirectional(LSTM(64, return_sequences=True))(embedding)\n",
    "\n",
    "# Attention mechanism (basic)\n",
    "attention = tf.keras.layers.Attention()([bi_lstm, bi_lstm])\n",
    "flattened = tf.keras.layers.GlobalAveragePooling1D()(attention)\n",
    "\n",
    "# Output layer\n",
    "outputs = Dense(1, activation='sigmoid')(flattened)\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Advantages:\n",
    "\n",
    "* **Captures both left and right context** for every word/token.\n",
    "* Attention improves **interpretability** (shows which words influenced the decision).\n",
    "* Highly effective for **token-level** tasks (e.g., NER, QA, labeling).\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Disadvantages:\n",
    "\n",
    "* Computationally **heavier** than simple LSTM/GRU.\n",
    "* Requires **careful attention tuning** and more training time.\n",
    "* **Not parallelizable** like Transformers ‚Üí slower on large datasets.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üìê **4. CNN for Text Classification**\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Definitions:\n",
    "\n",
    "* **Convolutional Neural Networks (CNNs)**, originally designed for images, can be applied to text by treating a sentence as a **1D sequence of word embeddings**.\n",
    "* Filters (kernels) slide over word vectors to detect **local n-gram patterns** (like \"not good\", \"very bad\").\n",
    "* Captures **local features** effectively, making it a solid choice for:\n",
    "\n",
    "  * Sentiment Analysis\n",
    "  * News/Topic Classification\n",
    "  * Toxic Comment Detection\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Real-time Use Case:\n",
    "\n",
    "* üìù **Short text classification** (e.g., tweets, reviews, headlines).\n",
    "* üìÇ **Multi-label classification** (e.g., tagging support tickets).\n",
    "* ‚ö°Ô∏è Use case where **speed matters** ‚Äî CNNs are highly parallelizable and fast.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå When Not to Use:\n",
    "\n",
    "* For tasks needing **long-range dependencies** (e.g., question answering, summarization).\n",
    "* For **sequence labeling tasks** (e.g., NER, POS tagging) ‚Äî CNNs don‚Äôt maintain token alignment.\n",
    "* In place of **transformers** when **contextual meaning** is critical.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Code Implementation (CNN in Keras):\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "\n",
    "vocab_size = 5000\n",
    "embedding_dim = 100\n",
    "input_length = 100\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, embedding_dim, input_length=input_length),\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(10, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Advantages:\n",
    "\n",
    "* **Fast training and inference** due to parallelism.\n",
    "* Detects **local patterns** like phrases or negation very effectively.\n",
    "* Excellent **baseline model** for classification problems.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Disadvantages:\n",
    "\n",
    "* No awareness of **word order beyond filter window**.\n",
    "* Doesn‚Äôt capture **long-distance dependencies**.\n",
    "* Less interpretable than attention-based or RNN models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üß† **5. Seq2Seq (Encoder‚ÄìDecoder Models)**\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Definitions:\n",
    "\n",
    "* **Seq2Seq (Sequence-to-Sequence)** models are used to convert one sequence into another.\n",
    "* It consists of two main parts:\n",
    "\n",
    "  * üîê **Encoder**: Compresses the input sequence into a fixed context vector.\n",
    "  * üîì **Decoder**: Generates the output sequence token by token using the context.\n",
    "* Often enhanced with **attention mechanisms** to avoid losing information.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Real-time Use Case:\n",
    "\n",
    "* üåê **Machine Translation** (e.g., English ‚ûù French)\n",
    "* üìù **Text Summarization**\n",
    "* üí¨ **Chatbot reply generation**\n",
    "* üßæ **Grammatical error correction**, **paraphrase generation**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå When Not to Use:\n",
    "\n",
    "* For **classification or token-labeling tasks** (e.g., NER, sentiment).\n",
    "* When the output length is **fixed** ‚Äî simpler models can work.\n",
    "* If latency is critical ‚Äî seq2seq with attention can be **slow at inference** (especially with beam search).\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Code Implementation (Basic LSTM Seq2Seq in Keras):\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Define input and output sequence sizes\n",
    "encoder_input = Input(shape=(None, 256))   # e.g., word embedding input\n",
    "encoder_lstm = LSTM(128, return_state=True)\n",
    "encoder_output, state_h, state_c = encoder_lstm(encoder_input)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder receives the encoder's final state\n",
    "decoder_input = Input(shape=(None, 256))\n",
    "decoder_lstm = LSTM(128, return_sequences=True, return_state=True)\n",
    "decoder_output, _, _ = decoder_lstm(decoder_input, initial_state=encoder_states)\n",
    "decoder_dense = Dense(1000, activation='softmax')  # vocab size = 1000\n",
    "decoder_output = decoder_dense(decoder_output)\n",
    "\n",
    "# Define model\n",
    "model = Model([encoder_input, decoder_input], decoder_output)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Advantages:\n",
    "\n",
    "* Can handle **variable-length input and output**.\n",
    "* Suitable for **complex generative tasks** like translation or summarization.\n",
    "* With attention, it captures **fine-grained word-to-word alignment**.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Disadvantages:\n",
    "\n",
    "* Encoder-only approach suffers from **information bottleneck** without attention.\n",
    "* Inference is **slow and sequential**, especially for long outputs.\n",
    "* Outperformed by **Transformers** in both accuracy and speed.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. Transformers and Foundation Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ü§ñ **1. BERT, RoBERTa, DistilBERT**\n",
    "\n",
    "> üåü Foundation models for classification, NER, and question answering.\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Definitions:\n",
    "\n",
    "* **BERT** (Bidirectional Encoder Representations from Transformers):\n",
    "\n",
    "  * Developed by Google, it uses **Transformer encoders** and **bidirectional attention**.\n",
    "  * Pretrained using **Masked Language Modeling (MLM)** and **Next Sentence Prediction (NSP)**.\n",
    "* **RoBERTa**:\n",
    "\n",
    "  * A **robustly optimized BERT**, trained longer with **larger data**, without NSP.\n",
    "* **DistilBERT**:\n",
    "\n",
    "  * A **smaller, faster, distilled** version of BERT ‚Äî 40% smaller, 60% faster, 97% performance.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Real-time Use Case:\n",
    "\n",
    "* üì• **Text classification** (spam, sentiment, topics)\n",
    "* üßæ **Named Entity Recognition** (NER)\n",
    "* ‚ùì **Question Answering** (e.g., extractive QA)\n",
    "* üîç **Semantic search** with embeddings\n",
    "* üìä **Document tagging** & **customer intent classification**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå When Not to Use:\n",
    "\n",
    "* For **generating new text** ‚Äî BERT is **not autoregressive** (unlike GPT).\n",
    "* In **low-latency environments** ‚Äî even DistilBERT may be too heavy without optimization.\n",
    "* On **long documents** beyond 512 tokens (unless chunked or extended).\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Code Implementation (Hugging Face for Sentiment Classification):\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load sentiment classifier\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "result = classifier(\"Transformers are amazing for NLP!\")\n",
    "print(result)  # Output: [{'label': 'POSITIVE', 'score': 0.99}]\n",
    "```\n",
    "\n",
    "**NER Example:**\n",
    "\n",
    "```python\n",
    "ner = pipeline(\"ner\", model=\"dslim/bert-base-NER\", grouped_entities=True)\n",
    "print(ner(\"Mukesh works at OpenAI in San Francisco.\"))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Advantages:\n",
    "\n",
    "* **Bidirectional context** ‚Üí deeper understanding of sentence meaning.\n",
    "* Works great for **classification, NER, QA, embeddings**.\n",
    "* Huge support in **Hugging Face Transformers** ecosystem.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Disadvantages:\n",
    "\n",
    "* Cannot perform **text generation** (unlike GPT).\n",
    "* Has a **fixed input length (512 tokens)**.\n",
    "* **Slower inference** for real-time systems without optimization.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üß† **2. GPT Family (GPT-2, GPT-3, GPT-4)**\n",
    "\n",
    "> üó£Ô∏è Foundation of modern **text generation and conversational AI**\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Definitions:\n",
    "\n",
    "* **GPT** stands for **Generative Pretrained Transformer**.\n",
    "* It uses only the **Transformer decoder** architecture and is trained with:\n",
    "\n",
    "  * üìñ **Causal Language Modeling** (predict next token given previous ones).\n",
    "* Versions:\n",
    "\n",
    "  * üîπ **GPT-2** ‚Äì Open-sourced, capable of generating paragraphs of coherent text.\n",
    "  * üîπ **GPT-3** ‚Äì 175B parameters, used in tools like Codex and early ChatGPT.\n",
    "  * üîπ **GPT-4** ‚Äì Multimodal (text + image), more accurate, capable of reasoning & coding.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Real-time Use Case:\n",
    "\n",
    "* üí¨ **Chatbots** and virtual assistants (like ChatGPT)\n",
    "* üß† **Text generation**, completion, and rewriting\n",
    "* üìö **Code generation** (e.g., GitHub Copilot)\n",
    "* üìà **Data-to-text** generation (e.g., turning charts into summaries)\n",
    "* üßæ **Email drafting**, **summarization**, **creative writing**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå When Not to Use:\n",
    "\n",
    "* Tasks requiring **strict factual accuracy** without a knowledge base.\n",
    "* For **structured predictions** (like NER or classification) unless prompted carefully.\n",
    "* When **output size** or **generation time** must be tightly controlled.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Code Implementation (Hugging Face with GPT-2):\n",
    "\n",
    "```python\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "input_text = \"Artificial intelligence is transforming\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "output = model.generate(input_ids, max_length=30, num_return_sequences=1)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "```\n",
    "\n",
    "**Chat Interface (via OpenAI GPT-3.5 / GPT-4):**\n",
    "\n",
    "```python\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"your_api_key\"\n",
    "\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain black holes in simple words.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Advantages:\n",
    "\n",
    "* **Generates fluent, creative, context-aware text**.\n",
    "* Powers **general-purpose chatbots**, writing tools, coding copilots.\n",
    "* Supports **few-shot and zero-shot learning** with prompts.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Disadvantages:\n",
    "\n",
    "* **High resource usage** (memory, GPU, latency).\n",
    "* May produce **hallucinations** or **inaccurate facts**.\n",
    "* Not ideal for tasks needing **structured output** unless carefully engineered.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üîÑ **3. T5 and BART ‚Äî Text-to-Text Transformers**\n",
    "\n",
    "> üîÅ Unified format for **summarization**, **translation**, **question answering**, and more\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Definitions:\n",
    "\n",
    "* **T5 (Text-To-Text Transfer Transformer)**:\n",
    "\n",
    "  * Google‚Äôs model that **frames all NLP tasks as text-to-text**, e.g.,\n",
    "    `\"summarize: input text\"` ‚ûù `\"summary\"`\n",
    "    `\"translate English to German: text\"` ‚ûù `\"√ºbersetzt\"`\n",
    "  * Trained on **Colossal Clean Crawled Corpus (C4)** with **multi-task learning**.\n",
    "* **BART (Bidirectional and Auto-Regressive Transformer)**:\n",
    "\n",
    "  * Facebook‚Äôs model combining **BERT‚Äôs encoder** and **GPT‚Äôs decoder**.\n",
    "  * Trained by **corrupting input text** and learning to reconstruct it.\n",
    "  * Excellent for **summarization**, **paraphrasing**, and **text infilling**.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Real-time Use Case:\n",
    "\n",
    "* üìù **Summarizing articles, legal docs, transcripts**\n",
    "* üåê **Language translation**\n",
    "* ‚ùì **Open-domain Q\\&A** from documents\n",
    "* üí° **Text rewriting/paraphrasing**\n",
    "* üß™ Used in **RAG pipelines** for retrieval + generation\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå When Not to Use:\n",
    "\n",
    "* For **classification or embedding-only tasks** ‚Äî overkill.\n",
    "* In **real-time edge devices** ‚Äî these are large models with higher latency.\n",
    "* When **controlling output length/format strictly** ‚Äî needs careful prompting.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Code Implementation (Hugging Face Transformers):\n",
    "\n",
    "**T5 Summarization Example:**\n",
    "\n",
    "```python\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "text = \"summarize: The stock market crashed after a sudden decline in tech shares.\"\n",
    "\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "output_ids = model.generate(input_ids, max_length=50, num_beams=4, early_stopping=True)\n",
    "\n",
    "summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(summary)\n",
    "```\n",
    "\n",
    "**BART Paraphrasing Example:**\n",
    "\n",
    "```python\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "input_ids = tokenizer(sentence, return_tensors=\"pt\").input_ids\n",
    "summary_ids = model.generate(input_ids, num_beams=4, max_length=40, early_stopping=True)\n",
    "\n",
    "print(tokenizer.decode(summary_ids[0], skip_special_tokens=True))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Advantages:\n",
    "\n",
    "* **Unified framework** ‚Äî same model for many tasks via prompting.\n",
    "* Excellent for **summarization**, **Q\\&A**, **translation**, **text cleaning**.\n",
    "* Supports **longer input sequences** than vanilla BERT.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Disadvantages:\n",
    "\n",
    "* Requires **specific prompt formats** (e.g., ‚Äúsummarize: ‚Ä¶‚Äù).\n",
    "* Inference can be **slow** without model quantization or acceleration.\n",
    "* Can **generate generic/boilerplate outputs** if not fine-tuned well.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üß™ **4. LLaMA, Falcon, Mistral ‚Äî Open Source LLMs**\n",
    "\n",
    "> üß† Foundation models for building **private GenAI apps** & **fine-tuned NLP solutions**\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Definitions:\n",
    "\n",
    "* **LLaMA (Large Language Model Meta AI)**:\n",
    "\n",
    "  * Released by Meta (LLaMA 1, 2, and 3)\n",
    "  * Focused on being **efficient, open, and adaptable**.\n",
    "  * Comes in multiple sizes (7B, 13B, 65B) and supports **chat, generation, coding**.\n",
    "\n",
    "* **Falcon (from TII - UAE)**:\n",
    "\n",
    "  * High-performance LLMs for **commercial and research** use.\n",
    "  * Known for Falcon-7B and Falcon-40B.\n",
    "\n",
    "* **Mistral (France-based)**:\n",
    "\n",
    "  * Focused on **small, fast, high-quality** models.\n",
    "  * **Mistral-7B** and **Mixtral 8x7B** (mixture of experts architecture).\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Real-time Use Case:\n",
    "\n",
    "* üè¢ **Private LLM deployment** (on-prem or in VPC)\n",
    "* üß† **Domain-specific finetuning** (legal, medical, finance)\n",
    "* üóÇÔ∏è **Enterprise search**, summarization, and document Q\\&A\n",
    "* ü§ñ **Building custom chatbots** with control over data and behavior\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå When Not to Use:\n",
    "\n",
    "* If you require **plug-and-play performance** (these models often need finetuning).\n",
    "* On **resource-constrained hardware** (unless quantized).\n",
    "* For **multi-modal tasks** ‚Äî these are usually text-only models.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Code Implementation (Using `transformers` and `auto-gptq`):\n",
    "\n",
    "**Example: Running LLaMA 2 (quantized):**\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_name = \"TheBloke/Llama-2-7B-Chat-GPTQ\"  # Quantized model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "\n",
    "prompt = \"Explain quantum computing in simple terms.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Advantages:\n",
    "\n",
    "* Fully **open-source and auditable**\n",
    "* Suitable for **finetuning on custom data** (no vendor lock-in)\n",
    "* Can be deployed on **local GPUs**, **cloud clusters**, or **edge devices**\n",
    "* Increasingly **competitive with proprietary models**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Disadvantages:\n",
    "\n",
    "* Typically require **manual setup** (quantization, tokenizer tweaks, adapters)\n",
    "* May **underperform out-of-the-box** compared to GPT-4 or Claude\n",
    "* Still evolving ‚Äî fewer tools/integrations than OpenAI API\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üì¶ **5. Hugging Face Transformers**\n",
    "\n",
    "> üöÄ Python library for using, training, and deploying **state-of-the-art transformer models**\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Definitions:\n",
    "\n",
    "* **Hugging Face Transformers** is an open-source library that provides:\n",
    "\n",
    "  * 100,000+ **pretrained models** for text, vision, audio, and multimodal tasks.\n",
    "  * A unified API to work with models like **BERT, GPT, T5, BART, RoBERTa, LLaMA**, and more.\n",
    "  * Easy integration with **TensorFlow**, **PyTorch**, and **JAX**.\n",
    "  * Built-in support for **pipelines**, **tokenizers**, **datasets**, and **training loops**.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Real-time Use Case:\n",
    "\n",
    "* üß† Rapid prototyping of NLP tasks (text classification, QA, summarization)\n",
    "* üõ†Ô∏è Fine-tuning LLMs on **domain-specific data**\n",
    "* üìà RAG pipelines for **enterprise search + generation**\n",
    "* üí¨ Building **chatbots, agents, content tools** in production\n",
    "* üß™ Hosting, evaluating, and sharing models via ü§ó **Model Hub**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå When Not to Use:\n",
    "\n",
    "* On **ultra-low resource devices** (unless using optimized/quantized models)\n",
    "* If **custom transformer implementation** is required from scratch\n",
    "* Not suitable for **non-Transformer architectures**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Key Components:\n",
    "\n",
    "| Feature                      | Description                                                     |\n",
    "| ---------------------------- | --------------------------------------------------------------- |\n",
    "| `transformers.pipeline()`    | Prebuilt pipelines for common tasks like QA, summarization, NER |\n",
    "| `AutoModel`, `AutoTokenizer` | Automatically loads model/tokenizer from name or path           |\n",
    "| `Trainer` API                | Handles training/finetuning with built-in eval, logging         |\n",
    "| `Datasets`                   | Integration with Hugging Face `datasets` library                |\n",
    "| `Model Hub`                  | Thousands of pretrained + finetuned models available            |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Example: Text Summarization with T5\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"t5-small\")\n",
    "text = \"Artificial intelligence is transforming industries by automating tasks and improving decision-making.\"\n",
    "summary = summarizer(text, max_length=30, min_length=5, do_sample=False)\n",
    "print(summary[0]['summary_text'])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Advantages:\n",
    "\n",
    "* üìö **Plug-and-play** access to state-of-the-art models\n",
    "* üîß Simplifies **finetuning and deployment**\n",
    "* üåç Huge community & contributions ‚Äî constantly updated\n",
    "* üß™ Supports **quantization, PEFT (LoRA), ONNX, inference APIs**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Disadvantages:\n",
    "\n",
    "* Some models are **very large** (RAM/GPU heavy)\n",
    "* May need **fine-grained customization** for advanced use cases\n",
    "* Hugging Face Hub-based workflows may not suit **highly regulated orgs**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **6. Key NLP Tasks in Industry**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üßæ **1. Sentiment Analysis**\n",
    "\n",
    "> üß† Helps businesses understand **customer emotion** and **public opinion**\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Definitions:\n",
    "\n",
    "* **Sentiment Analysis** (aka opinion mining) is the task of determining whether a piece of text expresses a **positive**, **negative**, or **neutral** sentiment.\n",
    "* It's a **classification problem** often solved using:\n",
    "\n",
    "  * üß™ Rule-based models (lexicons like VADER, TextBlob)\n",
    "  * üìä Machine Learning (TF-IDF + SVM, Logistic Regression)\n",
    "  * üß† Deep Learning & Transformers (BERT, RoBERTa, DistilBERT)\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Real-time Use Case:\n",
    "\n",
    "* üõçÔ∏è **E-commerce**: Analyze customer reviews or product feedback.\n",
    "* üìâ **Financial news sentiment**: Predict market reactions to headlines.\n",
    "* üè• **Healthcare**: Gauge emotional tone in patient notes or feedback.\n",
    "* üì± **Social media monitoring**: Track brand perception on Twitter, Reddit, etc.\n",
    "* üìû **Call center logs**: Detect frustration or satisfaction in transcripts.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå When Not to Use:\n",
    "\n",
    "* For **long documents** with **multiple conflicting sentiments** (use chunking or aspect-based SA instead).\n",
    "* When domain-specific **sarcasm or irony** is common.\n",
    "* In multilingual settings without **language-aware models**.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Code Implementation (with Hugging Face ü§ó DistilBERT):\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load pre-trained sentiment classifier\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "texts = [\n",
    "    \"This product exceeded my expectations!\",\n",
    "    \"Customer service was terrible and unhelpful.\"\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    result = classifier(text)[0]\n",
    "    print(f\"'{text}' ‚Üí {result['label']} (Score: {result['score']:.2f})\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Advantages:\n",
    "\n",
    "* **Automates large-scale feedback analysis**\n",
    "* Scalable across **multiple industries and platforms**\n",
    "* Pretrained models give **strong results out of the box**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Disadvantages:\n",
    "\n",
    "* May struggle with **domain-specific slang**, idioms, sarcasm.\n",
    "* Accuracy depends heavily on **training data quality** and **label balance**.\n",
    "* Sentiment **can shift over time** ‚Äî requires retraining or continual learning.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üóÇÔ∏è **2. Document Classification**\n",
    "\n",
    "> üè∑Ô∏è Automatically assigns **categories or labels** to unstructured documents\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Definitions:\n",
    "\n",
    "* **Document Classification** is the process of assigning a document to one or more predefined **categories** based on its content.\n",
    "* It's a **supervised learning** task where models learn from labeled documents.\n",
    "* Techniques range from:\n",
    "\n",
    "  * üìä **TF-IDF + ML algorithms** (Logistic Regression, SVM)\n",
    "  * üß† **Deep learning** (CNNs, RNNs)\n",
    "  * ü§ñ **Transformers** (BERT, RoBERTa)\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Real-time Use Case:\n",
    "\n",
    "* üßæ **Resume classification**: Job role, experience level, skill tags\n",
    "* üì© **Support ticket routing**: Billing, Technical, General Inquiry\n",
    "* üìä **News categorization**: Politics, Sports, Finance\n",
    "* üè• **Medical reports**: Disease category, diagnosis type\n",
    "* üìÅ **Legal documents**: Contract type, case classification\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå When Not to Use:\n",
    "\n",
    "* On **very short documents** (need enough signal in text)\n",
    "* For **multi-label problems** without proper label encoding\n",
    "* When categories are **not well-defined** or highly overlapping\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Code Implementation (Using BERT via Hugging Face ü§ó):\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load zero-shot classifier for flexible categories\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "text = \"Please help, I can't access my account and the password reset link isn't working.\"\n",
    "\n",
    "labels = [\"Technical Issue\", \"Billing\", \"General Inquiry\", \"Login Problem\"]\n",
    "result = classifier(text, candidate_labels=labels)\n",
    "\n",
    "print(\"Top predicted label:\", result['labels'][0])\n",
    "```\n",
    "\n",
    "> ‚úÖ You can also fine-tune models like `bert-base-uncased` for **custom classification** tasks using your own dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Advantages:\n",
    "\n",
    "* Speeds up **workflow automation** (ticket routing, resume screening)\n",
    "* Can handle **large document volumes** in real-time\n",
    "* Works well with **fine-tuned transformer models** for high accuracy\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Disadvantages:\n",
    "\n",
    "* Needs **high-quality labeled data** for best results\n",
    "* Poor performance if **labels are ambiguous** or **too granular**\n",
    "* Transformer-based models may be **slow for very large documents**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üìÑ **3. Named Entity Recognition (NER)**\n",
    "\n",
    "> üß† Extracts **real-world entities** (names, dates, locations, orgs, etc.) from unstructured text\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Definitions:\n",
    "\n",
    "* **Named Entity Recognition (NER)** is a **sequence labeling** task that identifies and classifies named entities in text into predefined categories:\n",
    "\n",
    "  * üë§ Person\n",
    "  * üè¢ Organization\n",
    "  * üìç Location\n",
    "  * üìÖ Date/Time\n",
    "  * üí≤ Money/Percent\n",
    "  * üßæ Custom entities (e.g., product names, invoice numbers)\n",
    "\n",
    "* Usually solved using:\n",
    "\n",
    "  * üß† **BiLSTM + CRF**\n",
    "  * üîÑ **BiLSTM + Attention**\n",
    "  * ü§ñ **Transformers (BERT, RoBERTa, SpaCy)**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Real-time Use Case:\n",
    "\n",
    "* üìÑ **Resume parsing** ‚Äì extract names, emails, universities, skills\n",
    "* üßæ **Invoice/document automation** ‚Äì extract company, amount, invoice #, etc.\n",
    "* üí¨ **Chatbot memory** ‚Äì remember user names, preferences, locations\n",
    "* üè• **Medical NER** ‚Äì extract diseases, symptoms, medications\n",
    "* üß† **Knowledge graph construction** ‚Äì structure facts from raw data\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå When Not to Use:\n",
    "\n",
    "* If text contains **non-standard formatting** (OCR noise, scanned docs)\n",
    "* For **generic entity types** without clear labeling guidelines\n",
    "* When **training data is scarce** or **highly domain-specific**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Code Implementation (Hugging Face Pipeline):\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "ner = pipeline(\"ner\", model=\"dslim/bert-base-NER\", grouped_entities=True)\n",
    "\n",
    "text = \"Mukesh Yadav joined OpenAI in San Francisco on 18th June 2024 as a research engineer.\"\n",
    "\n",
    "entities = ner(text)\n",
    "\n",
    "for e in entities:\n",
    "    print(f\"{e['word']} ‚Üí {e['entity_group']} (Score: {e['score']:.2f})\")\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```\n",
    "Mukesh Yadav ‚Üí PER (0.99)\n",
    "OpenAI ‚Üí ORG (0.99)\n",
    "San Francisco ‚Üí LOC (0.99)\n",
    "18th June 2024 ‚Üí DATE (0.98)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Advantages:\n",
    "\n",
    "* Extracts **structured insights** from free-form text\n",
    "* Works well with **fine-tuned BERT/Roberta models**\n",
    "* Can be **custom-trained** for niche entities (medical, financial, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Disadvantages:\n",
    "\n",
    "* Needs **high-quality token-level annotation** to train\n",
    "* May struggle with **nested entities** or **overlapping spans**\n",
    "* Domain shift can cause **entity confusion or drift**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ‚ùì **4. Question Answering (QA)**\n",
    "\n",
    "> ü§ñ Models that can **read a passage** and **answer questions** about it\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Definitions:\n",
    "\n",
    "* **Question Answering (QA)** is the task where a model is given a **context paragraph** and a **question**, and it must return the **most relevant answer span** or **generate an answer**.\n",
    "* Two main types:\n",
    "\n",
    "  * üìç **Extractive QA** ‚Äì Answer is a **span from the context** (e.g., SQuAD format)\n",
    "  * ‚úçÔ∏è **Generative QA** ‚Äì Model **generates answers** (can work without explicit span)\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Real-time Use Case:\n",
    "\n",
    "* üß† **Search engine QA** (e.g., Google‚Äôs \"People also ask\")\n",
    "* üí¨ **Chatbots with knowledge retrieval**\n",
    "* üìÑ **Document-based QA** (contracts, reports, policies)\n",
    "* üìö **Customer support automation**\n",
    "* üßæ **Enterprise RAG** (Retrieval-Augmented Generation) pipelines\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå When Not to Use:\n",
    "\n",
    "* On **very long documents** without chunking or retrieval\n",
    "* If **ground truth answers are ambiguous** or not clearly defined\n",
    "* In real-time settings without **latency optimization**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Code Implementation (Hugging Face Extractive QA with BERT):\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "qa = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
    "\n",
    "context = \"\"\"Mukesh Yadav is a research engineer at OpenAI. He works on large language models and generative AI applications.\"\"\"\n",
    "question = \"Where does Mukesh Yadav work?\"\n",
    "\n",
    "result = qa(question=question, context=context)\n",
    "print(f\"Answer: {result['answer']} (Score: {result['score']:.2f})\")\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```\n",
    "Answer: OpenAI (Score: 0.98)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Advantages:\n",
    "\n",
    "* Gives **precise answers** from context, not just classifications\n",
    "* Works well for **document QA, support bots, RAG**\n",
    "* Easy to build with **pretrained BERT, RoBERTa, DistilBERT models**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Disadvantages:\n",
    "\n",
    "* Limited to **short inputs (\\~512 tokens)** without chunking\n",
    "* Can return **incorrect spans** if question is tricky\n",
    "* For **open-domain QA**, it needs **retrievers or search index**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üìâ **5. Text Summarization**\n",
    "\n",
    "> üìù Condenses long documents into short, **coherent summaries**\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Definitions:\n",
    "\n",
    "* **Text Summarization** is the task of creating a **shorter version** of a long document while preserving its **key meaning**.\n",
    "* Two major types:\n",
    "\n",
    "  * üßæ **Extractive**: Pulls key sentences from the original (e.g., TextRank, LexRank)\n",
    "  * ‚úçÔ∏è **Abstractive**: **Generates new phrases** using deep learning (e.g., T5, BART, Pegasus)\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Real-time Use Case:\n",
    "\n",
    "* üóûÔ∏è **News summarization** (summarize long articles)\n",
    "* üßæ **Legal and financial docs** (highlight key clauses/figures)\n",
    "* üß† **Meeting transcripts** (TL;DR-style recaps)\n",
    "* üìö **Research paper summaries**\n",
    "* üì© **Email thread condensation**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå When Not to Use:\n",
    "\n",
    "* On **noisy or unstructured text** (e.g., OCR without cleaning)\n",
    "* When **verbatim accuracy** is critical (use extractive over abstractive)\n",
    "* In **real-time low-latency apps** ‚Äî summarization can be compute-intensive\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Code Implementation (Abstractive with T5):\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"t5-small\")\n",
    "\n",
    "text = \"\"\"\n",
    "Large language models have revolutionized natural language processing. They are capable of tasks ranging from text generation to summarization and question answering. Despite their success, challenges remain in areas such as bias, hallucination, and resource constraints.\n",
    "\"\"\"\n",
    "\n",
    "summary = summarizer(text, max_length=50, min_length=20, do_sample=False)\n",
    "print(summary[0]['summary_text'])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Advantages:\n",
    "\n",
    "* Saves **reading time** by reducing large content into digestible pieces\n",
    "* Powerful with **transformers like BART, T5, Pegasus**\n",
    "* Enables **faster insights** in legal, healthcare, customer service, and research\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Disadvantages:\n",
    "\n",
    "* **Abstractive models** may generate **inaccurate or hallucinated info**\n",
    "* Needs **large compute** for long docs unless optimized\n",
    "* Evaluation of summaries can be **subjective** (not always a single correct answer)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üìè **6. Text Similarity & Semantic Search**\n",
    "\n",
    "> üîç Measures **how similar** two pieces of text are, or finds **closest matches**\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Definitions:\n",
    "\n",
    "* **Text Similarity** determines how **closely related** two texts are in meaning.\n",
    "* **Semantic Search** finds the **most relevant documents** for a query by comparing **embeddings** instead of keywords.\n",
    "* Approaches:\n",
    "\n",
    "  * üìä **Traditional**: Cosine / Jaccard similarity on TF-IDF vectors\n",
    "  * üß† **Deep Learning**: Use **sentence embeddings** (BERT, SBERT, USE)\n",
    "  * üîç **Vector Search**: Combine with FAISS, Pinecone, ChromaDB for fast retrieval\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Real-time Use Case:\n",
    "\n",
    "* üîç **Enterprise search** (semantic document lookup)\n",
    "* üõí **Product recommendation** based on query similarity\n",
    "* üìö **FAQ matching** ‚Äì find best prewritten answer\n",
    "* ‚úçÔ∏è **Plagiarism or paraphrase detection**\n",
    "* üß† **Clustering similar documents** for topic modeling\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå When Not to Use:\n",
    "\n",
    "* On **short, vague text** (e.g., one-word queries)\n",
    "* When **exact matching or keyword rules** are required\n",
    "* For tasks that require **factual reasoning**, not just similarity\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Code Implementation (Using Sentence-BERT for Semantic Similarity):\n",
    "\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Two example sentences\n",
    "s1 = \"What is the fastest car in the world?\"\n",
    "s2 = \"Which car holds the record for highest speed?\"\n",
    "\n",
    "# Get embeddings\n",
    "emb1 = model.encode(s1, convert_to_tensor=True)\n",
    "emb2 = model.encode(s2, convert_to_tensor=True)\n",
    "\n",
    "# Compute cosine similarity\n",
    "score = util.pytorch_cos_sim(emb1, emb2)\n",
    "print(f\"Similarity Score: {score.item():.2f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Advantages:\n",
    "\n",
    "* Captures **semantic meaning**, not just word overlap\n",
    "* Enables **intelligent search and matching**\n",
    "* Works well with **Siamese networks**, **SBERT**, and **vector databases**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Disadvantages:\n",
    "\n",
    "* Embeddings may be **biased or context-insensitive** without proper finetuning\n",
    "* Needs **vector indexing** for large-scale search (e.g., FAISS or Pinecone)\n",
    "* Doesn‚Äôt explain **why** two texts are similar ‚Äî it‚Äôs a black box\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üß† **7. Topic Modeling**\n",
    "\n",
    "> üóÇÔ∏è Discovers **hidden themes** or **topics** from a large collection of text\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Definitions:\n",
    "\n",
    "* **Topic Modeling** is an **unsupervised learning** method to group words into **latent topics** that occur together in documents.\n",
    "* It helps make sense of **large, unlabeled corpora** by clustering common themes.\n",
    "* Popular techniques:\n",
    "\n",
    "  * üìö **LDA** (Latent Dirichlet Allocation)\n",
    "  * üßÆ **NMF** (Non-negative Matrix Factorization)\n",
    "  * ü§ñ **BERTopic** (leverages BERT + clustering)\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Real-time Use Case:\n",
    "\n",
    "* üìä **Customer feedback mining** ‚Äî discover themes in reviews or surveys\n",
    "* üóûÔ∏è **News classification** ‚Äî cluster articles into hidden topics\n",
    "* üßæ **Legal/academic discovery** ‚Äî explore large doc collections\n",
    "* üß† **Content recommendation systems** ‚Äî based on underlying topics\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå When Not to Use:\n",
    "\n",
    "* For **precise labeling** (use supervised classification instead)\n",
    "* When topics are **heavily overlapping** or noisy\n",
    "* If documents are **too short** (less meaningful word co-occurrence)\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Code Implementation (LDA with Gensim):\n",
    "\n",
    "```python\n",
    "from gensim import corpora, models\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "docs = [\n",
    "    \"Deep learning is transforming AI.\",\n",
    "    \"Healthcare is being revolutionized by AI.\",\n",
    "    \"Natural language processing is a key part of machine learning.\"\n",
    "]\n",
    "\n",
    "# Tokenization & stopword removal\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "texts = [[word for word in word_tokenize(doc.lower()) if word.isalpha() and word not in stop_words] for doc in docs]\n",
    "\n",
    "# Dictionary and corpus\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# LDA model\n",
    "lda = models.LdaModel(corpus, num_topics=2, id2word=dictionary, passes=15)\n",
    "\n",
    "# Print topics\n",
    "topics = lda.print_topics()\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Advantages:\n",
    "\n",
    "* Great for **exploratory analysis** without labels\n",
    "* Highlights **latent structure** in large text corpora\n",
    "* Easy to visualize with tools like **pyLDAvis** or **BERTopic plots**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Disadvantages:\n",
    "\n",
    "* Can be **hard to interpret** or control topic quality\n",
    "* Sensitive to **preprocessing** (stopwords, lemmatization, etc.)\n",
    "* LDA assumes **bag-of-words**, so it ignores word order/context\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ‚ö†Ô∏è **8. Anomaly Detection in Text**\n",
    "\n",
    "> üö® Identifies **unusual, rare, or suspicious patterns** in textual data\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Definitions:\n",
    "\n",
    "* **Anomaly Detection in Text** involves spotting documents, messages, or phrases that **deviate significantly** from the norm.\n",
    "* Unlike classification, it doesn‚Äôt always require labeled data.\n",
    "* Common techniques:\n",
    "\n",
    "  * üìä **TF-IDF + Isolation Forest / One-Class SVM**\n",
    "  * üß† **Autoencoders** ‚Äì reconstruct normal text, flag high-loss outputs\n",
    "  * üîç **Embedding-based** ‚Äì use cosine distance from centroid\n",
    "  * ü§ñ **LLMs** ‚Äì detect unexpected patterns with scoring (e.g., perplexity, log-likelihood)\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Real-time Use Case:\n",
    "\n",
    "* üí≥ **Fraud detection** in financial messages (claims, invoices)\n",
    "* üè¢ **Policy violation monitoring** in emails/chats\n",
    "* üßæ **Detecting fake news or spam articles**\n",
    "* üß† **Clinical note anomalies** (unexpected conditions, terms)\n",
    "* üõ°Ô∏è **Cybersecurity log monitoring**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå When Not to Use:\n",
    "\n",
    "* On small datasets where \"normal\" patterns are unclear\n",
    "* When you require **explainable decisions** (anomaly scores can be opaque)\n",
    "* For highly **subjective or creative content** (e.g., poems, social posts)\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Code Implementation (Embedding + Isolation Forest):\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Sample documents\n",
    "texts = [\n",
    "    \"The payment was processed successfully.\",\n",
    "    \"Login attempt failed due to invalid credentials.\",\n",
    "    \"Offer free credit cards to everyone now!!!\",  # Anomaly\n",
    "    \"Invoice #4569 was approved by the finance team.\"\n",
    "]\n",
    "\n",
    "# Get sentence embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(texts)\n",
    "\n",
    "# Fit Isolation Forest\n",
    "iso = IsolationForest(contamination=0.25)\n",
    "preds = iso.fit_predict(embeddings)\n",
    "\n",
    "for i, label in enumerate(preds):\n",
    "    status = \"Anomaly üö®\" if label == -1 else \"Normal ‚úÖ\"\n",
    "    print(f\"'{texts[i]}' ‚Üí {status}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Advantages:\n",
    "\n",
    "* Great for **fraud**, **compliance**, and **risk detection**\n",
    "* Can be **unsupervised** (no labels needed)\n",
    "* Works well with **embeddings or autoencoders**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Disadvantages:\n",
    "\n",
    "* Requires **fine-tuning contamination thresholds**\n",
    "* Sensitive to **data imbalance or drift**\n",
    "* May produce **false positives** in diverse datasets\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **7. Essential NLP Libraries**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ‚öôÔ∏è **1. üêç NLTK & SpaCy**\n",
    "\n",
    "> üî§ Powerful for **text preprocessing**, **tokenization**, **POS tagging**, and **NER**\n",
    "\n",
    "---\n",
    "\n",
    "#### üìå NLTK (Natural Language Toolkit)\n",
    "\n",
    "* A classic Python library used for:\n",
    "\n",
    "  * üìë **Tokenization**, stemming, lemmatization\n",
    "  * üè∑Ô∏è **POS tagging**, parsing, chunking\n",
    "  * üìà Statistical text analysis (frequency, n-grams)\n",
    "* Best for: **academic NLP**, tutorials, and **fine-grained control**\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = \"NLP is fun with NLTK and SpaCy!\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### üìå SpaCy\n",
    "\n",
    "* **Fast, production-ready** NLP toolkit with:\n",
    "\n",
    "  * ‚ö° Ultra-fast **tokenizer**\n",
    "  * üß† Built-in **POS tagging**, **NER**, **Dependency Parsing**\n",
    "  * üßæ **Pretrained pipelines** for multiple languages\n",
    "* Best for: **real-time apps**, **production deployment**\n",
    "\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Mukesh works at OpenAI in San Francisco.\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚úÖ Advantages:\n",
    "\n",
    "* NLTK: Great for **flexibility**, **teaching**, **linguistic analysis**\n",
    "* SpaCy: Ideal for **speed**, **modularity**, and **production use**\n",
    "\n",
    "#### ‚ùå Disadvantages:\n",
    "\n",
    "* NLTK: Slower, less maintained for deep NLP\n",
    "* SpaCy: Harder to **customize models** or do **deep learning** tasks\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ü§ó **2. Hugging Face Transformers**\n",
    "\n",
    "> üöÄ The **go-to Python library** for working with **pretrained transformer models**\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Overview:\n",
    "\n",
    "* **Hugging Face Transformers** provides:\n",
    "\n",
    "  * 100,000+ **pretrained models** for:\n",
    "\n",
    "    * üìÑ Text (BERT, GPT, T5, RoBERTa)\n",
    "    * üñºÔ∏è Vision (CLIP, DINO)\n",
    "    * üîä Audio (Wav2Vec, Whisper)\n",
    "    * üì¶ Multimodal (Flamingo, Llava)\n",
    "  * Compatible with **PyTorch**, **TensorFlow**, and **JAX**\n",
    "  * Integrates with:\n",
    "\n",
    "    * ü§ñ **Trainer API** for finetuning\n",
    "    * üîç **pipelines** for inference\n",
    "    * üß™ **Model Hub** for free model hosting\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Key Functionalities:\n",
    "\n",
    "| Feature                      | Description                                       |\n",
    "| ---------------------------- | ------------------------------------------------- |\n",
    "| `pipeline()`                 | Plug-and-play tasks (e.g. summarization, QA, NER) |\n",
    "| `AutoModel`, `AutoTokenizer` | Load model/tokenizer dynamically                  |\n",
    "| `Trainer`                    | Train/finetune on your dataset                    |\n",
    "| `Model Hub`                  | Access or publish pretrained models               |\n",
    "| `Datasets`                   | Load and manage datasets easily                   |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Real-time Use Case:\n",
    "\n",
    "* üß† **Finetune BERT for classification**\n",
    "* üí¨ **Build chatbots with GPT models**\n",
    "* üßæ **Summarize docs using T5/BART**\n",
    "* üìö **Multilingual QA**\n",
    "* üîç **Semantic search + embeddings**\n",
    "* üìà Used in **RAG**, **LLMOps**, and **enterprise GenAI apps**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Example: Text Classification Pipeline\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "result = classifier(\"Hugging Face makes NLP incredibly easy!\")\n",
    "print(result)\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "\n",
    "```\n",
    "[{'label': 'POSITIVE', 'score': 0.9998}]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Advantages:\n",
    "\n",
    "* üåç Huge model ecosystem & community\n",
    "* üöÄ Rapid prototyping with pipelines\n",
    "* ‚öôÔ∏è Fully compatible with custom training\n",
    "* üß† Access to **SOTA models**, including LLaMA, Mistral, Falcon, etc.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Disadvantages:\n",
    "\n",
    "* ‚ö†Ô∏è Can be **heavy** (GPU/VRAM needed for large models)\n",
    "* üß™ Some models may **hallucinate** or need fine-tuning\n",
    "* üß± Complex APIs for beginners (Trainer, config settings)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üß† **3. Sentence-Transformers (SBERT)**\n",
    "\n",
    "> üìè Used for **text similarity**, **semantic search**, and **clustering**\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Overview:\n",
    "\n",
    "* Built on top of **Hugging Face Transformers**, optimized for:\n",
    "\n",
    "  * üîç **Semantic similarity**\n",
    "  * üß≠ **Dense vector search**\n",
    "  * üîÅ **Paraphrase detection**\n",
    "  * üóÇÔ∏è **Clustering and topic modeling**\n",
    "* Models like:\n",
    "\n",
    "  * `all-MiniLM-L6-v2` (fast + accurate)\n",
    "  * `multi-qa-MiniLM` (for QA pipelines)\n",
    "  * `paraphrase-MPNet`, `distilroberta-base-v1`\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Real-time Use Case:\n",
    "\n",
    "* üß† **Semantic search** in enterprise knowledge bases\n",
    "* üìö **Duplicate question detection** (Quora, StackOverflow)\n",
    "* üì© **Email/thread clustering**\n",
    "* ‚úçÔ∏è **Text deduplication or document matching**\n",
    "* üîç Backbone of **vector databases** like FAISS, ChromaDB, Pinecone\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Code Example: Sentence Similarity\n",
    "\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "s1 = \"How do I reset my password?\"\n",
    "s2 = \"What's the process to recover a forgotten password?\"\n",
    "\n",
    "emb1 = model.encode(s1, convert_to_tensor=True)\n",
    "emb2 = model.encode(s2, convert_to_tensor=True)\n",
    "\n",
    "similarity = util.pytorch_cos_sim(emb1, emb2)\n",
    "print(f\"Similarity Score: {similarity.item():.2f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Advantages:\n",
    "\n",
    "* ‚ö° **Fast inference**, optimized for sentence-level inputs\n",
    "* üß© Easy to integrate with **vector search engines**\n",
    "* üß† Models trained with **triplet loss**, better than raw BERT embeddings\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Disadvantages:\n",
    "\n",
    "* Trained on English ‚Äî may underperform on other languages unless multilingual model used\n",
    "* Not ideal for **token-level tasks** like NER or POS tagging\n",
    "* Lacks the **generative capabilities** of full LLMs (e.g., GPT)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üîé **4. Gensim**\n",
    "\n",
    "> üìö Used for **Topic Modeling**, **TF-IDF**, and **Word Embeddings**\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Overview:\n",
    "\n",
    "* **Gensim** is a Python library for **unsupervised text modeling**, especially good at:\n",
    "\n",
    "  * üß† **Topic modeling** via **LDA**, **NMF**\n",
    "  * üìä **TF-IDF**, **BM25** vectorization\n",
    "  * üí¨ **Word2Vec**, **Doc2Vec** embeddings\n",
    "  * üîÑ Streaming and processing large corpora\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Real-time Use Case:\n",
    "\n",
    "* üóÇÔ∏è Discover hidden **topics in customer feedback**\n",
    "* üßæ **Summarize and group legal or policy documents**\n",
    "* üîç **Similarity-based search** using **Word2Vec vectors**\n",
    "* üß† **Build dictionaries and corpora** for unsupervised modeling\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Example: Topic Modeling with LDA\n",
    "\n",
    "```python\n",
    "from gensim import corpora, models\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "texts = [\n",
    "    \"Machine learning is amazing\",\n",
    "    \"AI is transforming industries\",\n",
    "    \"Natural language processing is part of AI\"\n",
    "]\n",
    "\n",
    "# Tokenize and clean\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "tokenized = [[word for word in word_tokenize(doc.lower()) if word.isalpha() and word not in stop_words] for doc in texts]\n",
    "\n",
    "# Create dictionary and corpus\n",
    "dictionary = corpora.Dictionary(tokenized)\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized]\n",
    "\n",
    "# Train LDA model\n",
    "lda = models.LdaModel(corpus, num_topics=2, id2word=dictionary, passes=10)\n",
    "\n",
    "# Show topics\n",
    "for idx, topic in lda.print_topics():\n",
    "    print(f\"Topic {idx}: {topic}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Advantages:\n",
    "\n",
    "* üîç Specialized for **unsupervised NLP**\n",
    "* üß± Handles **large datasets** efficiently (streaming support)\n",
    "* üéØ Great for **customizable topic modeling pipelines**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Disadvantages:\n",
    "\n",
    "* üì¶ Not deep-learning based (no transformers)\n",
    "* ‚ùå Doesn‚Äôt support modern **contextual embeddings**\n",
    "* ‚öôÔ∏è More manual steps than newer libraries (preprocessing, tokenization, etc.)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üß† **5. LangChain + Vector Databases**\n",
    "\n",
    "> üß© Combine **LLMs** + **memory** + **retrieval** for powerful **context-aware agents**\n",
    "\n",
    "---\n",
    "\n",
    "### üìå Overview:\n",
    "\n",
    "#### üß± **LangChain**\n",
    "\n",
    "* A Python framework to build **LLM-powered applications** that are:\n",
    "\n",
    "  * üîÅ Stateful (memory, conversation history)\n",
    "  * üì• Contextual (retrieval-augmented generation)\n",
    "  * üîß Modular (agents, tools, chains, retrievers)\n",
    "* Integrates with **OpenAI, Hugging Face, Cohere, Claude**, and more\n",
    "\n",
    "#### üóÇÔ∏è **Vector Databases** (for RAG: Retrieval-Augmented Generation)\n",
    "\n",
    "* Used to store and search **dense embeddings** from models like BERT, SBERT\n",
    "* Top tools:\n",
    "\n",
    "  * ‚ö° **FAISS** (Facebook): Local, fast, customizable\n",
    "  * üì¶ **ChromaDB**: Lightweight, open-source RAG-native\n",
    "  * üß† **Pinecone**: Managed, scalable, cloud-first\n",
    "  * üß¨ **Weaviate, Milvus**: Advanced hybrid and semantic search\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Real-time Use Case:\n",
    "\n",
    "* üß† **Chatbots with memory** (store user context & embeddings)\n",
    "* üìö **Private Q\\&A systems** over PDFs, Notion, databases\n",
    "* üèõÔ∏è **Legal/Healthcare/Finance RAG apps** (domain-specific)\n",
    "* ü§ñ **Autonomous agents** that use tools + documents to think\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è Example: LangChain + ChromaDB (RAG)\n",
    "\n",
    "```python\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Load and embed docs\n",
    "loader = TextLoader(\"company_policy.txt\")\n",
    "docs = loader.load()\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings()\n",
    "db = Chroma.from_documents(docs, embedding=embedding_model)\n",
    "\n",
    "# QA system\n",
    "retriever = db.as_retriever()\n",
    "llm = OpenAI()\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
    "result = qa.run(\"What is the company's leave policy?\")\n",
    "print(result)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Advantages:\n",
    "\n",
    "* üí° Enables **true context-aware generation**\n",
    "* üîÑ Reuses existing documents ‚Äî no retraining needed\n",
    "* üß± Modular and **LLM-agnostic** (OpenAI, local LLMs, etc.)\n",
    "* üîç Embeddings + Search = scalable **knowledge interfaces**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Disadvantages:\n",
    "\n",
    "* üõ†Ô∏è Requires setup of vector store + embedding model\n",
    "* üíæ Embeddings take space ‚Äî needs storage management\n",
    "* ü§ñ Agents/chains can get complex without best practices\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
