{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2wExpyaW/8FaJVIgkr/9V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mukeshrock7897/GenerativeAI/blob/main/2_LLM_Intermediate_Level.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Intermediate Level**\n",
        "\n",
        "1. Transformer Architecture\n",
        "    * Introduction to Transformers\n",
        "    * Self-attention mechanism\n",
        "    * Encoder-decoder architecture\n",
        "\n",
        "2. Pre-training and Fine-tuning\n",
        "    * Pre-training objectives (e.g., masked language modeling, causal language modeling)\n",
        "    * Fine-tuning for specific tasks\n",
        "\n",
        "3. Popular LLMs\n",
        "    * GPT (Generative Pre-trained Transformer)\n",
        "    * BERT (Bidirectional Encoder Representations from Transformers)\n",
        "    * T5 (Text-To-Text Transfer Transformer)\n",
        "\n",
        "4. Practical Implementation\n",
        "    * Using Hugging Face Transformers library\n",
        "    * Training and fine-tuning LLMs on custom datasets\n",
        "    * Handling large datasets and model training"
      ],
      "metadata": {
        "id": "_KFlZyqjk-qa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Transformer Architecture**\n",
        "\n",
        "**Introduction to Transformers**\n",
        "\n",
        "* Transformers are a type of deep learning model introduced in the paper \"Attention is All You Need\" by Vaswani et al. They revolutionized natural language processing (NLP) by effectively handling long-range dependencies in text and enabling parallelization of training.\n",
        "\n",
        "**Self-attention mechanism**\n",
        "\n",
        "* The self-attention mechanism allows each word in a sentence to attend to every other word, enabling the model to capture contextual relationships."
      ],
      "metadata": {
        "id": "ZEHKVSrjlHkL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-Yvz0MKktun"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Example: Self-attention mechanism\n",
        "def self_attention(query, key, value):\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(key.size(-1), dtype=torch.float32))\n",
        "    attention_weights = F.softmax(scores, dim=-1)\n",
        "    output = torch.matmul(attention_weights, value)\n",
        "    return output, attention_weights\n",
        "\n",
        "# Dummy data\n",
        "query = torch.randn(1, 5, 64)  # (batch_size, sequence_length, hidden_dim)\n",
        "key = torch.randn(1, 5, 64)\n",
        "value = torch.randn(1, 5, 64)\n",
        "\n",
        "output, attention_weights = self_attention(query, key, value)\n",
        "print(\"Output:\", output)\n",
        "print(\"Attention Weights:\", attention_weights)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encoder-decoder architecture**\n",
        "\n",
        "* The encoder-decoder architecture is a common design for sequence-to-sequence tasks such as translation. The encoder processes the input sequence, and the decoder generates the output sequence."
      ],
      "metadata": {
        "id": "ARza4o_ylScG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "\n",
        "# Load a pre-trained BART model (encoder-decoder architecture)\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
        "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n",
        "\n",
        "# Example: Translation\n",
        "input_text = \"Hello, how are you?\"\n",
        "inputs = tokenizer(input_text, return_tensors='pt')\n",
        "output = model.generate(**inputs)\n",
        "translated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(\"Translated text:\", translated_text)\n"
      ],
      "metadata": {
        "id": "B9kBQGm_lXip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Pre-training and Fine-tuning**\n",
        "\n",
        "**Pre-training objectives**\n",
        "\n",
        "* **Masked Language Modeling (MLM):** Predicting missing words in a sentence (used by BERT).\n",
        "\n",
        "* **Causal Language Modeling (CLM):** Predicting the next word in a sequence (used by GPT).\n",
        "\n",
        "\n",
        "**Fine-tuning for specific tasks**\n",
        "\n",
        "* Fine-tuning involves taking a pre-trained model and training it further on a specific task with task-specific data."
      ],
      "metadata": {
        "id": "1y8z2Q9blY6V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "# Load a pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Prepare data (dummy example)\n",
        "train_texts = [\"I love programming\", \"I hate bugs\"]\n",
        "train_labels = [1, 0]\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, return_tensors='pt')\n",
        "train_dataset = torch.utils.data.TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], torch.tensor(train_labels))\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    warmup_steps=10,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs'\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "avIzY3BoljpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Popular LLMs**\n",
        "\n",
        "**GPT (Generative Pre-trained Transformer)**\n",
        "\n",
        "* GPT is designed for text generation tasks. It uses a unidirectional transformer model that predicts the next word in a sequence.\n",
        "\n",
        "**Example: Text generation with GPT**"
      ],
      "metadata": {
        "id": "FbrnmZ7lllZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "# Load a pre-trained GPT-2 model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Generate text\n",
        "input_text = \"Artificial intelligence is\"\n",
        "inputs = tokenizer(input_text, return_tensors='pt')\n",
        "output = model.generate(**inputs, max_length=50)\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(\"Generated text:\", generated_text)\n"
      ],
      "metadata": {
        "id": "_ChZkNEKlse3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BERT (Bidirectional Encoder Representations from Transformers)**\n",
        "\n",
        "* BERT is designed for tasks requiring understanding of both the left and right context. It uses masked language modeling for pre-training.\n",
        "\n",
        "**Example: Text classification with BERT**"
      ],
      "metadata": {
        "id": "-pZOxtMnluJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "# Load a pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Prepare data (dummy example)\n",
        "train_texts = [\"I love programming\", \"I hate bugs\"]\n",
        "train_labels = [1, 0]\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, return_tensors='pt')\n",
        "train_dataset = torch.utils.data.TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], torch.tensor(train_labels))\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    warmup_steps=10,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs'\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "IbTVKfK7l0vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**T5 (Text-To-Text Transfer Transformer)**\n",
        "\n",
        "* T5 treats all NLP tasks as a text-to-text problem. It uses a text generation approach for both input and output.\n",
        "\n",
        "**Example: Text summarization with T5**"
      ],
      "metadata": {
        "id": "0w-Tl734l2pP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Load a pre-trained T5 model and tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "\n",
        "# Summarize text\n",
        "input_text = \"summarize: Artificial intelligence is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans and animals. Leading AI textbooks define the field as the study of 'intelligent agents'.\"\n",
        "inputs = tokenizer(input_text, return_tensors='pt')\n",
        "output = model.generate(**inputs, max_length=50)\n",
        "summary = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(\"Summary:\", summary)\n"
      ],
      "metadata": {
        "id": "kakk1HIIl6_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Practical Implementation**\n",
        "\n",
        "**Using Hugging Face Transformers library**\n",
        "\n",
        "* The Hugging Face Transformers library provides an easy-to-use interface for working with state-of-the-art NLP models.\n",
        "\n",
        "**Training and fine-tuning LLMs on custom datasets**\n",
        "\n",
        "* You can fine-tune pre-trained models on your own datasets using the Trainer class in the Transformers library.\n",
        "\n",
        "**Handling large datasets and model training**\n",
        "\n",
        "* When dealing with large datasets, it's important to use efficient data processing and training techniques to handle the computational load."
      ],
      "metadata": {
        "id": "xyOrhXiPl8p3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "\n",
        "# Prepare dataset (dummy example)\n",
        "texts = [\"I love programming\", \"I hate bugs\", \"Machine learning is fascinating\"]\n",
        "encodings = tokenizer(texts, truncation=True, padding=True, return_tensors='pt')\n",
        "dataset = torch.utils.data.TensorDataset(encodings['input_ids'], encodings['attention_mask'])\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True)\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    warmup_steps=10,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "iXdIIEIKmGkh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}