{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJtSpjsE+7Lhk0LvysEcxQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mukeshrock7897/GenerativeAI/blob/main/3_LLM_Advanced_level.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Advanced Level**\n",
        "\n",
        "1. Advanced LLM Architectures\n",
        "    * Transformer-XL\n",
        "    * Reformer\n",
        "    * Longformer\n",
        "\n",
        "2. Techniques for Scaling LLMs\n",
        "    * Distributed training\n",
        "    * Model parallelism\n",
        "    * Efficient fine-tuning methods (e.g., adapters, LoRA)\n",
        "\n",
        "3. Advanced Applications\n",
        "    * Zero-shot and few-shot learning\n",
        "    * Multimodal models (e.g., CLIP, DALL-E)\n",
        "\n",
        "4. Challenges and Future Directions\n",
        "    * Overcoming limitations of LLMs\n",
        "    * Future trends in LLM research and development\n",
        "\n",
        "# **Frameworks and Libraries for LLMs**\n",
        "1. Hugging Face Transformers\n",
        "    * Overview and key features\n",
        "    * Installing and using the library\n",
        "    * Pre-trained models and fine-tuning\n",
        "\n",
        "2. LangChain\n",
        "    * Overview and key features\n",
        "    * Integrating LLMs into applications\n",
        "    * Example use cases and implementation\n",
        "\n",
        "3. GPT-3 and GPT-4 by OpenAI\n",
        "    * Overview and capabilities\n",
        "    * Using the API for various tasks\n",
        "    * Examples and code snippets\n",
        "\n",
        "4. Other Frameworks\n",
        "    * Fairseq by Facebook AI\n",
        "    * DeepSpeed by Microsoft\n",
        "    * Megatron-LM by NVIDIA\n",
        "    * EleutherAI’s GPT-Neo and GPT-J"
      ],
      "metadata": {
        "id": "IT5K59L-miB5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#**1. Advanced LLM Architectures**\n",
        "\n",
        "**Transformer-XL**\n",
        "\n",
        "Transformer-XL addresses the fixed-length context limitation of traditional transformers by introducing a segment-level recurrence mechanism and a novel relative positional encoding scheme, enabling learning dependencies beyond a fixed length without disrupting temporal coherence.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tJvf41EPoGka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TransfoXLTokenizer, TransfoXLLMHeadModel\n",
        "\n",
        "# Load pre-trained Transformer-XL model and tokenizer\n",
        "tokenizer = TransfoXLTokenizer.from_pretrained('transfo-xl-wt103')\n",
        "model = TransfoXLLMHeadModel.from_pretrained('transfo-xl-wt103')\n",
        "\n",
        "# Example: Text generation with Transformer-XL\n",
        "input_text = \"Artificial intelligence is\"\n",
        "inputs = tokenizer(input_text, return_tensors='pt')\n",
        "output = model.generate(**inputs, max_length=50)\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(\"Generated text:\", generated_text)"
      ],
      "metadata": {
        "id": "uVnIIkAKosmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Reformer**\n",
        "\n",
        "Reformer improves the efficiency of transformers by using locality-sensitive hashing for self-attention and reversible residual layers to save memory during training, making it suitable for handling longer sequences with reduced computational costs.\n"
      ],
      "metadata": {
        "id": "ryfMrQ1Iol9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ReformerTokenizer, ReformerModelWithLMHead\n",
        "\n",
        "# Load pre-trained Reformer model and tokenizer\n",
        "tokenizer = ReformerTokenizer.from_pretrained('google/reformer-enwik8')\n",
        "model = ReformerModelWithLMHead.from_pretrained('google/reformer-enwik8')\n",
        "\n",
        "# Example: Text generation with Reformer\n",
        "input_text = \"Natural language processing\"\n",
        "inputs = tokenizer(input_text, return_tensors='pt')\n",
        "output = model.generate(**inputs, max_length=50)\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(\"Generated text:\", generated_text)"
      ],
      "metadata": {
        "id": "hEKtZTIwo8r7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Longformer**\n",
        "\n",
        "Longformer extends the transformer model to process longer sequences by using a combination of sliding window attention and global attention, allowing it to efficiently handle longer documents without the quadratic complexity of standard transformers.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kbHmXzCXo3OF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LongformerTokenizer, LongformerForQuestionAnswering\n",
        "\n",
        "# Load pre-trained Longformer model and tokenizer\n",
        "tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
        "model = LongformerForQuestionAnswering.from_pretrained('allenai/longformer-base-4096')\n",
        "\n",
        "# Example: Question answering with Longformer\n",
        "question, text = \"What is artificial intelligence?\", \"Artificial intelligence is the simulation of human intelligence in machines that are programmed to think like humans and mimic their actions.\"\n",
        "inputs = tokenizer(question, text, return_tensors='pt')\n",
        "output = model(**inputs)\n",
        "answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(\"Answer:\", answer)"
      ],
      "metadata": {
        "id": "YD7L3Mb6pQF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Techniques for Scaling LLMs\n",
        "\n",
        "**Distributed Training**\n",
        "\n",
        "Distributed training involves splitting the training process across multiple GPUs or nodes to handle large models and datasets, speeding up the training process."
      ],
      "metadata": {
        "id": "AK_nVWrypa_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Distributed training example using PyTorch\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "\n",
        "def setup(rank, world_size):\n",
        "    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
        "\n",
        "def train(rank, world_size, model, data_loader, optimizer):\n",
        "    setup(rank, world_size)\n",
        "    model = DDP(model)\n",
        "    for data, target in data_loader:\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Example usage with 4 GPUs\n",
        "world_size = 4\n",
        "model = ...  # Define your model\n",
        "data_loader = ...  # Define your data loader\n",
        "optimizer = ...  # Define your optimizer\n",
        "torch.multiprocessing.spawn(train, args=(world_size, model, data_loader, optimizer), nprocs=world_size)"
      ],
      "metadata": {
        "id": "QtRgjswcpe4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Model Parallelism**\n",
        "\n",
        "Model parallelism splits the model across multiple GPUs, enabling training of very large models that don't fit into the memory of a single GPU.\n"
      ],
      "metadata": {
        "id": "0Yxl_N2Mpn6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model parallelism example using PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ModelParallelResNet50(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ModelParallelResNet50, self).__init__()\n",
        "        self.seq = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            # ... (other layers)\n",
        "        ).to('cuda:0')\n",
        "\n",
        "        self.fc = nn.Linear(512, 1000).to('cuda:1')\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.seq(x.to('cuda:0'))\n",
        "        x = self.fc(x.to('cuda:1'))\n",
        "        return x\n",
        "\n",
        "model = ModelParallelResNet50()"
      ],
      "metadata": {
        "id": "3SbFFDxjprTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Efficient Fine-tuning Methods (e.g., Adapters, LoRA)**\n",
        "\n",
        "Adapters and LoRA (Low-Rank Adaptation) are methods to efficiently fine-tune pre-trained models by adding small trainable layers or low-rank matrices without modifying the entire model."
      ],
      "metadata": {
        "id": "AniGNk-kpv-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, AdapterConfig\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Add an adapter\n",
        "adapter_config = AdapterConfig.load(\"pfeiffer\")\n",
        "model.add_adapter(\"my_adapter\", config=adapter_config)\n",
        "model.train_adapter(\"my_adapter\")\n",
        "\n",
        "# Fine-tune with adapters\n",
        "input_texts = [\"I love AI\", \"I dislike bugs\"]\n",
        "inputs = tokenizer(input_texts, return_tensors='pt', padding=True, truncation=True)\n",
        "labels = torch.tensor([1, 0]).unsqueeze(0)\n",
        "outputs = model(**inputs, labels=labels)\n",
        "loss = outputs.loss\n",
        "print(\"Loss:\", loss)"
      ],
      "metadata": {
        "id": "fTF6eJS2p0Ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Advanced Applications**\n",
        "\n",
        "**Zero-shot and Few-shot Learning**\n",
        "\n",
        "* Zero-shot and few-shot learning allow models to generalize to new tasks with little to no task-specific training data by leveraging knowledge gained from pre-training."
      ],
      "metadata": {
        "id": "NnG2SsjmpLOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Zero-shot classification using a pre-trained model\n",
        "classifier = pipeline(\"zero-shot-classification\")\n",
        "sequence_to_classify = \"This is a great movie!\"\n",
        "candidate_labels = [\"positive\", \"negative\"]\n",
        "result = classifier(sequence_to_classify, candidate_labels)\n",
        "print(\"Result:\", result)\n"
      ],
      "metadata": {
        "id": "te3wAw_Au6UO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multimodal Models (e.g., CLIP, DALL-E)**\n",
        "\n",
        "* Multimodal models like CLIP and DALL-E handle inputs from multiple modalities (e.g., text and images) to perform tasks like image generation from text descriptions."
      ],
      "metadata": {
        "id": "-ktiDYQWu8Eb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "# Load pre-trained CLIP model and processor\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Example: Image and text similarity\n",
        "image = ...  # Load an image\n",
        "texts = [\"a photo of a cat\", \"a photo of a dog\"]\n",
        "inputs = processor(text=texts, images=image, return_tensors=\"pt\", padding=True)\n",
        "outputs = model(**inputs)\n",
        "logits_per_image = outputs.logits_per_image\n",
        "print(\"Logits per image:\", logits_per_image)\n"
      ],
      "metadata": {
        "id": "6wBNiDq3vAcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Challenges and Future Directions**\n",
        "\n",
        "**Overcoming Limitations of LLMs**\n",
        "\n",
        "* Addressing challenges like bias, interpretability, and computational cost to improve the robustness and fairness of LLMs.\n",
        "\n",
        "**Future Trends in LLM Research and Development**\n",
        "\n",
        "* Exploring future trends like more efficient architectures, better understanding of model internals, and applications in diverse domains."
      ],
      "metadata": {
        "id": "WXDIZ-45vB-q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Frameworks and Libraries for LLMs**\n",
        "\n",
        "# **1. Hugging Face Transformers**\n",
        "\n",
        "**Overview and Key Features**\n",
        "\n",
        "* Hugging Face Transformers provides a comprehensive library for state-of-the-art NLP models, including tools for training, fine-tuning, and deployment.\n",
        "\n",
        "**Installing and Using the Library**"
      ],
      "metadata": {
        "id": "UDpEI9qZvOBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Example: Text classification\n",
        "input_text = \"I love artificial intelligence!\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "outputs = model(**inputs)\n",
        "print(\"Logits:\", outputs.logits)\n"
      ],
      "metadata": {
        "id": "uTRBG6bHvLxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pre-trained Models and Fine-tuning**\n",
        "\n",
        "* Fine-tuning pre-trained models for specific tasks using the Trainer class."
      ],
      "metadata": {
        "id": "En15inLsvYsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=8,\n",
        "    logging_dir='./logs',\n",
        ")\n",
        "\n",
        "# Define trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "AwBUE33MvdFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. LangChain**\n",
        "\n",
        "**Overview and Key Features**\n",
        "\n",
        "* LangChain focuses on integrating LLMs into applications, providing tools for creating complex workflows and handling interactions with LLMs.\n",
        "\n",
        "**Integrating LLMs into Applications**\n",
        "\n",
        "* LangChain allows seamless integration of LLMs into various applications, enabling easy implementation of language-based tasks.\n",
        "\n",
        "**Example Use Cases and Implementation**\n",
        "\n",
        "* LangChain can be used for tasks like text generation, summarization, and translation, with APIs for interacting with LLMs.\n",
        "\n",
        "# **3. GPT-3 and GPT-4 by OpenAI**\n",
        "\n",
        "**Overview and Capabilities**\n",
        "\n",
        "* GPT-3 and GPT-4 by OpenAI are powerful LLMs capable of performing a wide range of language tasks with high accuracy and fluency.\n",
        "\n",
        "**Using the API for Various Tasks**\n",
        "\n",
        "* OpenAI provides an API for interacting with GPT-3 and GPT-4, enabling tasks like text generation, summarization, and translation."
      ],
      "metadata": {
        "id": "D7cDj8P1vesL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Initialize OpenAI API\n",
        "openai.api_key = \"your-api-key\"\n",
        "\n",
        "# Example: Text generation with GPT-3\n",
        "response = openai.Completion.create(\n",
        "    engine=\"davinci-codex\",\n",
        "    prompt=\"Once upon a time\",\n",
        "    max_tokens=50\n",
        ")\n",
        "print(\"Generated text:\", response.choices[0].text.strip())\n"
      ],
      "metadata": {
        "id": "oCp0VaCnvy-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Other Frameworks**\n",
        "\n",
        "**Fairseq by Facebook AI**\n",
        "\n",
        "* Fairseq is a sequence-to-sequence learning toolkit by Facebook AI, supporting tasks like translation, summarization, and language modeling.\n",
        "\n",
        "**DeepSpeed by Microsoft**\n",
        "\n",
        "* DeepSpeed is a deep learning optimization library by Microsoft, providing tools for efficient training of large models with support for model parallelism and mixed precision training.\n",
        "\n",
        "**Megatron-LM by NVIDIA**\n",
        "\n",
        "* Megatron-LM is a library by NVIDIA for training large-scale language models using model parallelism and efficient training techniques.\n",
        "\n",
        "**EleutherAI’s GPT-Neo and GPT-J**\n",
        "\n",
        "* EleutherAI provides open-source implementations of large-scale language models like GPT-Neo and GPT-J, enabling training and fine-tuning on custom datasets."
      ],
      "metadata": {
        "id": "8wQjjZ0Pvzs8"
      }
    }
  ]
}